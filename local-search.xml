<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>计算机视觉</title>
    <link href="/2024/08/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    <url>/2024/08/29/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/</url>
    
    <content type="html"><![CDATA[<h1 id="计算机视觉第一节"><a href="#计算机视觉第一节" class="headerlink" title="计算机视觉第一节"></a>计算机视觉第一节</h1><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><ul><li><p>图像预处理：</p></li><li><p>特征提取：SIFT 、HOG</p><p><strong>SIFT</strong>：尺度不变特征变换</p><ol><li>构建尺度空间，检测极值点作为特征点（LOG, DOG）</li><li>方向匹配</li><li>关键点描述符</li><li>关键点匹配</li></ol><p><strong>HOG</strong>：</p></li><li><p>特征表示：</p></li><li><p>训练ML模型：</p></li></ul><h2 id="传统图像分类"><a href="#传统图像分类" class="headerlink" title="传统图像分类"></a>传统图像分类</h2><ul><li><p>基于词袋的图像分类</p><p>计算特征点（SIFT, HOG）的相似度</p></li><li></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>基于图片生成诗句（二）</title>
    <link href="/2024/08/27/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E8%AF%97%E5%8F%A5%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <url>/2024/08/27/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E8%AF%97%E5%8F%A5%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="基于图片生成诗句（二）"><a href="#基于图片生成诗句（二）" class="headerlink" title="基于图片生成诗句（二）"></a>基于图片生成诗句（二）</h1><h1 id="一、CLIP模型"><a href="#一、CLIP模型" class="headerlink" title="一、CLIP模型"></a>一、CLIP模型</h1><p>该模型的核心思想是使用大量图像和文本的配对数据进行预训练，一学习图像和文本之间的对齐关系。CLIP包含2个模态：文本和视觉模态。</p><ul><li>Text Encoder： 用于把文本转成低维向量表示</li><li>Image Encoder：用于把图像转成类似向量表示</li></ul><p>在预测阶段，计算文本和图像向量之间的<strong>余弦相似度</strong>来生成预测。</p><h2 id="1-Components"><a href="#1-Components" class="headerlink" title="1. Components"></a>1. Components</h2><p><img src="/2024/08/27/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E8%AF%97%E5%8F%A5%EF%BC%88%E4%BA%8C%EF%BC%89/image-20240827143515727.png" alt="image-20240827143515727"></p><h3 id="1-1-Image-Encoder"><a href="#1-1-Image-Encoder" class="headerlink" title="1.1 Image Encoder"></a>1.1 Image Encoder</h3><ul><li>架构一：基于ResNet50, 根据ResNetD改进，还将<strong>全局平均池化层</strong>替换为<strong>注意力池化机制</strong>。</li><li>架构二：基于VIT</li></ul><h3 id="1-2-Text-Encoder"><a href="#1-2-Text-Encoder" class="headerlink" title="1.2 Text Encoder"></a>1.2 Text Encoder</h3><p>文本编辑器使用Transformer架构，并在此基础上根据Radford模型进行了架构修改。</p><h2 id="2-Method"><a href="#2-Method" class="headerlink" title="2. Method"></a>2. Method</h2><h3 id="2-1-数据集"><a href="#2-1-数据集" class="headerlink" title="2.1 数据集"></a>2.1 数据集</h3><p>文章构建了一个新的数据集，其中包含4亿对(图像、文本)，这些数据集来自互联网上各种公开可用的资源。为了尝试覆盖尽可能广泛的视觉概念集，文中将搜索(图像，文本)对作为构建过程的一部分，其文本包含500,000个查询集中的一个。</p><h3 id="2-2-预训练方法"><a href="#2-2-预训练方法" class="headerlink" title="2.2 预训练方法"></a>2.2 预训练方法</h3><p>给定一批N对的(img, text)对，CLIP来训练这$N \times N$的组合中哪几对会实际发生。最大化N对真实嵌入对的余弦相似度，最小化剩下$N^2 - N$对错误嵌入的余弦相似度。在这些相似性得分上优化对称交叉熵损失。</p><h3 id="2-3-训练方法"><a href="#2-3-训练方法" class="headerlink" title="2.3 训练方法"></a>2.3 训练方法</h3><p>CLIP预训练时训练的Text Encoder和Image Encoder在大量数据上能正确配对图像和文本。 然后， 使用这种方法把CLIP变成zero-shot分类器，把数据集种所有类转为文本，’a photo of a {object}’， 来预测CLIP估计的标题类与给定图像的最佳配对。</p><h1 id="二、本项目为什么要引入CLIP"><a href="#二、本项目为什么要引入CLIP" class="headerlink" title="二、本项目为什么要引入CLIP"></a>二、本项目为什么要引入CLIP</h1><p>在之前 版本1（<a href="https://wendyflv.github.io/2024/06/07/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E5%8F%A4%E8%AF%97/">基于图片生成古诗 - Wendyflv的博客</a>）中，我基于Resnet在Cifar100数据上进行图像分类，把图像的标签局限于100个标签（且关键词的对应还是自己人为设定的），尽管之后使用了word2vec进行补救充沛关键词，但是扩充的关键词只是基于原图像的标签，并不能发掘更多的特征。如果考虑使用CLIP模型，生成古诗意象关键词向量和图像向量，给图像匹配关键词，能提高模型的泛化性能。</p><p>先搜集一个关键词数据集(<code>keyword.txt</code>)，然后使用CLIP对图像和所有关键词进行编码，计算它们之间的相似度，取相似度最高的K个关键词，然后放置于语言模型进行生成.</p><h1 id="三、项目架构"><a href="#三、项目架构" class="headerlink" title="三、项目架构"></a>三、项目架构</h1><p><img src="/2024/08/27/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E8%AF%97%E5%8F%A5%EF%BC%88%E4%BA%8C%EF%BC%89/image-20240829155351418.png" alt="image-20240829155351418"></p><h2 id="1-CLIP模型"><a href="#1-CLIP模型" class="headerlink" title="1. CLIP模型"></a>1. CLIP模型</h2><p>直接调用中文CLIP模型</p><p>下面举个使用Chinese_CLIP对给定的一副图像和多个关键词匹配相似度的例子</p><p>img——&gt;(CLIP)——&gt;img_feature [1 × 512]</p><p>key_words ——&gt;(CLIP)——&gt; text_features [n × 512]</p><p>img , key_words——&gt; cosine_similariy</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> ChineseCLIPProcessor, ChineseCLIPModel<br><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-comment"># 加载模型</span><br>model = ChineseCLIPModel.from_pretrained(<span class="hljs-string">&quot;./Chinese_CLIP&quot;</span>)<br>processor = ChineseCLIPProcessor.from_pretrained(<span class="hljs-string">&quot;./Chinese_CLIP&quot;</span>)<br><br><span class="hljs-comment"># 对图像编码</span><br>imag = Image.<span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;D:/NLP/CLIPForPoems/Image2Poem/datasets/images/chun.jpg&#x27;</span>)<br>inputs = processor(images=imag, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br>image_features = model.get_image_features(**inputs)<br><span class="hljs-built_in">print</span>(image_features.shape)<br><br><span class="hljs-comment"># 对关键词编码</span><br>key_words = [<span class="hljs-string">&#x27;余晖&#x27;</span>, <span class="hljs-string">&#x27;樱花&#x27;</span>, <span class="hljs-string">&#x27;春色&#x27;</span>,<span class="hljs-string">&#x27;晚霞&#x27;</span> ,<span class="hljs-string">&#x27;夏日&#x27;</span>, <span class="hljs-string">&#x27;沙漠&#x27;</span>, <span class="hljs-string">&#x27;旅人&#x27;</span>]<br>text_features = []<br><span class="hljs-keyword">for</span> keyword <span class="hljs-keyword">in</span> key_words:<br>    feature = processor(text=keyword, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>)<br>    text_features.append(model.get_text_features(**feature))<br>    <br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">cosine_similarity</span>(<span class="hljs-params">x, y</span>):<br>    <span class="hljs-keyword">return</span> torch.<span class="hljs-built_in">sum</span>(x * y) / (torch.sqrt(torch.<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">pow</span>(x, <span class="hljs-number">2</span>))) * torch.sqrt(torch.<span class="hljs-built_in">sum</span>(<span class="hljs-built_in">pow</span>(y, <span class="hljs-number">2</span>))))<br><br><br><span class="hljs-comment"># 将图片和关键词编码做余弦相似度计算</span><br>i = <span class="hljs-number">0</span><br><span class="hljs-keyword">for</span> text_feature <span class="hljs-keyword">in</span> text_features:<br>    similar = cosine_similarity(text_feature, image_features)<br>    <span class="hljs-built_in">print</span>(key_words[i], similar)<br>    i +=<span class="hljs-number">1</span><br></code></pre></td></tr></table></figure><p>结果：下面输出了每个关键词与图像的相似度（[0,1]）。</p><p>余晖 tensor(0.4195, grad_fn&#x3D;<DivBackward0>)<br>樱花 tensor(0.3808, grad_fn&#x3D;<DivBackward0>)<br>春色 tensor(0.4233, grad_fn&#x3D;<DivBackward0>)<br>晚霞 tensor(0.3944, grad_fn&#x3D;<DivBackward0>)<br>夏日 tensor(0.3573, grad_fn&#x3D;<DivBackward0>)<br>沙漠 tensor(0.3276, grad_fn&#x3D;<DivBackward0>)<br>旅人 tensor(0.3683, grad_fn&#x3D;<DivBackward0>)</DivBackward0></DivBackward0></DivBackward0></DivBackward0></DivBackward0></DivBackward0></DivBackward0></p><h2 id="2-诗句生成模型"><a href="#2-诗句生成模型" class="headerlink" title="2. 诗句生成模型"></a>2. 诗句生成模型</h2><p>之前自己训练的效果太差了，改用T5模型在古诗词上微调。</p><p>T5模型采用的是基于Transformer的Encoder-Decoder结构。</p><p><img src="/2024/08/27/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E8%AF%97%E5%8F%A5%EF%BC%88%E4%BA%8C%EF%BC%89/image-20240830141904021.png" alt="image-20240830141904021"></p><p>下面使用(<a href="https://modelscope.cn/models/langboat/mengzi-t5-base">孟子中文T5</a>)微调一个诗句生成模型。</p><h3 id="2-1-数据预处理"><a href="#2-1-数据预处理" class="headerlink" title="2.1 数据预处理"></a>2.1 数据预处理</h3><p>数据来源：中国古典诗歌匹配数据集</p><p><img src="/2024/08/27/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E8%AF%97%E5%8F%A5%EF%BC%88%E4%BA%8C%EF%BC%89/image-20240830145625233.png" alt="image-20240830145625233"></p><p>把数据处理成 输入：关键词+诗句的 输出：诗句</p><p>例如：</p><p>x &#x3D; [CLS]关键词：春日 细雨 余晖[EOS] 春山新雨后[EOS] 天气晚来秋[EOS] 明月松间照[EOS]  </p><p>y &#x3D; [CLS]清泉石上流[EOS]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 读取数据</span><br><span class="hljs-comment"># 把关键词和数据清洗出来</span><br><span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./datasets/CCPC/ccpc_train_v1.0.json&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>, encoding=<span class="hljs-string">&quot;utf-8&quot;</span>) <span class="hljs-keyword">as</span> f:<br>    lines = []<br>    total_lines = <span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">&quot;./datasets/CCPC/ccpc_train_v1.0.json&quot;</span>, <span class="hljs-string">&quot;r&quot;</span>,encoding=<span class="hljs-string">&quot;utf-8&quot;</span>))<br>    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> tqdm(f, desc=<span class="hljs-string">&quot;Loading Data&quot;</span>, total=total_lines):<br>        json_line = json.loads(line)<br>        keywords=  json_line[<span class="hljs-string">&quot;keywords&quot;</span>].strip()<br>        poems = json_line[<span class="hljs-string">&quot;content&quot;</span>].strip().split(<span class="hljs-string">&quot;|&quot;</span>)<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(poems)):<br>            <span class="hljs-keyword">if</span> i ==<span class="hljs-number">0</span> :<br>                x = <span class="hljs-string">&quot;关键词：&quot;</span>+ keywords<br>            <span class="hljs-keyword">else</span>: x = <span class="hljs-string">&quot;关键词：&quot;</span>+ keywords + <span class="hljs-string">&quot;EOS&quot;</span> <br>            line = (x + <span class="hljs-string">&quot;EOS&quot;</span>.join(poems[:i]) + <span class="hljs-string">&quot;EOS&quot;</span>, poems[i] + <span class="hljs-string">&quot;EOS&quot;</span>)<br>            <span class="hljs-built_in">print</span>(line)<br>            lines.append((x + <span class="hljs-string">&quot;EOS&quot;</span>.join(poems[:i]) + <span class="hljs-string">&quot;EOS&quot;</span>, poems[i] + <span class="hljs-string">&quot;EOS&quot;</span>))<br>            <br>        corpus_lines = <span class="hljs-built_in">len</span>(lines)<br>        <span class="hljs-keyword">break</span><br><br>       <br></code></pre></td></tr></table></figure><p>构造一个batch输入和输出：确定输入长度，输出长度。对不满足的文本做填充。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">__getitem__</span>(<span class="hljs-params">item</span>):<br>    <span class="hljs-comment"># 取 [item * bs: (item + 1) * bs ]的文本数据</span><br>    data = lines[item * batch_size : (item + <span class="hljs-number">1</span>) * batch_size]<br>    input_len = <span class="hljs-built_in">max</span>([ <span class="hljs-built_in">len</span>( s[<span class="hljs-number">0</span>].replace(<span class="hljs-string">&quot; &quot;</span>,<span class="hljs-string">&quot;&quot;</span>).replace(<span class="hljs-string">&quot;EOS&quot;</span>, <span class="hljs-string">&quot;&quot;</span>)) <span class="hljs-keyword">for</span> s  <span class="hljs-keyword">in</span> data]) + <span class="hljs-number">6</span><br>    output_len = <span class="hljs-built_in">max</span>([ <span class="hljs-built_in">len</span>(s[<span class="hljs-number">1</span>].replace(<span class="hljs-string">&quot; &quot;</span>,<span class="hljs-string">&quot;&quot;</span>).replace(<span class="hljs-string">&quot;EOS&quot;</span>, <span class="hljs-string">&quot;&quot;</span>)) <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> data]) +<span class="hljs-number">3</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;input_len:&quot;</span>, input_len)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;output_len: &quot;</span>, output_len)<br><br>    tokens, targets, attens_x, attens_y = [], [], [], []<br><br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(batch_size):<br>        <span class="hljs-comment"># 处理输入</span><br>        <span class="hljs-comment"># 对不满足最长长度的文本后面补上PAD</span><br>        x = tokenizer.encode(data[i][<span class="hljs-number">0</span>])<br>        <span class="hljs-comment"># 有效字符为1</span><br>        atten_x = [<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(x))]<br>        <span class="hljs-comment"># 填充部分为0</span><br>        atten_x += [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(input_len - <span class="hljs-built_in">len</span>(x))]<br>        x.extend([tokenizer.pad_token_id <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(input_len - <span class="hljs-built_in">len</span>(x))])<br><br>        <span class="hljs-comment"># 处理输出</span><br>        y = tokenizer.encode(data[i][<span class="hljs-number">1</span>])<br>        atten_y =[<span class="hljs-number">1</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(y))]<br>        atten_y += [<span class="hljs-number">0</span> <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(output_len - <span class="hljs-built_in">len</span>(y))]<br>        y.extend([tokenizer.pad_token_id <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(output_len - <span class="hljs-built_in">len</span>(y))])<br><br>        <span class="hljs-built_in">print</span>(x)<br>        <span class="hljs-built_in">print</span>(tokenizer.decode(x))<br>        <span class="hljs-built_in">print</span>(y)<br>        <span class="hljs-built_in">print</span>(tokenizer.decode(y))<br>        <span class="hljs-built_in">print</span>(atten_x)<br>        <span class="hljs-built_in">print</span>(atten_y)<br><br>        tokens.append(x)<br>        targets.append(y)<br>        attens_x.append(atten_x)<br>        attens_y.append(atten_y)<br><br>        <span class="hljs-keyword">break</span><br></code></pre></td></tr></table></figure><p><img src="/2024/08/27/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E8%AF%97%E5%8F%A5%EF%BC%88%E4%BA%8C%EF%BC%89/image-20240830153207649.png" alt="image-20240830153207649"></p><h3 id="2-2-T5模型"><a href="#2-2-T5模型" class="headerlink" title="2.2 T5模型"></a>2.2 T5模型</h3><p>简单输出一下模型的结构看看：</p><p>input——&gt;[Embedding Layer]——&gt;Encoder[n × T5Block ] ——&gt;Decoder[n ×T5Block ] ——&gt;LM Head——&gt; output</p><p>对于Encoder的一个T5Block：包括一个自注意力机制，前馈网络 和 层归一化。</p><p><img src="/2024/08/27/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E8%AF%97%E5%8F%A5%EF%BC%88%E4%BA%8C%EF%BC%89/image-20240830162646630.png" alt="image-20240830162646630"></p><p>对于Decoder的一个T5Block：包括：字注意力机制，跨注意力机制（关注解码器的输出）， 前馈网络 和 层归一化</p><p><img src="/2024/08/27/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E8%AF%97%E5%8F%A5%EF%BC%88%E4%BA%8C%EF%BC%89/image-20240830163003430.png" alt="image-20240830163003430"></p><h3 id="2-3-训练"><a href="#2-3-训练" class="headerlink" title="2.3 训练"></a>2.3 训练</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>基于解耦注意力的GraphTransformer</title>
    <link href="/2024/08/24/%E5%9F%BA%E4%BA%8E%E8%A7%A3%E8%80%A6%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84GraphTransformer/"/>
    <url>/2024/08/24/%E5%9F%BA%E4%BA%8E%E8%A7%A3%E8%80%A6%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84GraphTransformer/</url>
    
    <content type="html"><![CDATA[<h1 id="基于解耦注意力的GraphTransformer"><a href="#基于解耦注意力的GraphTransformer" class="headerlink" title="基于解耦注意力的GraphTransformer"></a>基于解耦注意力的GraphTransformer</h1><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>token（$x$） + 可学习的位置信息（$\phi$）</p><p>该项目基于SMPL模型进行3D单目人体形状和姿态估计。在经典的Transformer模型的基础上进行了注意力解耦和图卷积模块的拼接。传统的姿态估计的方法在特征长度方面的二次计算和内存复杂性，通过引入注意力解耦操作能降低到线性层面。此外，为</p><p>本项目致力于改进单目3D人体形状和姿态估计技术，基于 SMPL 模型实现高效和精准的姿态预测。项目的核心创新在于对VIT架构的优化，通过解耦注意力机制，显著降低了特征计算的复杂度，将计算成本从二次计算减少到线性层面。此外，我们在VIT中引入了图卷积网络（GCN），以对人体关节表示进行深度特征提取，增强了模型对空间结构信息的捕捉能力，从而进一步优化了目标表示。实验结果表明，我们提出的方法在多个基准上明显优于以前最先进的方法，包括 Human3.6M、3DPW数据集。</p><p>训练：20min&#x2F;次</p><p>测试：15min次</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>3DPW相关数据处理</title>
    <link href="/2024/08/24/3DPW%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
    <url>/2024/08/24/3DPW%E7%9B%B8%E5%85%B3%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</url>
    
    <content type="html"><![CDATA[<h1 id="3DPW相关数据处理"><a href="#3DPW相关数据处理" class="headerlink" title="3DPW相关数据处理"></a>3DPW相关数据处理</h1><h2 id="数据集介绍"><a href="#数据集介绍" class="headerlink" title="数据集介绍"></a>数据集介绍</h2><p>3DPW 全称 3D Poses in the Wild，是首个具有精确 3D 姿态的户外数据集，可用于解决姿态估计问题。</p><p>下载：<a href="https://datarelease.blob.core.windows.net/metro/datasets/3dpw.tar">https://datarelease.blob.core.windows.net/metro/datasets/3dpw.tar</a></p><p>包括：</p><ul><li><p>img: train.img.tsv 人体图像数据</p></li><li><p>hw: train.hw.tsv  图像height, width数据</p></li><li><p>label: train.label.tsv 图像标签（文件路径，中心点，尺度因子，2D关节，3D关节，pose，betas ）</p></li><li><p>linelist: train.linelist.tsv 图像编号</p></li></ul><h2 id="相关处理"><a href="#相关处理" class="headerlink" title="相关处理"></a>相关处理</h2><p>img ——&gt;imge_feat、grid_feat ——&gt;</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>spark</title>
    <link href="/2024/08/21/spark/"/>
    <url>/2024/08/21/spark/</url>
    
    <content type="html"><![CDATA[<h1 id="spark"><a href="#spark" class="headerlink" title="spark"></a>spark</h1><h2 id="框架模块"><a href="#框架模块" class="headerlink" title="框架模块"></a>框架模块</h2><p>spark core：核心RDD</p><p>spark sql：针对结构化数据，本身针对离线计算。</p><p>spark streaming： 在core基础上，流式计算</p><h2 id="运行模式"><a href="#运行模式" class="headerlink" title="运行模式"></a>运行模式</h2><p>本地模式：local，以一个独立进程，通过其内部多个线程模拟spark（开发，测试）</p><p>standalone：集群，各个角色以独立进程的形式存在 </p><p>YARN：集群，角色运行在YARN的容器内，</p><p>Kubernets：在Kubernets容器内</p><p>云服务：在云平台上运行</p><h2 id="架构角色"><a href="#架构角色" class="headerlink" title="架构角色"></a>架构角色</h2><p>YARN</p><p>resoumanger:集群管理</p><p>nodemanger：单机管理</p><p>applicationmaster：单任务管理者</p><p>task：单任务执行</p><p><img src="/2024/08/21/spark/image-20240821164007378.png" alt="image-20240821164007378"></p><p>spark</p><p>master:集群资源管家</p><p>worker：单机资源管家</p><p>Driver：管理单任务 （在local模式在，既管理又干活）</p><p>Executor：运行单任务</p><h3 id="local模式"><a href="#local模式" class="headerlink" title="local模式"></a>local模式</h3><p>local[N] 指定N个线程模拟</p><p>local[*] 对线程数无限制，根据cpu的cores设定</p><p><img src="/2024/08/21/spark/image-20240821164406582.png" alt="image-20240821164406582"></p><h3 id="sparkcount"><a href="#sparkcount" class="headerlink" title="sparkcount"></a>sparkcount</h3><p>创建sparkconf对象</p><p>通过sparkcontext创建对象</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">conf = SparkConf(<span class="hljs-string">&quot;local[*]&quot;</span>).setAppName(<span class="hljs-string">&quot;WordCountHelloWorld&quot;</span>)<br><span class="hljs-comment"># 通过 SparkConf 对象构建 SparkContext 对象</span><br>sc = SparkContext(conf=conf)<br></code></pre></td></tr></table></figure><p>读文件</p><ul><li><p>读本地的文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">file_rdd = sc.textFile(<span class="hljs-string">&quot;./data/input/word.tx&quot;</span>)<br><span class="hljs-comment"># &quot;file:///tmp/pycharm_project_846/data/input/word.txt&quot; linux上文件</span><br></code></pre></td></tr></table></figure></li><li><p>读HDFS上的文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">file_rdd = sc.textFile(<span class="hljs-string">&quot;hdfs://node1:8020/input/word.txt&quot;</span>)<br></code></pre></td></tr></table></figure></li></ul><h3 id="wordcount原理"><a href="#wordcount原理" class="headerlink" title="wordcount原理"></a>wordcount原理</h3><p><img src="/2024/08/21/spark/image-20240822085308917.png" alt="image-20240822085308917"></p><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>RDD：弹性（数据讯息再内存或磁盘中）分布式（跨进程存储）数据集合，分布式框架下的统一数据抽象对象</p><p>特性：有分区；计算方法都会作用再每一个分片上；RDD之间又相互依赖关系；KV型RDD可以有分区器；分区数据的读取尽量靠近数据所在服务器</p><p>一份RDD本质分成了多个分区：</p><p><img src="/2024/08/21/spark/image-20240822095233696.png" alt="image-20240822095233696"></p><p>KV型RDD：RDD内存存储的数据式二元元组(“hadoop”,3)(“hadoop”,1)(“flink”,3)</p><p>默认分区器：hash分区器 </p><h3 id="编程操作"><a href="#编程操作" class="headerlink" title="编程操作"></a>编程操作</h3><p>程序入口：sparkcontext对象</p><p>RDD创建：</p><ul><li><p>并行化集合创建（本地转分布式RDD）；</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">conf = SparkConf().setMaster(<span class="hljs-string">&quot;local[*]&quot;</span>).setAppName(<span class="hljs-string">&quot;test&quot;</span>)<br>    sc = SparkContext(conf=conf)<br>    <span class="hljs-comment"># 本地转分布式RDD对象</span><br>    rdd = sc.parallelize([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>,<span class="hljs-number">9</span>],<span class="hljs-number">3</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;分区：&quot;</span>,rdd.getNumPartitions())<br>    <span class="hljs-comment"># 把RDD每个分区的对象collect到driver</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;RDD内容：&quot;</span>, rdd.collect())<br></code></pre></td></tr></table></figure></li><li><p>读取外部文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 读取本地文件</span><br>    file_rdd = sc.textFile(<span class="hljs-string">&quot;../../data/input/word.txt&quot;</span>)<br>    <span class="hljs-built_in">print</span>(file_rdd.getNumPartitions())<br>    <span class="hljs-built_in">print</span>(file_rdd.collect())<br>    file_rdd2 = sc.textFile(<span class="hljs-string">&quot;../../data/input/word.txt&quot;</span>,<span class="hljs-number">3</span>)<br>    <span class="hljs-built_in">print</span>(file_rdd2.getNumPartitions())<br>    file_rdd3 = sc.textFile(<span class="hljs-string">&quot;../../data/input/word.txt&quot;</span>, <span class="hljs-number">100</span>)<br>    <span class="hljs-built_in">print</span>(file_rdd3.getNumPartitions())<br><br>    <span class="hljs-comment"># 读取HDFS</span><br>    file_rdd4 = sc.textFile(<span class="hljs-string">&quot;hdfs://node1:8020/input/word.txt&quot;</span>)<br>    <span class="hljs-built_in">print</span>(file_rdd4.collect())<br><br>    <span class="hljs-comment"># 针对小文件</span><br>    rdd = sc.wholeTextFiles(<span class="hljs-string">&quot;../../data/input/tiny_files&quot;</span>)<br>    <span class="hljs-built_in">print</span>(rdd.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">1</span>]).collect())<br></code></pre></td></tr></table></figure></li></ul><p>RDD算子：分布式集合上的API</p><ul><li><p>Transformation转换算子 RDD——&gt;RDD</p><p>构建执行计划，对RDD迭代，无Action不干活</p><ol><li><p>map算子（接收处理函数）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-built_in">print</span>(rdd.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: x*<span class="hljs-number">10</span>).collect())<br></code></pre></td></tr></table></figure></li><li><p>flatmap算子（先map 再解除嵌套）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">rdd = sc.parallelize([<span class="hljs-string">&quot;hadoop spark map&quot;</span>, <span class="hljs-string">&quot;spark flink&quot;</span>, <span class="hljs-string">&quot;hadoop spark&quot;</span>])<br><span class="hljs-built_in">print</span>(rdd.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: (x.split(<span class="hljs-string">&quot; &quot;</span>))).collect())<br><span class="hljs-built_in">print</span>(rdd.flatMap(<span class="hljs-keyword">lambda</span> x: (x.split(<span class="hljs-string">&quot; &quot;</span>))).collect())<br></code></pre></td></tr></table></figure></li><li><p>reducebykey （自动按照key分组，完成组内数据的聚合）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 传入的参数只是对value</span><br>rdd = sc.parallelize([[<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-number">1</span>], [<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-number">1</span>], [<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-number">1</span>], [<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-number">2</span>]])<br><span class="hljs-built_in">print</span>(rdd.reduceByKey(<span class="hljs-keyword">lambda</span> a,b: a+b).collect())<br></code></pre></td></tr></table></figure></li><li><p>mapvalues 只针对value的map算子</p></li></ol>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">rdd = sc.parallelize([(<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-number">1</span>)])<br><span class="hljs-built_in">print</span>(rdd.mapValues(<span class="hljs-keyword">lambda</span> value: value*<span class="hljs-number">10</span>).collect())<br></code></pre></td></tr></table></figure><ol start="5"><li>groupby （确定按照k还是v分组）</li></ol>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">rdd = sc.parallelize([(<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-number">1</span>), (<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-number">1</span>),(<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-number">4</span>), (<span class="hljs-string">&#x27;b&#x27;</span>,<span class="hljs-number">4</span>), (<span class="hljs-string">&#x27;c&#x27;</span>,<span class="hljs-number">6</span>)])<br><span class="hljs-built_in">print</span>(rdd.groupBy(<span class="hljs-keyword">lambda</span> x:x[<span class="hljs-number">0</span>]).<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x:(x[<span class="hljs-number">0</span>], <span class="hljs-built_in">list</span>(x[<span class="hljs-number">1</span>]))).collect())<br><br></code></pre></td></tr></table></figure><ol start="6"><li>Filter （过滤数据，结果为True的保留）</li></ol>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">rdd = sc.parallelize([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>])<br>result = rdd.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> x: x %<span class="hljs-number">2</span> == <span class="hljs-number">1</span>)<br><span class="hljs-built_in">print</span>(result.collect())<br></code></pre></td></tr></table></figure><ol start="7"><li>distinct（去重）</li></ol>   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">rdd = sc.parallelize([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br><span class="hljs-built_in">print</span>(rdd.distinct().collect())<br></code></pre></td></tr></table></figure><ol start="8"><li><p>union（2个RDD合并成一个）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">rdd = sc.parallelize([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>])<br>   <br>rdd2 = sc.parallelize([<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-string">&#x27;v&#x27;</span>])<br>rdd3 = rdd.union(rdd2)<br><span class="hljs-built_in">print</span>(rdd3.collect())<br></code></pre></td></tr></table></figure></li><li><p>join （对两个RDD执行JOIN操作 只用于二元组， 内连接，左外，右外）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">rdd = sc.parallelize([(<span class="hljs-number">1001</span>, <span class="hljs-string">&#x27;zhangsan&#x27;</span>), (<span class="hljs-number">1002</span>, <span class="hljs-string">&#x27;lisi&#x27;</span>),<br>                          (<span class="hljs-number">1003</span>, <span class="hljs-string">&#x27;wangwu&#x27;</span>), (<span class="hljs-number">1004</span>, <span class="hljs-string">&#x27;laoliu&#x27;</span>)]<br>                         )<br>rdd2 = sc.parallelize([(<span class="hljs-number">1001</span>, <span class="hljs-string">&#x27;科技部&#x27;</span>), (<span class="hljs-number">1002</span>,<span class="hljs-string">&#x27;教育部&#x27;</span>)])<br><span class="hljs-comment"># 按照key关联</span><br>rdd3 = rdd.join(rdd2)<br><span class="hljs-built_in">print</span>(rdd3.collect())<br><span class="hljs-built_in">print</span>(rdd.leftOuterJoin(rdd2).collect())<br><span class="hljs-built_in">print</span>(rdd.rightOuterJoin(rdd2).collect())<br></code></pre></td></tr></table></figure></li><li><p>intersection （求两个RDD的交集）</p><p>rdd.intersection(rdd2)</p></li><li><p>glom （将RDD数据加上嵌套， 这个嵌套按照分区处理）</p><p>rdd.glom().collect()</p></li><li><p>groupbykey（针对KV型，自动按照Key分组）</p></li><li><p>sortby（对RDD数据进行排序）</p></li></ol><pre><code class="hljs">rdd.sortby(func, ascending=True)<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">rdd = sc.parallelize([(<span class="hljs-string">&#x27;a&#x27;</span>, <span class="hljs-number">1</span>), (<span class="hljs-string">&#x27;a&#x27;</span>,<span class="hljs-number">3</span>), (<span class="hljs-string">&#x27;c&#x27;</span>, <span class="hljs-number">5</span>), (<span class="hljs-string">&#x27;b&#x27;</span>, <span class="hljs-number">2</span>)])<br><span class="hljs-built_in">print</span>(rdd.sortBy(<span class="hljs-keyword">lambda</span> x :x[<span class="hljs-number">0</span>], ascending=<span class="hljs-literal">True</span>, numPartitions=<span class="hljs-number">1</span>).collect())<br><br></code></pre></td></tr></table></figure></code></pre><ol start="14"><li><p>sortbykey（对KV型的，按Key排序）</p><p>sortbykey(ascending&#x3D;True, numparttions, func)</p><p>func： 再排序前对数据key处理</p></li><li><p>mappartitions （对每个分区只有一次IO）结果与map无差异</p><p>func里面传入的是迭代器对象</p></li><li><p>partitionby （自定义分区操作）</p></li><li><p>repartition（对RDD分区重新分区，仅改变数量，不约定新分区规则）</p></li></ol><p><strong>案例</strong>：对订单数据，提取背景的数据，组合北京合商品类别进行输出，并对结果进行去重，得到北京售卖的商品类别信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python">rdd = sc.textFile(<span class="hljs-string">&quot;../../data/input/order.text&quot;</span>)<br>file_rdd = rdd.flatMap(<span class="hljs-keyword">lambda</span> line: line.split(<span class="hljs-string">&quot;|&quot;</span>))<br>dict_rdd = file_rdd.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> json_dir : json.loads(json_dir))<br><span class="hljs-comment"># 过滤数据</span><br>bj_rdd = dict_rdd.<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> d : d[<span class="hljs-string">&#x27;areaName&#x27;</span>]==<span class="hljs-string">&#x27;北京&#x27;</span>)<br><span class="hljs-comment"># 组合北京合商品类型</span><br>ctg_rdd = bj_rdd.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> d : d[<span class="hljs-string">&#x27;areaName&#x27;</span>] + <span class="hljs-string">&#x27;-&#x27;</span> + d[<span class="hljs-string">&#x27;category&#x27;</span>])<br><span class="hljs-comment"># 去重</span><br>res_rdd = ctg_rdd.distinct()<br><span class="hljs-built_in">print</span>(res_rdd.collect())<br></code></pre></td></tr></table></figure></li><li><p>Action算子 RDD——&gt; not RDD</p><ol><li><p>countBykey（对key进行计数）</p></li><li><p>collect（把RDD各个分区数据收集到driver，形成一个List对象）</p></li><li><p>reduce （对RDD数据集按照传入逻辑聚合）</p></li><li><p>fold （带有初始值的聚合）</p><p>rdd.fold(10, lambda a,b:a+b) 初始值会在分区内 和 分区键聚合 都用到</p></li><li><p>first （取出RDD的第一个元素）</p></li><li><p>take （取RDD的前N个元素） 返回list</p></li><li><p>top （对RDD结果进行降序排序，取前N个）</p></li><li><p>count （RDD中数据量）</p></li><li><p>takesample（随机抽样RDD数据）</p><p>takesample(True， 采样数，随机数种子) True：允许重复取一个数字</p></li><li><p>takeordered 对RDD排序取前N个</p><p>takeordered（前N个， 对排序数据更改）  默认升序</p></li><li><p>foreach （对每个元素进行你想要的操作） +func 由excutor直接输出</p></li><li><p>saveastextfile （将RDD数据写入文件）由excutor直接输出</p></li><li><p>mappartitions （）</p></li><li><p>foreachpartiton （一次处理一整个分区的数据）</p></li></ol><p>流水线的开关</p></li></ul><h3 id="RDD缓存"><a href="#RDD缓存" class="headerlink" title="RDD缓存"></a>RDD缓存</h3><p>RDD是过程数据，老旧没用的会从内存清理</p><p>RDD缓存API：rdd.cache() rdd.persist()</p><h3 id="Spark案例练习"><a href="#Spark案例练习" class="headerlink" title="Spark案例练习"></a>Spark案例练习</h3><p>任务：搜索关键词统计，用户搜索点击统计，搜索时间段统计</p><h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><p>进程内资源共享，一个进程里面的数据发了一份即可</p><p>broadcast &#x3D; sc.broadcast(原数据)</p><p>value &#x3D; broadcast.value</p><p>本地集合对象和分布式对象操作时需要使用广播变量避免多次网络IO</p><p>累加器：</p><p>ac &#x3D; sc.accumalate(0)</p><p>能把每个分区的数据反映到全局变量中</p><h2 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h2><ul><li>概述</li></ul><p>Spark的用于处理<strong>结构化数据</strong>的module</p><p>pandas: dataframe 二维表，单机集合</p><p>sparkcore：RDD 无标准数据结构，分布式集合</p><p>sparksql：datatframe 二维表，分布式集合</p><p>dataset：用于java和scala</p><p>dataframe: 用于python java scala</p><ul><li>sparkSession对象：sparksql的入口对象，也可以用于RDD编程</li><li>dataframe组成：structtype描述表结构。structfield描述一个列的信息， row对象记录一行数据， column记录一列列数据并包含列的信息</li></ul><h3 id="dataframe代码构建"><a href="#dataframe代码构建" class="headerlink" title="dataframe代码构建"></a>dataframe代码构建</h3><ul><li><p>基于RDD</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">rdd = sc.textFile(<span class="hljs-string">&quot;../data/input/sql/people.txt&quot;</span>).\<br>        <span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> line: line.split(<span class="hljs-string">&quot;,&quot;</span>)).\<br>        <span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> line: (line[<span class="hljs-number">0</span>], <span class="hljs-built_in">int</span>(line[<span class="hljs-number">1</span>])))<br><br>        <span class="hljs-comment"># 参数1：rdd 参数2：列名</span><br>        df = spark.createDataFrame(rdd, schema=[<span class="hljs-string">&#x27;name&#x27;</span>, <span class="hljs-string">&#x27;age&#x27;</span>] )<br>        df.printSchema()<br>        <span class="hljs-comment"># n:展示多少数据 truncate:是否对数据进行截断</span><br>        df.show(<span class="hljs-number">20</span>, truncate=<span class="hljs-literal">False</span>)<br><br>        df.createOrReplaceTempView(<span class="hljs-string">&quot;people&quot;</span>)<br>        spark.sql(<span class="hljs-string">&quot;select * from people where age &gt; 30&quot;</span>).show()<br><br></code></pre></td></tr></table></figure></li><li><p>读取外部数据：parquet：SPARK的列式存储文件格式</p></li><li><p>编程操作：</p><ol><li><p>DSL语法 df.where().limit()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># df.select([&quot;id&quot;, &quot;subject&quot;]).show()</span><br><span class="hljs-comment"># df.select(&quot;subject&quot;, &quot;score&quot;).show()</span><br><span class="hljs-comment"># df.select(id_column, subject_column).show()</span><br><br><span class="hljs-comment"># # filter</span><br><span class="hljs-comment"># df.filter(&quot;score &lt; 99&quot;).show()</span><br><span class="hljs-comment"># df.filter(df[&quot;score&quot;] &lt; 99).show()</span><br><span class="hljs-comment">#</span><br><span class="hljs-comment"># # where</span><br><span class="hljs-comment"># df.where(&quot;score &lt; 99&quot;).show()</span><br><span class="hljs-comment"># df.where(df[&quot;score&quot;] &lt; 99).show()</span><br><br><span class="hljs-comment"># groupby 分组 返回值groupeddata，不是df了 得接上聚合mean, max,count</span><br>df.groupBy(<span class="hljs-string">&quot;subject&quot;</span>).count().show()<br>df.groupBy(df[<span class="hljs-string">&quot;subject&quot;</span>]).count().show()<br></code></pre></td></tr></table></figure></li><li><p>SQL语法 spark.sql()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">df.createTempView(<span class="hljs-string">&quot;score&quot;</span>)<br>df.createOrReplaceTempView(<span class="hljs-string">&quot;score_1&quot;</span>)<br>df.createGlobalTempView(<span class="hljs-string">&quot;score_2&quot;</span>)<br><br>spark.sql(<span class="hljs-string">&quot;select subject, count(*) from score group by subject&quot;</span>).show()<br>spark.sql(<span class="hljs-string">&quot;select subject, count(*) from score_1 group by subject&quot;</span>).show()<br>spark.sql(<span class="hljs-string">&quot;select subject, count(*) from global_temp.score_2 group by subject&quot;</span>).show()<br><br></code></pre></td></tr></table></figure></li></ol></li><li><p>wordcount案例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python">rdd = sc.textFile(<span class="hljs-string">&quot;../data/input/word.txt&quot;</span>).\<br>flatMap(<span class="hljs-keyword">lambda</span> x: x.split(<span class="hljs-string">&quot; &quot;</span>)).\<br><span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: [x])<br><br>df = rdd.toDF([<span class="hljs-string">&quot;word&quot;</span>])<br>df.createTempView(<span class="hljs-string">&quot;words&quot;</span>)<br>spark.sql(<span class="hljs-string">&quot;select word, count(*) as cnt from words group by word order by cnt desc &quot;</span>).show()<br><br>df =spark.read.<span class="hljs-built_in">format</span>(<span class="hljs-string">&quot;text&quot;</span>).load(<span class="hljs-string">&quot;../data/input/word.txt&quot;</span>)<br><span class="hljs-comment"># withcolumn：对已存在的列做新操作，返回一个新列</span><br>df2 = df.withColumn(<span class="hljs-string">&quot;value&quot;</span>, F.explode(F.split(df[<span class="hljs-string">&#x27;value&#x27;</span>], <span class="hljs-string">&quot; &quot;</span>)))<br>df2.groupBy(<span class="hljs-string">&quot;value&quot;</span>).count().\<br>withColumnRenamed(<span class="hljs-string">&quot;value&quot;</span>, <span class="hljs-string">&quot;word&quot;</span>).\<br>withColumnRenamed(<span class="hljs-string">&quot;count&quot;</span>, <span class="hljs-string">&quot;cnt&quot;</span>).\<br>orderBy(<span class="hljs-string">&quot;cnt&quot;</span>, ascending=<span class="hljs-literal">False</span>).show()<br></code></pre></td></tr></table></figure></li><li><p>电影评分案例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python">schema= StructType().add(<span class="hljs-string">&quot;user_id&quot;</span>, StringType(), nullable=<span class="hljs-literal">True</span>).add(<span class="hljs-string">&quot;movie_id&quot;</span>, IntegerType(), nullable=<span class="hljs-literal">True</span>).add(<span class="hljs-string">&quot;rank&quot;</span>, IntegerType(), nullable=<span class="hljs-literal">True</span>).add(<span class="hljs-string">&quot;ts&quot;</span>, StringType(), nullable=<span class="hljs-literal">True</span>)<br>df = spark.read.<span class="hljs-built_in">format</span>(<span class="hljs-string">&quot;csv&quot;</span>).option(<span class="hljs-string">&quot;header&quot;</span>, <span class="hljs-literal">False</span>).option(<span class="hljs-string">&quot;sep&quot;</span>, <span class="hljs-string">&quot;\t&quot;</span>).option(<span class="hljs-string">&quot;encoding&quot;</span>, <span class="hljs-string">&quot;utf-8&quot;</span>).schema(schema).load(<span class="hljs-string">&quot;../data/input/sql/u.data&quot;</span>)<br><br>df.groupBy(<span class="hljs-string">&quot;user_id&quot;</span>).avg(<span class="hljs-string">&quot;rank&quot;</span>).withColumnRenamed(<span class="hljs-string">&#x27;avg(rank)&#x27;</span>,<span class="hljs-string">&#x27;avg_rank&#x27;</span>).\<br>withColumn(<span class="hljs-string">&#x27;avg_rank&#x27;</span>, F.<span class="hljs-built_in">round</span>(<span class="hljs-string">&#x27;avg_rank&#x27;</span>, <span class="hljs-number">2</span>)).\<br>orderBy(<span class="hljs-string">&#x27;avg_rank&#x27;</span>, ascending=<span class="hljs-literal">False</span>).show()<br><br>df.createTempView(<span class="hljs-string">&quot;movie&quot;</span>)<br>spark.sql(<span class="hljs-string">&quot;select movie_id, round(avg(rank), 2) as avg_rank from movie group by movie_id order by avg_rank&quot;</span>).show()<br><br><span class="hljs-comment"># 查询大于平均分电影数量</span><br>num = df.<span class="hljs-built_in">filter</span>(df[<span class="hljs-string">&quot;rank&quot;</span>] &gt; df.select(F.avg(df[<span class="hljs-string">&quot;rank&quot;</span>])).first()[<span class="hljs-string">&#x27;avg(rank)&#x27;</span>]).count()<br><span class="hljs-built_in">print</span>(num)<br><br><span class="hljs-comment"># 查询大于3分电影中打分最多次的用户，其平均分</span><br>usr_id = df.where(df[<span class="hljs-string">&quot;rank&quot;</span>]&gt; <span class="hljs-number">3</span>).groupBy(<span class="hljs-string">&quot;user_id&quot;</span>).count().withColumnRenamed(<span class="hljs-string">&#x27;count&#x27;</span>, <span class="hljs-string">&#x27;cnt&#x27;</span>).\<br>orderBy(<span class="hljs-string">&#x27;cnt&#x27;</span>, ascending=<span class="hljs-literal">False</span>).limit(<span class="hljs-number">1</span>).first()[<span class="hljs-string">&quot;user_id&quot;</span>]<br>df.<span class="hljs-built_in">filter</span>(df[<span class="hljs-string">&quot;user_id&quot;</span>]==usr_id).select(F.<span class="hljs-built_in">round</span>(F.avg(df[<span class="hljs-string">&quot;rank&quot;</span>]),<span class="hljs-number">2</span>)).show()<br><br><span class="hljs-comment"># 查询每个用户的最低和最高打分</span><br>df.groupBy(<span class="hljs-string">&#x27;user_id&#x27;</span>).\<br>agg(<br>    F.<span class="hljs-built_in">round</span>( F.avg(<span class="hljs-string">&quot;rank&quot;</span>), <span class="hljs-number">2</span>).alias(<span class="hljs-string">&#x27;avg_rank&#x27;</span>),<br>    F.<span class="hljs-built_in">min</span>(<span class="hljs-string">&#x27;rank&#x27;</span>).alias(<span class="hljs-string">&#x27;min_rank&#x27;</span>),<br>    F.<span class="hljs-built_in">max</span>(<span class="hljs-string">&#x27;rank&#x27;</span>).alias(<span class="hljs-string">&#x27;max_rank&#x27;</span>)<br><br>).show()<br><br><span class="hljs-comment"># 查询评分超过100次的电影的平均分排名top10</span><br>df.groupBy(<span class="hljs-string">&#x27;movie_id&#x27;</span>). agg(<br>    F.count(<span class="hljs-string">&#x27;movie_id&#x27;</span>).alias(<span class="hljs-string">&#x27;cnt&#x27;</span>),<br>    F.<span class="hljs-built_in">round</span>(F.avg(<span class="hljs-string">&#x27;rank&#x27;</span>),<span class="hljs-number">2</span>).alias(<span class="hljs-string">&#x27;avg_rank&#x27;</span>)<br>).where(<span class="hljs-string">&#x27;cnt &gt; 100&#x27;</span> ).orderBy(<span class="hljs-string">&#x27;avg_rank&#x27;</span>, ascending=<span class="hljs-literal">False</span>).\<br>limit(<span class="hljs-number">10</span>).show()<br><br><span class="hljs-comment"># agg : 是groupeddata对象的api，在里面可以写多个聚合</span><br><span class="hljs-comment"># alias: 是column对象api，对列改名</span><br><span class="hljs-comment"># withcolumnRename： 是dataframe的api</span><br><span class="hljs-comment"># orderby:dataframe的api</span><br><span class="hljs-comment"># first: dataframe的API 返回第一个row对象，不再是df</span><br></code></pre></td></tr></table></figure></li><li><p>shuffle分区数目设置</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">spark = SparkSession.builder.appName(<span class="hljs-string">&quot;test&quot;</span>).\<br>        master(<span class="hljs-string">&quot;local[*]&quot;</span>).\<br>        config(<span class="hljs-string">&quot;spark.sql.shuffle.partitions&quot;</span>, <span class="hljs-string">&quot;2&quot;</span>).\<br>        getOrCreate()<br></code></pre></td></tr></table></figure></li><li><p>数据清洗API</p></li><li></li></ul>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>基于SMPL的单目3D人体姿态估计</title>
    <link href="/2024/08/21/%E5%9F%BA%E4%BA%8ESMPL%E7%9A%84%E5%8D%95%E7%9B%AE3D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/"/>
    <url>/2024/08/21/%E5%9F%BA%E4%BA%8ESMPL%E7%9A%84%E5%8D%95%E7%9B%AE3D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/</url>
    
    <content type="html"><![CDATA[<h1 id="基于SMPL的单目3D人体姿态估计——驱动virtual-Avatar"><a href="#基于SMPL的单目3D人体姿态估计——驱动virtual-Avatar" class="headerlink" title="基于SMPL的单目3D人体姿态估计——驱动virtual Avatar"></a>基于SMPL的单目3D人体姿态估计——驱动virtual Avatar</h1><p>输入一段视频——&gt;</p><p>对每一帧—SMPLer—&gt;进行3D形状和姿态估计（$\theta$,$cam$,$\beta$）</p><h2 id="1-SMPLer"><a href="#1-SMPLer" class="headerlink" title="1. SMPLer"></a>1. SMPLer</h2><p>前向传播：</p><p>Step1：提取图片全局特征 img——&gt;(HRNet)——&gt;global_feat</p><p>Step2：计算初步的SMPL参数 global_feat——&gt;（FC+dropout）——&gt;theta, beta, cam ——&gt;SMPL模型——&gt;smpl_joints, 2Djoints</p><p>Step3：融合初始查询向量 beta,theta,cam + global_feat ——&gt;query</p><p>Step4：Transfomer层</p><ul><li>query + feat_list——&gt;(全局多尺度单元)——&gt; global_query</li><li>query[:,:-2] + local_feat_list + 对应的local_spat_list ——&gt; （局部多尺度单元）——&gt; local_query</li><li>0.5(global_query[:,:-2] + local_query) ——&gt; global_query——&gt; (self trans)——&gt; query_embed</li></ul><h3 id="1-1-New-Contributions"><a href="#1-1-New-Contributions" class="headerlink" title="1.1 New Contributions"></a>1.1 New Contributions</h3><ol><li><p>解耦的注意力机制：target-feature  target-target</p><p>相较于全局注意力，把原二次方的计算降低到线性</p></li><li><p>多尺度注意力模块 和 关节感知注意力模块</p></li><li><p>基于参数化的SMPL模型的目标表示，只需要学习人体形状和三维身体旋转参数，使得学习的目标嵌入减少。</p></li></ol><p><img src="/2024/08/21/%E5%9F%BA%E4%BA%8ESMPL%E7%9A%84%E5%8D%95%E7%9B%AE3D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20240815155731933.png" alt="image-20240815155731933"></p><h3 id="1-2-Related-Work"><a href="#1-2-Related-Work" class="headerlink" title="1.2 Related Work"></a>1.2 Related Work</h3><p>GraphCMR（GNNS）、</p><p>SPIN、RSC-Net（CNNS）、</p><p>METRO、Mesh Graphormer（ViT）</p><h2 id="2-Attention"><a href="#2-Attention" class="headerlink" title="2 Attention"></a>2 Attention</h2><p>$h(Q,K,V) &#x3D; softmax(\frac{(QW_q)(KW_k)^T}{\sqrt d})(VW_v)$</p><p>Q：Query查询矩阵 $\in R^{l_Q \times d}$</p><p>K：Key键 $\in R^{l_K \times d}$</p><p>V：Value值 $\in R^{l_K \times d}$</p><p>$h(Q,K,V) \in R^{l_Q \times d}$ 又可看作一个新的Q</p><p>When query, key and value are the same, Eq. 1 is called <strong>self-attention</strong> which we denote as $h_{self}(Q) &#x3D; h(Q, Q, Q)$</p><p>When only key and value are the same while query is different, the operation becomes <strong>cross-attention</strong>, denoted as $h_{cross}(Q, K) &#x3D; h(Q, K, K)$.</p><h3 id="2-1-全局Attention"><a href="#2-1-全局Attention" class="headerlink" title="2.1 全局Attention"></a>2.1 全局Attention</h3><p>$h_{self}(T||F)$  即 Q 是将 目标特征 和 目标嵌入 进行concatenation</p><p>$T||F \in R^{(l_T+l_F, d)}$ 进行self-attention时会造成二次计算 $O((l_F  +l_T)^2)$</p><h3 id="2-2-解耦Attention"><a href="#2-2-解耦Attention" class="headerlink" title="2.2 解耦Attention"></a>2.2 解耦Attention</h3><p>建模特征-特征依赖关系在3D姿态估计中不太重要，因此全局Attention可以改进为一下公式：</p><p>$h_{self}(h_{cross}(T,F))$  复杂度 $O(l_T l_F + l_T^2)$ 避免了$l_F$的二次计算，而是线性的</p><h2 id="3-目标特征的表示"><a href="#3-目标特征的表示" class="headerlink" title="3. 目标特征的表示"></a>3. 目标特征的表示</h2><p>虽然注意解耦策略有效地减轻了计算负担，但较大的lT仍可能阻碍高分辨率特征的利用。</p><p>以往的工作都是回归SMPL的顶点坐标$Y \in R^{N \times 3}$作为目标，但是这里的$N&#x3D;6890$会导致注意力操作的计算量和内存消耗也很大</p><p>为此设计一个基于参数化人体模型SMPL的更紧凑的目标表示是有必要的。</p><p>SMPL是一种灵活且具有表现力的人体模型，已广泛应用于三维人体形状和姿态建模。它由一组姿态参数$θ∈ R^{ H×3}$和一个紧致形状向量$β∈R^{1×10}$来参数化。</p><p>通过$\theta, \beta$ 可以得到3D的身体mesh：$Y \in R^{N \times 3} &#x3D; f_{SMPL }(\theta, \beta)$</p><p>对顶点做线性映射可得到3D关节坐标：$J \in R^{H \times 3} &#x3D; M Y$</p><p>如果有相机参数$C \in R^3$，可通过弱透视投影计算得到2D关节坐标：$\mathcal{J}&#x3D; \Pi_C (J)  $</p><p>故对目标${\theta_i }_{i&#x3D;1}^H, \beta, C$可表示为$\mathcal{T} \in \mathbb{R}^{(H+2) \times d}$</p><h2 id="4-多尺度注意力设计"><a href="#4-多尺度注意力设计" class="headerlink" title="4. 多尺度注意力设计"></a>4. 多尺度注意力设计</h2><h3 id="4-1-结合多尺度特征"><a href="#4-1-结合多尺度特征" class="headerlink" title="4.1 结合多尺度特征"></a>4.1 结合多尺度特征</h3><p>$h_{ms}(\mathcal{T, \mathcal{F}} )&#x3D; \frac{1}{S} \sum_{i&#x3D;1}^{S}h_{cross}(\mathcal{T}, F_i)$</p><p>为每个尺度使用不同的投影权重，输出是所有尺度的平均值</p><h3 id="4-2-多尺度特征位置编码"><a href="#4-2-多尺度特征位置编码" class="headerlink" title="4.2 多尺度特征位置编码"></a>4.2 多尺度特征位置编码</h3><p><img src="/2024/08/21/%E5%9F%BA%E4%BA%8ESMPL%E7%9A%84%E5%8D%95%E7%9B%AE3D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20240815172531617.png" alt="image-20240815172531617"></p><p>我们对目标和特征使用可学习的位置编码，通常采用x + φ的形式，其中φ是一组可学习的参数，表示标记x的位置信息。</p><p>只学习最高尺度的位置嵌入，即ϕ1，而其他尺度的嵌入是通过聚集ϕ1来产生的:</p><p>![image-20240815172749418](D:\blog\基于SMPL的单目3D人体姿态估计——驱动virtual Avatar\image-20240815172749418.png)</p><p>递推可得：$\phi_i &#x3D; f_{pool}^2(\phi _{i-1})$</p><h2 id="5-关节感知注意力"><a href="#5-关节感知注意力" class="headerlink" title="5. 关节感知注意力"></a>5. 关节感知注意力</h2><p><img src="/2024/08/21/%E5%9F%BA%E4%BA%8ESMPL%E7%9A%84%E5%8D%95%E7%9B%AE3D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20240821153416897.png" alt="image-20240821153416897"></p><p>人体关节周围的局部关节状态强烈暗示了邻近身体部位之间的相对旋转。$F_1^{\mathcal{N}(\mathcal{J}_i)}$局部特征是从<strong>最高分辨率</strong>的图像特征（F1）中采样得到的，覆盖了以关节为中心的r×r大小的区域。</p><p>由此，对$\mathcal{T}_i$关节，有如下cross-attention：</p><p>$h_{ja}(\mathcal{T}<em>i, \mathcal{F}) &#x3D; f</em>{soft}(\frac{(QW_q)(KW_k)^T}{\sqrt{d}} + \eta) (F_1^{\mathcal{J}_i}W_v)$</p><p>在softmax函数中加入了一个相对位置编码$η∈ \mathbb{R}^{1 \times r^2} $， 它是从一个可学习的张量中根据$\mathcal{J}_i$与$N (\mathcal{J}_i)$中像素之间的距离进行双线性采样。</p><p>与多尺度注意力结合，取平均值即可。</p><p>$$h_{co}(\mathcal{T}<em>i, \mathcal{F}) &#x3D; \begin{cases} \frac{1}{2}(h</em>{ja}(\mathcal{T}<em>i, \mathcal{F}) + h</em>{ms}(\mathcal{T, \mathcal{F}} ) ), i \leq H \ h_{ms}(\mathcal{T, \mathcal{F}} ), i &gt;H \end{cases}$$</p><p>注意力模块的最终公式：</p><p>$h_{final}(\mathcal{T}, \mathcal{F}) &#x3D; h_{self}(h_{co}(\mathcal{T}, \mathcal{F}))$</p><h2 id="6-整体结构"><a href="#6-整体结构" class="headerlink" title="6. 整体结构"></a>6. 整体结构</h2><p>当前设计的一个重要问题是关节感知注意力依赖于二维关节$J$，这应该是我们算法的一个输出。</p><p>换句话说，我们需要$J$来重构三维人，同时需要三维人来回归$J$。为了避免这个问题，SMPLer提出了一个层次结构来<strong>迭代</strong>地改进二维联合估计和三维重建结果。见下图。</p><p><img src="/2024/08/21/%E5%9F%BA%E4%BA%8ESMPL%E7%9A%84%E5%8D%95%E7%9B%AE3D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20240821160629116.png" alt="image-20240821160629116"></p><p>把b阶段的结果写作$P^b &#x3D; {R_{\theta_1}^b,…,R_{\theta_H}^b, \beta^b, C^b }$，迭代过程：</p><p>$\mathcal{T}^b &#x3D;  f_{TB}^b (\mathcal{T}^{b-1} , P^{b-1}, \mathcal{F})$</p><p>$P^b &#x3D; f_{fusion}(\mathcal{T}^b, P^{b-1}), b &#x3D; 1,2,…,B$</p><p>初始化操作：$\mathcal{T}<em>0 &#x3D; f</em>{global}(F_S) + f_{linear}(P^0)$</p><h2 id="7-损失与评价"><a href="#7-损失与评价" class="headerlink" title="7. 损失与评价"></a>7. 损失与评价</h2><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><ul><li>顶点坐标损失：$w_Y · ||Y-\hat{Y}||_1$</li></ul><ul><li>3D关节损失：$w_J · ||J - \hat{J}||_2 $</li><li>2D关节损失：$w_\mathcal{J} · ||\mathcal{J} - \hat{\mathcal{J}}||_2 $</li><li>旋转正则化项：$w_R · \frac{1}{H} \sum_{i&#x3D;1}^{H} ||R_{\theta_i} - \hat{R_{\theta_i}}||_1$</li></ul><h3 id="评价"><a href="#评价" class="headerlink" title="评价"></a>评价</h3><p><strong>MPJPE</strong>(mean per-joint position error)：$\frac{1}{H} \sum_{i&#x3D;1}^{H}||J_i - \hat{J_i}||_2$，易被缩放，旋转，平移等操作影响。</p><p>PA-MPJPE(Procrustes-aligned mean per-joint position error)：</p><p><img src="/2024/08/21/%E5%9F%BA%E4%BA%8ESMPL%E7%9A%84%E5%8D%95%E7%9B%AE3D%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E4%BC%B0%E8%AE%A1/image-20240820170041042.png" alt="image-20240820170041042"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>HRNet</title>
    <link href="/2024/08/21/HRNet/"/>
    <url>/2024/08/21/HRNet/</url>
    
    <content type="html"><![CDATA[<h1 id="HRNet"><a href="#HRNet" class="headerlink" title="HRNet"></a>HRNet</h1><p>针对2D单一人体姿态估计。对于人体姿态估计问题，现有的基于深度学习的方法分2种：</p><ol><li><strong>regressing</strong>方式：直接预测关键点的坐标位置</li><li><strong>heatmap</strong>方式：针对每个关键点预测一张热力图（出现在每个位置上的分数）</li></ol><p>HRNet（<a href="https://arxiv.org/abs/1902.09212)%E9%87%87%E5%8F%96%E7%9A%84%E4%B9%9F%E6%98%AF%E5%9F%BA%E4%BA%8Eheatmap%E7%9A%84%E6%96%B9%E5%BC%8F%E3%80%82">https://arxiv.org/abs/1902.09212)采取的也是基于heatmap的方式。</a></p><ul><li><strong>并行连接</strong>高分辨率到低分辨率的子网，而不是像大多数现有解决方案那样串行连接。因此，能够<strong>保持高分辨率</strong>，而不是通过一个低到高的过程恢复分辨率，因此预测的热图可能在空间上更精确。（parallel high-to-low resolution subnetworks）</li><li>大多数现有的融合方案都将低层和高层的表示集合起来。HRNet使用<strong>重复的多尺度融合</strong>，利用相同深度和相似级别的低分辨率表示来提高高分辨率表示，反之亦然，从而使得高分辨率表示对于姿态的估计也很充分。multi-resolution subnetworks (multi-scale fusion)</li></ul><h2 id="HRNet网络结构"><a href="#HRNet网络结构" class="headerlink" title="HRNet网络结构"></a>HRNet网络结构</h2><p><img src="/2024/08/21/HRNet/image-20240821150014049.png" alt="image-20240821150014049"></p><p><strong>· Step1</strong>: 首先通过两个卷积核大小为<code>3x3</code>步距为2的卷积层下采样4倍</p><p><strong>· Step2</strong>: 通过<code>layer1</code>模块，重复堆叠<code>Bottleneck</code>，不改变特征层大小，只改变了通道数</p><p><strong>· Step3</strong>: 接着通过<code>Transition1</code>结构，增一个尺度分支。在<code>layer1</code>的输出基础上通过并行两个卷积核大小为<code>3x3</code>的卷积层得到两个不同的尺度分支。（上方s&#x3D;1,不改变特征层大小； 下方s&#x3D;2，进一步下采样）</p><img src="/2024/08/21/HRNet/image-20240819192135837.png" alt="image-20240819192135837" style="zoom:67%;" align="center"><p><strong>· Step4</strong>: 通过<code>stage2</code>模块，对于每个尺度分支，首先通过4个<code>Basic Block</code>，然后融合不同尺度上的信息。</p><p>分支1的输出+分支2的输出<strong>上采样2倍</strong>——&gt;ReLU——&gt;分支1 </p><p>分支2的输出+分支1的输出<strong>下采样2倍</strong>——&gt;ReLU——&gt;分支2</p><p><img src="/2024/08/21/HRNet/image-20240819193145151.png" alt="image-20240819193145151"></p><p><strong>上采样</strong>：先通过 $1 \times 1$的的卷积核，不改变特征层大小，缩减通道数；——&gt;(BN)——&gt;(最近邻插值)——&gt;特征层增大，通道数不变</p><p><img src="/2024/08/21/HRNet/image-20240819202701453.png" alt="image-20240819202701453"></p><p><strong>下采样</strong>：通过$2 \times 2$的卷积核，特征层$h,w$减小，通道数增大——&gt;BN   （Down×2）</p><p>Down×4：通过两个$2 \times 2$的卷积核——&gt;BN</p><p>Down×8：通过3个$2 \times 2$的卷积核——&gt;BN</p><p><img src="/2024/08/21/HRNet/image-20240819203532603.png" alt="image-20240819203532603"></p><p><strong>· Step5</strong>: 在<code>Transition2</code>中在原来的两个尺度分支基础上再新加一个下采样的尺度，注意这里是直接在之前尺度基础上通过一个卷积核大小为<code>3x3</code>步距为2的卷积层得到的。之前的分支（$32 \times 24 \times 64$）做一个下采样即可。</p><img src="/2024/08/21/HRNet/image-20240819204504759.png" alt="image-20240819204504759" style="zoom:67%;" align="center"><p><strong>· Step6</strong>: 在<code>stage3</code>模块，对于每个尺度分支，首先通过4个<code>Basic Block</code>，然后融合不同尺度上的信息。对于每个尺度分支上的输出都是由所有分支上的输出进行融合得到的。</p><img src="/2024/08/21/HRNet/image-20240819204800517.png" alt="image-20240819204800517" style="zoom:80%;" align="center"><p><strong>· Step7</strong>: 在<code>Transition3</code>中，在原来的3个尺度分支基础上再新加一个下采样的尺度，对最后一个分支做一个下采样即可。</p><img src="/2024/08/21/HRNet/image-20240819204936420.png" alt="image-20240819204936420" style="zoom:80%;" align="center"><p><strong>· Step8</strong>: 在<code>stage4</code>模块，对于每个尺度分支，首先通过4个<code>Basic Block</code>，然后融合不同尺度上的信息。（重复2遍）然后，四个分支在通过4个<code>Basic Block</code>，下面三个分支分别进行不同程度的上采样，只返回分辨率最高的分支的输出，再经过一个$1\times 1$，卷积核个数为17的卷积层（COCO有17个关节点），返回最终结果。</p><img src="/2024/08/21/HRNet/image-20240819205528156.png" alt="image-20240819205528156" style="zoom:80%;"><h2 id="损失的计算"><a href="#损失的计算" class="headerlink" title="损失的计算"></a>损失的计算</h2><p>针对每个关键点，我们先生成一张值全为0的heatmap，然后将对应关键点坐标处填充1。以关键点坐标为中心应用一个2D的高斯分布（没有做标准化处理）得到GT。利用这个GT heatmap配合网络预测的heatmap就能计算MSE损失了。</p><h2 id="数据增强"><a href="#数据增强" class="headerlink" title="数据增强"></a>数据增强</h2><p>随机翻转</p><p>随机缩放</p><p>随机水平翻转</p><p>half-body 一定概率对目标裁剪</p><p>要注意再图片缩放过程中人物的比例不变！不要直接简单粗暴的拉伸！</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>这里在SMPL模型的3D姿态估计的背景下实现</p><p>img——&gt;(HRNet)——&gt;feat_list——&gt;(MLP)——&gt;beta,theta,cam——&gt;(smplx)——&gt;2D_joints</p><h3 id="一、前置工作"><a href="#一、前置工作" class="headerlink" title="一、前置工作"></a><strong>一、前置工作</strong></h3><p>&#x3D;&#x3D;BasicBlock&#x3D;&#x3D;: 2个$3 \times 3$的卷积块， 在ResNet18、ResNet34中有使用 </p><img src="/2024/08/21/HRNet/image-20240820094057284.png" alt="image-20240820094057284" style="zoom:67%;" align="center"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">conv3x3</span>(<span class="hljs-params">inchanel,outchanel, stride=<span class="hljs-number">1</span> </span>):<br>    <span class="hljs-comment"># (w,h,i)——&gt;s=1,k=3,p=1——&gt;(w,h,o)只改变通道数</span><br>    <span class="hljs-keyword">return</span> nn.conv2d(inchanel, outchanel, kernel_size=<span class="hljs-number">3</span>, stride=stride, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">BasicBlock</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inchanel, outchanel, stride=<span class="hljs-number">1</span>, downsample=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(BasicBlock, self).__init__()<br>        self.conv1 = conv3x3(inchanel, outchanel, stride) <span class="hljs-comment">#s=1,2</span><br>        self.bn1 = nn.BatchNorm2d(outchanel)<br>        self.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.conv2 = conv3x3(outchanel, outchanel) <span class="hljs-comment"># s=1</span><br>        self.bn2 = nn.BatchNorm2d(outchanel)<br>        self.downsample = downsample <span class="hljs-comment">#if s=2, need downsample</span><br>        self.stride = stride<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        residual = x<br>        out = self.conv1(x)<br>        out = self.bn1(out)<br>        out = self.relu(out)<br>        out = self.conv2(out)<br>        out = self.bn2(out)<br><br>        <span class="hljs-keyword">if</span> self.downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            residual = self.downsample(x)<br><br>        out += residual<br>        <span class="hljs-keyword">return</span> self.relu(out)<br></code></pre></td></tr></table></figure><p>&#x3D;&#x3D;Bottleneck&#x3D;&#x3D;:</p><img src="/2024/08/21/HRNet/image-20240820102828722.png" alt="image-20240820102828722" style="zoom:67%;" align="center"><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Bottleneck</span>(nn.modules):<br>    expansion = <span class="hljs-number">4</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, inchanel, outchanel, stride=<span class="hljs-number">1</span>, downsample=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>(BasicBlock, self).__init__()<br>        self.conv1= nn.Conv2d(inchanel, outchanel, kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn1 = nn.BatchNorm2d(outchanel)<br>        self.relu = nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.conv2 = conv3x3(inchanel, outchanel, stride) <span class="hljs-comment">#s=1,2</span><br>        self.conv3 = conv3x3(outchanel, outchanel*self.expansion) <span class="hljs-comment"># s=1, outchanel扩展</span><br>        self.bn2 = nn.BatchNorm2d(outchanel)<br>        self.bn3 = nn.BatchNorm2d(outchanel*self.expansion)<br>        self.downsample = downsample <span class="hljs-comment">#if s=2, need downsample</span><br>        self.stride = stride<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        residual = x<br>        out = self.conv1(x)<br>        out = self.bn1(out)<br>        out = self.relu(out)<br>        out = self.conv2(out)<br>        out = self.bn2(out)<br>        out = self.relu(out)<br>        out = self.conv3(out)<br>        out = self.bn3(out)<br><br>        <span class="hljs-keyword">if</span> self.downsample <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            residual = self.downsample(x)<br><br>        out += residual<br>        <span class="hljs-keyword">return</span> self.relu(out)<br><br></code></pre></td></tr></table></figure><h3 id="二、HRNet实现"><a href="#二、HRNet实现" class="headerlink" title="二、HRNet实现"></a><strong>二、HRNet实现</strong></h3><p>&#x3D;&#x3D;make_layer&#x3D;&#x3D;： 对每个分支做BasicBlock（通道数不变）或者Bottleneck（通道数*4）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layer</span>(<span class="hljs-params">self, block, planes, blocks, stride=<span class="hljs-number">1</span></span>):<br>        downsample = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> self.inplanes != planes * block.expansion:<br>            downsample = nn.Sequential(<br>                nn.Conv2d(<br>                    self.inplanes, planes*block.expansion,<br>                    kernel_size=<span class="hljs-number">1</span>, stride=stride, bias=<span class="hljs-literal">False</span><br>                ),<br>                nn.BatchNorm2d(planes * block.expansion)<br>            )<br>        layers = []   <br>        layers.append(block(self.inplanes, planes, stride, downsample))<br>        self.inplanes = planes * block.expansion<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>, blocks):<br>            layers.append(block(self.inplanes, planes))<br><br>        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br></code></pre></td></tr></table></figure><p>&#x3D;&#x3D;make_transition_layer&#x3D;&#x3D;: 增一个尺度分支</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_transition_layer</span>(<span class="hljs-params">self,</span><br><span class="hljs-params">                               num_channels_pre_layer, num_channels_cur_layer</span>):<br>        num_branches_cur = <span class="hljs-built_in">len</span>(num_channels_cur_layer)<br>        num_branches_pre = <span class="hljs-built_in">len</span>(num_channels_pre_layer)<br><br>        transition_layers = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_branches_cur):<br>            <span class="hljs-keyword">if</span> i &lt; num_branches_pre:<br>                <span class="hljs-keyword">if</span> num_channels_cur_layer[i] != num_channels_pre_layer[i]:<br>                    transition_layers.append(<br>                        nn.Sequential(<br>                            nn.Conv2d(num_channels_pre_layer[i],<br>                                      num_channels_cur_layer[i],<br>                                      <span class="hljs-number">3</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,bias=<span class="hljs-literal">False</span>),<br>                                      nn.BatchNorm2d(num_channels_cur_layer[i]),<br>                                      nn.ReLU(<span class="hljs-literal">True</span>)<br>                        )<br>                    )<br>                <span class="hljs-keyword">else</span>:<br>                    transition_layers.append(<span class="hljs-literal">None</span>)<br>            <span class="hljs-keyword">else</span>:<br>                conv3x3 = []<br>                <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i+<span class="hljs-number">1</span>-num_branches_pre):<br>                    inchannels = num_branches_pre[-<span class="hljs-number">1</span>]<br>                    outchannels = num_channels_cur_layer[i] \<br>                        <span class="hljs-keyword">if</span> j == i-num_branches_pre <span class="hljs-keyword">else</span> inchannels<br>                    conv3x3.append(<br>                        nn.Sequential(<br>                            nn.Conv2d(<br>                                inchannels, outchannels,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>,bias=<span class="hljs-literal">True</span><br>                            ),<br>                            nn.BatchNorm2d(outchannels),<br>                            nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>                        )<br>                    )<br>                transition_layers.append(nn.Sequential(*conv3x3))<br><br>            <br>        <span class="hljs-keyword">return</span> nn.ModuleList(transition_layers)<br></code></pre></td></tr></table></figure><p>&#x3D;&#x3D;make_fuse_layers&#x3D;&#x3D;  ：融合不同尺度的信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_fuse_layers</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-keyword">if</span> self.num_branches == <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">None</span><br><br>        num_branches = self.num_branches<br>        num_inchannels = self.num_inchannels<br>        fuse_layers = []<br>        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_branches <span class="hljs-keyword">if</span> self.multi_scale_output <span class="hljs-keyword">else</span> <span class="hljs-number">1</span>):<br>            fuse_layer = []<br>            <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_branches):<br>                <span class="hljs-keyword">if</span> j &gt; i:<br>                    fuse_layer.append(<br>                        nn.Sequential(<br>                            nn.Conv2d(<br>                                num_inchannels[j],<br>                                num_inchannels[i],<br>                                <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">0</span>, bias=<span class="hljs-literal">False</span><br>                            ),<br>                            nn.BatchNorm2d(num_inchannels[i]),<br>                            nn.Upsample(scale_factor=<span class="hljs-number">2</span>**(j-i), mode=<span class="hljs-string">&#x27;nearest&#x27;</span>)<br>                        )<br>                    )<br>                <span class="hljs-keyword">elif</span> j == i:<br>                    fuse_layer.append(<span class="hljs-literal">None</span>)<br>                <span class="hljs-keyword">else</span>:<br>                    conv3x3s = []<br>                    <span class="hljs-keyword">for</span> k <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(i-j):<br>                        <span class="hljs-keyword">if</span> k == i - j - <span class="hljs-number">1</span>:<br>                            num_outchannels_conv3x3 = num_inchannels[i]<br>                            conv3x3s.append(<br>                                nn.Sequential(<br>                                    nn.Conv2d(<br>                                        num_inchannels[j],<br>                                        num_outchannels_conv3x3,<br>                                        <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span><br>                                    ),<br>                                    nn.BatchNorm2d(num_outchannels_conv3x3)<br>                                )<br>                            )<br>                        <span class="hljs-keyword">else</span>:<br>                            num_outchannels_conv3x3 = num_inchannels[j]<br>                            conv3x3s.append(<br>                                nn.Sequential(<br>                                    nn.Conv2d(<br>                                        num_inchannels[j],<br>                                        num_outchannels_conv3x3,<br>                                        <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span><br>                                    ),<br>                                    nn.BatchNorm2d(num_outchannels_conv3x3),<br>                                    nn.ReLU(<span class="hljs-literal">True</span>)<br>                                )<br>                            )<br>                    fuse_layer.append(nn.Sequential(*conv3x3s))<br>            fuse_layers.append(nn.ModuleList(fuse_layer))<br><br>        <span class="hljs-keyword">return</span> nn.ModuleList(fuse_layers)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>人体模型SMPL介绍</title>
    <link href="/2024/07/29/%E4%BA%BA%E4%BD%93%E6%A8%A1%E5%9E%8BSMPL%E4%BB%8B%E7%BB%8D/"/>
    <url>/2024/07/29/%E4%BA%BA%E4%BD%93%E6%A8%A1%E5%9E%8BSMPL%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<p>SMPL: A Skinned Multi-Person Linear Model （<a href="https://dl.acm.org/doi/pdf/10.1145/2816795.2818013">SMPL (acm.org)</a>）</p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>SMPL是一种3D人体建模方法，支持人体的各种形状及动作。这些人体模型的形状、姿态都可以被参数化表示。</p><p>下面记录下面特有名词的解释：</p><p>**· **vertex（顶点）：模型由多个小三角形或四边形构成。一个小三角形可看作一个顶点，顶点越多，模型越精细。</p><p><strong>·</strong> Joints（骨骼点）：人体的一些关节点，类似于人体姿态估计的关键点。每个骨骼点都由一个三元组作为参数去控制（）</p><p><strong>·</strong> 骨骼蒙皮（Rig）：建立骨骼点和顶点的关联关系。每个骨骼点会关联许多顶点，并且每一个顶点权重不一样。通过这种关联关系，就可以通过控制骨骼点的旋转向量来控制整个人运动。</p><h1 id="SMPL"><a href="#SMPL" class="headerlink" title="SMPL"></a>SMPL</h1><p>一个3D人体mesh由<strong>6890</strong>个网格顶点和<strong>23</strong>个关节点组成：</p><p><img src="/2024/07/29/%E4%BA%BA%E4%BD%93%E6%A8%A1%E5%9E%8BSMPL%E4%BB%8B%E7%BB%8D/image-20240729165318535.png" alt="image-20240729165318535"></p><p>输入：体型参数$\beta$和姿态参数$\theta$，前者决定人体的高矮胖瘦身材比例等，后者决定人体具体姿态。</p><p>每个人体模型有10个体型参数，以及$3 \times (23 +1 )&#x3D; 72$个姿态参数 （23代表骨架节点数，1是人体中心）</p><p>输出： N 个顶点的坐标，维度为 3N。N: 顶点数（6890）</p><h1 id="SMPLer"><a href="#SMPLer" class="headerlink" title="SMPLer"></a>SMPLer</h1><p>对单目图片进行人体的姿态和形状估计。</p><p><img src="/2024/07/29/%E4%BA%BA%E4%BD%93%E6%A8%A1%E5%9E%8BSMPL%E4%BB%8B%E7%BB%8D/image-20240731190342631.png" alt="image-20240731190342631"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>变量选择</title>
    <link href="/2024/06/29/%E5%8F%98%E9%87%8F%E9%80%89%E6%8B%A9/"/>
    <url>/2024/06/29/%E5%8F%98%E9%87%8F%E9%80%89%E6%8B%A9/</url>
    
    <content type="html"><![CDATA[<h1 id="带有约束条件的高维空间自回归模型的稳健的变量选择"><a href="#带有约束条件的高维空间自回归模型的稳健的变量选择" class="headerlink" title="带有约束条件的高维空间自回归模型的稳健的变量选择"></a>带有约束条件的高维空间自回归模型的稳健的变量选择</h1><p><img src="/2024/06/29/%E5%8F%98%E9%87%8F%E9%80%89%E6%8B%A9/image-20240629212115046.png" alt="image-20240629212115046"></p><p>如何求解参数$\theta$</p><p><img src="/2024/06/29/%E5%8F%98%E9%87%8F%E9%80%89%E6%8B%A9/image-20240629212821871.png" alt="image-20240629212821871"></p><p>优化步3和4：一维非线性问题——Brent方法（以已经得到的三个点（比如从bracketing来的）为基础，构建一个二次曲线，如果迭代目标函数与二次函数相似，那么二次函数的顶点就是极值点；如果不是，这次迭代就变成普通的黄金分点搜索。）</p><p>优化步5：LQA算法</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>计网——网络互连</title>
    <link href="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E4%BA%92%E8%BF%9E/"/>
    <url>/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E4%BA%92%E8%BF%9E/</url>
    
    <content type="html"><![CDATA[<h1 id="网络互连"><a href="#网络互连" class="headerlink" title="网络互连"></a>网络互连</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p>互链：用网络设备把不同网络连接起来使得不同网络中计算机之间可相互通信</p><p>中继系统&#x2F;中间设备：把网络相互连接起来的设备</p><p>中继系统：</p><ol><li>物理层：转发器</li><li>数据链路层：网桥</li><li>网络层中断系统：路由器</li><li>网桥和路由器混合：桥路器</li><li>网络层以上：网关</li></ol><p>网络互连使用 路由器</p><p>直接交付：两个主机在一个网络上，不用经过路由器</p><p>间接交付：两主机不在同一个网络，要经过路由交付</p><h2 id="因特网的网际协议IP"><a href="#因特网的网际协议IP" class="headerlink" title="因特网的网际协议IP"></a>因特网的网际协议IP</h2><p>IP协议：提供无连接的IP数据传输机制，不可靠但是有效</p><p>IP地址（4B）：把每个连接在因特网上的主机分配一个在全世界范围是唯一的32位的标识符</p><h3 id="IP地址编址方法"><a href="#IP地址编址方法" class="headerlink" title="IP地址编址方法"></a>IP地址编址方法</h3><ol><li><p>分类的IP地址</p><p>网络号 + 主机号</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E4%BA%92%E8%BF%9E/image-20240625210916852.png" alt="image-20240625210916852"></p><p>点分十进制记法：128.11.3.31</p><p>127.0.0.0保留使用</p><p>127.0.0.1 指代本地本机</p><p><strong>ARP协议</strong>：广播报文，询问目的主机的IP地址与物理地址映射关系，目的主机响应报文，回答</p><p>IP数据报格式：首部固定部分20字节</p></li><li><p>子网划分</p><p>子网划分：从主机部分拿出若干字节作为子网号</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E4%BA%92%E8%BF%9E/image-20240626153711328.png" alt="image-20240626153711328"><br>子网掩码：找出IP地址中的子网部分</p><p>将 三级IP地址 位&amp; 子网掩码 &#x3D; 划分子网时的网络地址</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E4%BA%92%E8%BF%9E/image-20240626154006849.png" alt="image-20240626154006849"></p><p>路由分组转发算法：</p><ol><li>从分组首部提取目的IP地址D</li><li>把各网络的子网掩码与D逐比特相与，若与相应网络地址匹配，则直接交付，否则，转3</li><li>若 路由表中由目的地址为D的特定主机路由，则把分组传给指明的下一跳路由，否则，转4</li><li>将 路由表 的 每一行 的 子网掩码 与D 位与， 若结果等于改行的目的网络，则将分组传给该行指明的吓一跳路由，否则，转5</li><li>若路由表有默认路由，则把分组传给默认路由，否则，转6</li><li>报告失败</li></ol></li><li><p>构成超网</p><p>无分类的两级编址：IP地址 ::&#x3D; {&lt;网络前缀&gt;, &lt;主机号&gt;}  </p><p>CIDR记法：IP地址&#x2F;网络前缀长度  </p><p>CIDR地址块：网络前缀都相同的连续IP地址组成的</p><p>构成超网：一个CIDR地址块可以表示多个地址</p><p>最长前缀匹配：正在路由表查找时，一个在匹配结果里选择具有最长网络前缀的路由</p></li></ol><h2 id="因特网控制报文协议ICMP"><a href="#因特网控制报文协议ICMP" class="headerlink" title="因特网控制报文协议ICMP"></a>因特网控制报文协议ICMP</h2><p>ICMP作用：允许主机或路由器报告差错情况和提供有关异常情况的报告</p><p>首部：8字节</p><p>分类：</p><ol><li><p>差错报告报文</p><p>报文类型：终点不可达、源点抑制、时间超过、参数问题、改变路由</p><p>不应发送情况：对ICMP差错报文、第一个分片的数据报片的所有后续数据报片都步发送、广播，多播的、特殊地址</p></li><li><p>询问报文</p><p>报文类型：回送请求或回答报文、时间戳请求或回答报文</p></li></ol><p>应用举例：Ping，Tracert</p><h2 id="IP多播"><a href="#IP多播" class="headerlink" title="IP多播"></a>IP多播</h2><p>多播使用组地址——IP使用D类地址支持多播</p><p>224.0.1.0——238.255.255.255</p><p>多播地址只能用于目的地址，而不能用于源地址</p><p>局域网中的多播帧中，只要MAC地址后23位与网卡设置的多播IP地址的后23位相同的，本机网卡才接收</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E4%BA%92%E8%BF%9E/image-20240626163736392.png" alt="image-20240626163736392"></p><p>两个或多个IP多播地址可能映射到同一个MAC多播地址  </p><p>隧道技术：</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E4%BA%92%E8%BF%9E/image-20240626163939578.png" alt="image-20240626163939578"></p><h2 id="虚拟专用网络VPN和网络地址转换NAT"><a href="#虚拟专用网络VPN和网络地址转换NAT" class="headerlink" title="虚拟专用网络VPN和网络地址转换NAT"></a>虚拟专用网络VPN和网络地址转换NAT</h2><p>本地地址（内部地址、专用地址）：仅在机构内部使用的IP地址</p><p>全球地址：全球唯一的IP地址</p><p>因特网的所有路由器对目的地址时专用地址的数据一律步转发</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E7%BD%91%E7%BB%9C%E4%BA%92%E8%BF%9E/image-20240626164420235.png" alt="image-20240626164420235"></p><p>VPN对IP进行二次封装</p><p>NAT：更改了IP地址</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>计网——局域网</title>
    <link href="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E5%B1%80%E5%9F%9F%E7%BD%91/"/>
    <url>/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E5%B1%80%E5%9F%9F%E7%BD%91/</url>
    
    <content type="html"><![CDATA[<h1 id="局域网"><a href="#局域网" class="headerlink" title="局域网"></a>局域网</h1><h2 id="局域网概述"><a href="#局域网概述" class="headerlink" title="局域网概述"></a>局域网概述</h2><h2 id="MAC层"><a href="#MAC层" class="headerlink" title="MAC层"></a>MAC层</h2><ol><li><p>MAC地址（6字节）：</p><p>局域网地址、以太网地址、物理地址，确认网络设备位置的地址</p><p>唯一标识一个网卡</p></li><li><p>MAC帧分类：单播帧（收到的帧的MAC地址与本站NAC地址相同）、广播帧（发给局域网内所有站点全1）、多播帧</p></li><li><p>以太网V2格式MAC帧：</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E5%B1%80%E5%9F%9F%E7%BD%91/image-20240625200107309.png" alt="image-20240625200107309"></p><p>对无效MAC帧，直接丢弃</p></li></ol><h2 id="扩展的局域网"><a href="#扩展的局域网" class="headerlink" title="扩展的局域网"></a>扩展的局域网</h2><ol><li><p>在物理层扩展（使用集线器）</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E5%B1%80%E5%9F%9F%E7%BD%91/image-20240625200542135.png" alt="image-20240625200542135"></p></li><li><p>在数据链路层扩展</p><p>网桥：</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E5%B1%80%E5%9F%9F%E7%BD%91/image-20240625201017826.png" alt="image-20240625201017826"></p><p>多接口网桥——以太网交换机</p></li></ol><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E5%B1%80%E5%9F%9F%E7%BD%91/image-20240625201744754.png" alt="image-20240625201744754"></p><h2 id="WLAN"><a href="#WLAN" class="headerlink" title="WLAN"></a>WLAN</h2><p>WLAN可分为两大类：</p><ol><li>有固定基础设施的 WLAN   </li><li>无固定基础设施的 WLAN</li></ol><p>IEEE 802.11  ：</p><ol><li><p>基本服务集BSS：一个AP+若干移动站STA</p><p>为该 AP 分配一个不超过 32 字节的服务集标识符 SSID （该局域网名称）和 通信信道</p><p>每个 AP 有一个唯一的 48 位 MAC 地址，名称是基本服务集标识符 BSSID  </p></li><li><p>扩展服务集ESS：一个 BSS 可以通过 AP 连接到一个分配系统 DS (Distribution System)，然后再连接到另一个 BSS，构成了一个扩展服务集ESS  </p><p>ESS 也有个标识符，是不超过 32 字符的字符串名字 (不是地址)，叫做扩展服务集标识符 ESSID</p></li></ol><p>建立关联：一个移动站若要加入到一个 BSS，就必须先与某个 AP 建立关联。分 被动扫描 和 主动扫描</p><p>CASM&#x2F;CA协议：增加碰撞避免 CA (Collision Avoidance) ：尽量减少碰撞发生的概率</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E5%B1%80%E5%9F%9F%E7%BD%91/image-20240625203146180.png" alt="image-20240625203146180"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>计网——数据链路层</title>
    <link href="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/"/>
    <url>/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/</url>
    
    <content type="html"><![CDATA[<h1 id="数据链路层协议"><a href="#数据链路层协议" class="headerlink" title="数据链路层协议"></a>数据链路层协议</h1><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><ol><li><p>链路：物理链路，无源的点到点的物理线路段0</p></li><li><p>数据链路：带规程的软硬件+物理链路（逻辑链路）</p></li><li><p>数据链路层协议作用：在不太可靠的数据链路层上实现可靠的数据传输</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/image-20240625160502358.png" alt="image-20240625160502358"></p><p>在发送端将网络层的数据流封装成帧并交给物理层进行传输；<br>在接收端将从物理层得到的帧进行差错检测， 拆封还原成数据流并交给网络层。</p></li></ol><h2 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h2><h3 id="封装成帧"><a href="#封装成帧" class="headerlink" title="封装成帧"></a>封装成帧</h3><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/image-20240625163256738.png" alt="image-20240625163256738"></p><p>关键问题：帧定界，即确定帧的界限  </p><p>方法：</p><ol><li><p>字符计数法</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/image-20240625163431597.png" alt="image-20240625163431597"></p></li><li><p>带字符填充的首尾字符定界法</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/image-20240625163450460.png" alt="image-20240625163450460"></p></li><li><p>使用比特填充的首位标记定界</p><p>帧的起始和结束都用一个特殊的位串:“01111110”，称为标记(Flag)</p></li></ol><h3 id="透明传输"><a href="#透明传输" class="headerlink" title="透明传输"></a>透明传输</h3><p>与传输的数据无关  </p><ol><li><p>面向字符——字节插入法</p><p>在数据中出现的控制字符SOH或EOT前面插入一个转义字符ESC(一个字节， 0x1B)；</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/image-20240625163746999.png" alt="image-20240625163746999"></p></li><li><p>面向比特——0比特插入删除法</p><p>当数据中连续出现5个1即“11111”时就在其后插入”0”比特，即将其转换成“111110”。</p><p><img src="/2024/06/25/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/image-20240625163818107.png" alt="image-20240625163818107"></p></li></ol><h3 id="差错控制技术"><a href="#差错控制技术" class="headerlink" title="差错控制技术"></a>差错控制技术</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>计网——计算机网络概述</title>
    <link href="/2024/06/24/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/"/>
    <url>/2024/06/24/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="计算机网络概述"><a href="#计算机网络概述" class="headerlink" title="计算机网络概述"></a>计算机网络概述</h1><h2 id="计算机网络的形成"><a href="#计算机网络的形成" class="headerlink" title="计算机网络的形成"></a>计算机网络的形成</h2><p>计算机技术与通信技术的结合</p><h3 id="发展阶段"><a href="#发展阶段" class="headerlink" title="发展阶段"></a>发展阶段</h3><ol><li><p>阶段一：电路交换</p><p><img src="/2024/06/24/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/image-20240624201506659.png" alt="image-20240624201506659"></p></li><li><p>阶段二：分组交换</p><p>核心：分组转发 + 动态分配资源</p><p>路由器功能：存储分组+转发分组</p><p>处理过程：把收到的分组暂存，查转发表，找到目的地址应从哪个口转发，把分组送至该端口</p><p>主机功能：为用户进行信息处理，发生分组，接受分组</p><p><img src="/2024/06/24/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/image-20240624201645558.png" alt="image-20240624201645558"></p></li><li><p>阶段三：网络体系结构和协议标准化</p><p>国际标准化组织  ISO ——&gt;开放系统互连参考模型  (OSI&#x2F;RM)</p><p>传输控制协议和网络协议 TCP&#x2F;IP</p></li><li><p>阶段四：Internet技术</p></li></ol><h2 id="计算机网络定义"><a href="#计算机网络定义" class="headerlink" title="计算机网络定义"></a>计算机网络定义</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>以能够<strong>相互共享资源</strong>的方式互连起来的<strong>自治</strong>计算机系统的集合。</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><ol><li><p>早期网络结构</p><p>资源子网（由网络上的主机和终端组成  ） + 通信子网（由传输线和交换单元组成  ）</p><p><img src="/2024/06/24/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/image-20240624203258305.png" alt="image-20240624203258305"></p></li><li><p>现代网络结构</p></li></ol><p><img src="/2024/06/24/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/image-20240624204140538.png" alt="image-20240624204140538"></p><h2 id="计算机网络的拓扑构型"><a href="#计算机网络的拓扑构型" class="headerlink" title="计算机网络的拓扑构型"></a>计算机网络的拓扑构型</h2><p>网络拓扑：通过网中结点与通信线路之间的几何关系表示网络结构，反映出网络中各实体间的结构关系。（在通信子网）</p><p>通信信道类型：</p><ol><li>广播信道：总线型、树型、环型（局域网）</li><li>点-点线路：星型、环型、树型、网状（广域网）</li></ol><h2 id="计算机网络分类"><a href="#计算机网络分类" class="headerlink" title="计算机网络分类"></a>计算机网络分类</h2><ol><li>按传输技术分：广播式网络、点-点式网络</li><li>按网络覆盖范围分：个人区域网、局域网、城域网、广域网</li><li>按使用范围分：专用网、公用网</li></ol><h2 id="性能指标"><a href="#性能指标" class="headerlink" title="性能指标"></a>性能指标</h2><ol><li><p>速率（b&#x2F;s)</p></li><li><p>带宽：数字信道的传送的最高数据率（b&#x2F;s)</p></li><li><p>吞吐量：单位时间通过某网络的数据量(b&#x2F;s)</p></li><li><p>时延：传播时延+传输时延+处理时延+等待时延</p><p><img src="/2024/06/24/%E8%AE%A1%E7%BD%91%E2%80%94%E2%80%94%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E6%A6%82%E8%BF%B0/image-20240624205949493.png" alt="image-20240624205949493"></p></li><li><p>时延带宽积：传播时延*带宽</p></li><li><p>利用率</p></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>应用统计——线性回归</title>
    <link href="/2024/06/22/%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <url>/2024/06/22/%E5%BA%94%E7%94%A8%E7%BB%9F%E8%AE%A1%E2%80%94%E2%80%94%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    
    <content type="html"><![CDATA[<h1 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h1><h2 id="一元线性回归"><a href="#一元线性回归" class="headerlink" title="一元线性回归"></a>一元线性回归</h2>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>贝叶斯统计学</title>
    <link href="/2024/06/19/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1%E5%AD%A6/"/>
    <url>/2024/06/19/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%BB%9F%E8%AE%A1%E5%AD%A6/</url>
    
    <content type="html"><![CDATA[<h1 id="贝叶斯统计学"><a href="#贝叶斯统计学" class="headerlink" title="贝叶斯统计学"></a>贝叶斯统计学</h1><h2 id="统计推断中可用的三种信息"><a href="#统计推断中可用的三种信息" class="headerlink" title="统计推断中可用的三种信息"></a>统计推断中可用的三种信息</h2><ol><li>总体信息</li><li>样本信息</li><li>先验信息</li></ol><h2 id="贝叶斯公式"><a href="#贝叶斯公式" class="headerlink" title="贝叶斯公式"></a>贝叶斯公式</h2><ol><li><p>事件形式</p></li><li><p>密度形式</p><p>$p(\theta|x_1, x_2,…,x_n) &#x3D; \frac{p(\theta ) p (x_1,x_2,…,x_n|\theta)}{\int p(\theta ) p (x_1,x_2,…,x_n|\theta) d\theta} $</p></li><li><p>离散形式</p><p>若$\theta$是离散形式，写作$p(\theta_i|x_1, x_2,…,x_n) &#x3D; \frac{p(\theta_i ) p (x_1,x_2,…,x_n|\theta_i)}{\sum_j p(\theta_j ) p (x_1,x_2,…,x_n|\theta_j) }$</p><p>若$x$也是离散形式，写作$p(x|\theta)$换成$p(X&#x3D;x|\theta)$</p></li></ol><p><strong>后验概率的计算</strong>：</p><p>step1: 假设先验分布$p(\theta)$（一般假设为$U ～ [0,1]$）</p><p>step2: 计算联合分布$h(\theta, x)$</p><p>step3: 计算边际分布$m(x) &#x3D; \int h(\theta,x)d\theta$</p><p>step4: 由贝叶斯公式计算后验概率$p(\theta|x)$</p><p><strong>共轭先验分布</strong>：先验—似然—&gt;后验 （其与先验分布服从同一类分布）</p><p>二项分布的成功概率θ的共轭先验分布是贝塔分布。</p><h1 id="贝叶斯推断"><a href="#贝叶斯推断" class="headerlink" title="贝叶斯推断"></a>贝叶斯推断</h1><h2 id="条件方法："><a href="#条件方法：" class="headerlink" title="条件方法："></a>条件方法：</h2><p>贝叶斯推断的后验信息集三种信息为一体，基于后验分布的统计推断实际上只考虑已出现的数据（样本观察值）而认为<strong>未出现的数据与推断无关</strong>。</p><p>条件方法 和 频率方法 的区别：频率方法要考虑样本空间中所有可能出现的样本，而条件方法认为为出现的数据与推断无关</p><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><ol><li><p>最大后验估计</p></li><li><p>后验中位数估计</p></li><li><p>后验期望估计</p></li><li><p>误差估计</p><p>$MSE(\hat{\theta}_E|x) &#x3D; var(\theta|x) $</p><p>$MSE(\hat{\theta}|x) &#x3D; var(\theta|x) + (\hat{\theta}_E  - \hat{\theta})^2$</p><p>可见，当$\hat{\theta}  &#x3D; \hat{\theta}_E$时，有最小值</p></li></ol><h2 id="区间估计"><a href="#区间估计" class="headerlink" title="区间估计"></a>区间估计</h2><h2 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h2>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数理统计初步</title>
    <link href="/2024/06/18/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%88%9D%E6%AD%A5/"/>
    <url>/2024/06/18/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%88%9D%E6%AD%A5/</url>
    
    <content type="html"><![CDATA[<h1 id="数理统计初步"><a href="#数理统计初步" class="headerlink" title="数理统计初步"></a>数理统计初步</h1><p>数理统计：由样本推断总体情况</p><h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="总体、样品、样本"><a href="#总体、样品、样本" class="headerlink" title="总体、样品、样本"></a>总体、样品、样本</h3><p>总体：统计上要研究的对象的全体 $X$</p><p>样品：从总体中随机抽取的一个个体 $X_i$</p><p>样本：由若跟样品构成的样本 $X_1, X_2,..,X_n$</p><p>对<strong>简单随机样本</strong>（i.i.d)而言，其联合分布函数：</p><h3 id="样本的联合分布"><a href="#样本的联合分布" class="headerlink" title="样本的联合分布"></a>样本的联合分布</h3><h3 id="统计量"><a href="#统计量" class="headerlink" title="统计量"></a>统计量</h3><p>统计量定义</p><p>常用统计量</p><p>定理1</p><h3 id="三大分布"><a href="#三大分布" class="headerlink" title="三大分布"></a>三大分布</h3><p>卡方分布、t分布、F分布</p><p>三大结论</p><h2 id="参数估计"><a href="#参数估计" class="headerlink" title="参数估计"></a>参数估计</h2><h3 id="点估计"><a href="#点估计" class="headerlink" title="点估计"></a>点估计</h3><p>矩估计</p><p>极大似然估计</p><p>评价标准：相合性、无偏性、有效性</p><h3 id="区间估计"><a href="#区间估计" class="headerlink" title="区间估计"></a>区间估计</h3><p>方差已知，求均值的区间估计</p><p>方差未知，求均值的区间估计</p><p>求方差的区间估计</p><p>方差比的区间估计</p><h2 id="假设检验"><a href="#假设检验" class="headerlink" title="假设检验"></a>假设检验</h2><p>双边假设（$\alpha&#x2F;2$）</p><p>单边假设（$\alpha$）</p><p><img src="/2024/06/18/%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1%E5%88%9D%E6%AD%A5/image-20240621211830888.png" alt="image-20240621211830888"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>ResNet18介绍</title>
    <link href="/2024/06/13/ResNet18%E4%BB%8B%E7%BB%8D/"/>
    <url>/2024/06/13/ResNet18%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>神经网络知识点总结</title>
    <link href="/2024/06/11/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/"/>
    <url>/2024/06/11/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h1 id="神经网络知识点总结"><a href="#神经网络知识点总结" class="headerlink" title="神经网络知识点总结"></a>神经网络知识点总结</h1><h2 id="绪论"><a href="#绪论" class="headerlink" title="绪论"></a>绪论</h2><ol><li><p>神经网络，深度学习和人工智能的关系：</p><p>神经网络和深度学习是人工智能的一个子领域，神经网络是一种以神经元为基本单位的模型，深度学习是一类机器学习问题，主要解决贡献度分配问题。</p></li><li><p>人工智能发展的三个阶段：</p><p>符号主义（规则驱动）——&gt;联结主义（数据驱动）——&gt;行为主义（交互驱动）</p></li><li><p>机器学习和深度学习步骤的区别：</p><p>输入——&gt;特征工程——&gt;分类——&gt;输出</p><p>输入——&gt;特征工程+分类——&gt;输出</p></li><li><p>什么是表示学习：通过深度模型学习特征（底层特征——&gt;中层特征——&gt;高层特征）</p></li><li><p>什么是深度学习：构建具有一定深度的模型，可以让模型来自动学习好的特征表示，最终提升预测或识别的准确性。</p></li><li><p>人工神经网络三部分：</p><p>神经元激活规则、网络拓扑结构、学习算法</p></li></ol><h2 id="机器学习概述"><a href="#机器学习概述" class="headerlink" title="机器学习概述"></a>机器学习概述</h2><ol><li><p>什么是机器学习：</p><p>通过算法使得机器能从大量数据中学习规律从而对新样本做决策</p></li><li><p>机器学习的分类：</p><p>有监督学习、无监督学习、（半监督学习）、强化学习</p></li><li><p>机器学习四要素：</p><p>数据、模型、学习准则、优化方法</p></li><li><p>风险函数：</p><p>经验风险：只考虑了损失</p><p>结构风险：损失+惩罚项</p></li></ol><h2 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h2><ol><li><p>交叉熵损失</p><p>二分类：$-\frac{1}{N}\sum_{i&#x3D;1}^{N}(y^ilog(\hat{y}^i) + (1-y^i)log(1-\hat{y}^i ))$</p></li><li><p>均方误差：</p><p>二分类：$-\frac{1}{N}\sum_{i&#x3D;1}^N (y^i - \hat{y}^i)^2$</p></li></ol><h2 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h2><ol><li><p>感知机模型的功能：</p><p>单计算节点感知机能实现逻辑运算“与”，“或“问题，却无法解决线性不可分问题</p><p>两计算层感知机能解决线性不可分问题，实现逻辑运算”异或“</p><p>双隐层感知器足以解决任何复杂分类问题</p></li><li><p>激活函数的性质：</p><p>（1）连续可导的非线性函数</p><p>（2）单增</p><p>（3）函数值和其导数值在一个合适范围内</p></li><li><p>常见的激活函数：</p><p>（1）S型：$\sigma(x)$， $tanh(x)$ 都是饱和函数，会出现梯度消失现象</p><p>（2）斜坡函数：ReLU （在一定程度上缓解梯度消失）， LeakyReLU,  PReLU, ELU, softplus(x)</p><p>（3）符合函数：$swish(x) &#x3D; x \sigma(\beta x) $， $GELU (x )&#x3D; xP(X \leq x)$</p></li><li><p>前馈神经网络的结构：</p><p>（1）层内无连接</p><p>（2）两层之间两两神经元相连</p><p>（3）网络无反馈，信号从输入向输出单向传播，无反馈</p></li><li><p>前馈计算：</p></li><li><p>万能逼近定理：对任意一个函数，可以使用两层网络来近似实现。</p></li><li><p>反向传播计算：</p><p>step1: 正向传播，计算每一层的净输入和激活后的值</p><p>step2：反向传播，计算每一层的误差</p><p>step3：计算参数的导数，更新每一层的参数</p></li></ol><h2 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><ol><li><p>卷积类型：</p><p>窄卷积：步长为1，无填充，输出：</p><p>宽卷积：步长为1，两侧填充，输出：</p><p>等宽卷积：步长为1，两端填充，输出：</p></li><li><p>二维卷积的定义</p></li><li><p>卷积神经网络的结构</p><p>卷积：局部特征提取，参数学习，每个卷积核提取特定模式的特征</p><p>池化：降维，增强感受野，提高平移不变性</p><p>全连接：特征提取到分类的桥梁</p></li><li><p>卷积神经网络的结构特性：</p><p>局部连接、权重共享、空间或时间上的次采样</p></li><li><p>多个卷积核的卷积结构的计算</p></li><li><p>整个卷积网络的结构组织方式</p></li><li><p>resnet结构</p></li></ol><h2 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h2><ol><li>递归神经网络的结构</li><li>递归神经网络正向传播</li><li>RNN梯度消失和梯度爆炸的原因</li><li>LSTM结构</li><li>LSTM能解决梯度消失和梯度爆炸的原因</li></ol><h2 id="网络优化"><a href="#网络优化" class="headerlink" title="网络优化"></a>网络优化</h2><ol><li>BGD</li><li>SGD</li><li>mini-batch GD</li></ol><h2 id="深度生成模型"><a href="#深度生成模型" class="headerlink" title="深度生成模型"></a>深度生成模型</h2><ol><li>生成模型定义</li><li>生成模型和判别模型的区别</li><li>GAN结构（生成器和判别器）</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>深度生成模型</title>
    <link href="/2024/06/10/%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/06/10/%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="深度生成模型"><a href="#深度生成模型" class="headerlink" title="深度生成模型"></a>深度生成模型</h1><h2 id="生成模型"><a href="#生成模型" class="headerlink" title="生成模型"></a>生成模型</h2><p>机器学习的两种范式</p><p><img src="/2024/06/10/%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20240610154818533.png" alt="image-20240610154818533"></p><p>生成模型：用于随机生成可观测数据的模型，是一种密度估计问题。</p><p>包括两个步骤：</p><ol><li>密度估计：分显式密度估计（明确定义模型，直接从样本来估计概率分布）和隐式密度估计（通过拟合模型，使其能够生成符合数据分布的样本）。</li><li>采样</li></ol><p>深度生成模型：</p><p>👉<a href="#VAE">变分自编码器VAE</a></p><p>👉<a href="#GAN">生成对抗网络GAN</a></p><h2 id="变分自编码器"><a href="#变分自编码器" class="headerlink" title="变分自编码器"></a>变分自编码器</h2><h3 id="自编码器"><a href="#自编码器" class="headerlink" title="自编码器"></a>自编码器</h3><p><span id="VAE">对</span>一组$D$维的样本$x^n \in R^D, 1 \leq n  \leq N$，自编码器将这组数据样本映射到特征空间，得到样本的编码$z^n \in R^M$。每个样本都有对应的一个编码， 并且通过这组编码可以重构出原来的样本集合。</p><p><img src="/2024/06/10/%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20240610160021264.png" alt="image-20240610160021264"></p><p>自编码器的结构可分为两部分：</p><p>（1） 编码器（Encoder）$f: R^D \rightarrow R^M  $  $z &#x3D; f(W^1x + b^1)$</p><p>（2） 解码器（Decoder）$g:R^M \rightarrow R^D$  $x^{‘} &#x3D; g(W^2z + b^2)$</p><p>优化目标： $L &#x3D; \sum_{n &#x3D;1}^N ||x^{(n)} - x^{‘(n)}||^2$</p><p><strong>·</strong> 如果特征空间的维度$M$小于原始空间的维度$D$，自编码器相当于是一种<strong>降维</strong>或者<strong>特征提取</strong>的方法。</p><p><strong>·</strong> 如果令$W^2$等于$W^1$的转置，则称为<strong>捆绑权重</strong>。捆绑权重自编码器的参数更少，因此更容易学习，且捆绑权重还在一定程度上起到正则化的作用。</p><h3 id="变分自编码器-1"><a href="#变分自编码器-1" class="headerlink" title="变分自编码器"></a>变分自编码器</h3><p>变分自编码器是一种深度生成模型，其思想是利用神经网络来分别建模两个复杂的<strong>条件概率密度函数</strong>。</p><ol><li><p>用神经网络来估计概率分布$q(z|x;\phi )$称为推断网络$f_I(x;\phi)$。 推断网络输入为$x$，输出为概率分布$q(z|x;\phi)$</p></li><li><p>用神经网络来估计概率分布$p(x|z;\phi)$称为生成网络$f_G(z;\theta)$。生成网络输入为$z$，输出为概率分布$p(x|z;\theta)$</p><p><img src="/2024/06/10/%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20240610161941447.png" alt="image-20240610161941447"></p></li></ol><h3 id="VAE和AE的区别"><a href="#VAE和AE的区别" class="headerlink" title="VAE和AE的区别"></a>VAE和AE的区别</h3><ol><li>AE的隐变量服从的分布未知，而VAE中的隐变量是假设服从某种分布的（如高斯分布）</li><li>AE的编码器输出为确定的编码，而VAE的编码器和编码器的输出为分布（或分布的参数）</li><li>AE智能从一个$x$，重构出相对应的$\hat{x}$，而VAE可以采样生成新的$z$，从而得到新的$\hat{x}$，即产生不同的新样本</li></ol><h2 id="生成对抗网络（GAN）"><a href="#生成对抗网络（GAN）" class="headerlink" title="生成对抗网络（GAN）"></a>生成对抗网络（GAN）</h2><p><img src="/2024/06/10/%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20240610162904606.png" alt="image-20240610162904606"></p><p>$Z$ : <span id="GAN">服从</span>$p_z$分布的样本集（$p_z$为预先自定义的分布）</p><p>$G$：生成器（三层BP网络）——&gt;模仿原始数据集的分布$p_{data}$</p><p>$G(Z)$：生成器的输出</p><p>$p_g$：生成器的分布</p><p>$X$：原始数据集</p><p>$p_{data}$：原始数据集的分布</p><p>$D$：判别器（三层BP网络）——&gt;判别输入来自与$p_g$还是$p_{data}$</p><p>生成器最大化其对$p_{data}$的<strong>模仿能力</strong></p><p>判别器最大化其对数据是来与$p_g$还是$p_{data}$的<strong>判别能力</strong></p><p><strong>目标函数：</strong> $\underset{G}{\min} \underset{D}{\max} V(G,D) &#x3D; E_{x ～p_{data}(x)}[log(D(X))] + E_{z ～p_{z}(z)}[1- log(D(G(z)))]$</p><p>$D($ *  $)$是判别器输出，标量，即样本*自于$p_{data}$而不是$p_g$的概率</p><p>生成器最大化其对$p_{data}$的模仿能力，即最小化$log(1- D(G(z)))$</p><p><strong>算法：</strong></p><p><img src="/2024/06/10/%E6%B7%B1%E5%BA%A6%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/image-20240610164827734.png" alt="image-20240610164827734"></p><p>for i in 1…n do: (把上述算法的k取1)</p><p><strong>Step1:</strong> 从噪声中采样输入生成器生成出样本${z^{(1)}, z^{(2)},.., z^{(m)}}$</p><p>**Step2:**从真实样本中采样生成样本${x^{(1)}, x^{(2)},…,x^{(m)}}$</p><p>**Step3:**根据目标函数计算器损失，求梯度$\triangledown_{\theta_d} \frac{1}{m} \sum_{i&#x3D;1}^{m} [logD(x^{(i)}) + log(1 - D(G(z^{(i)})))]$，更新判别器的参数$\theta_d$</p><p>**Step4:**再从噪声中采样输入生成器生成出样本${z^{(1)}, z^{(2)},.., z^{(m)}}$</p><p>**Step5:**计算生成器的梯度$\triangledown_{\theta_g} \frac{1}{m} \sum_{i&#x3D;1}^{m} [log(1 - D(G(z^{(i)})))]$，更新生成器的参数$\theta_g$</p><p>end for 判别器收敛即可（判别器无法区分输入样本是来自于生成器还是原始数据集）</p><h1 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h1><ol><li><p>掌握什么是生成模型，以及机器学习的两大范式</p></li><li><p>自编码器和变分自编码器的结构</p></li><li><p>GAN的结构以及算法</p></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>网络优化与正则化</title>
    <link href="/2024/06/09/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <url>/2024/06/09/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="网络优化与正则化"><a href="#网络优化与正则化" class="headerlink" title="网络优化与正则化"></a>网络优化与正则化</h1><h2 id="网络优化"><a href="#网络优化" class="headerlink" title="网络优化"></a>网络优化</h2><p>网络优化的难点：</p><ol><li>不同网络的结构差异大，没有通用的优化算法，超参数多</li><li>非凸优化问题，如何继续参数初始化和逃离局部最优</li><li>梯度消失和梯度爆炸问题</li></ol><p>网络优化的方法</p><p>梯度下降法（GD）</p><ol><li><p>批量梯度下降法（BGD)</p><p>批量梯度下降得到的是一个所有训练数据上的全局最优解，每一次的参数更新都用到了所有的训练数据，如果训练数据非常多的话，执行效率较低。</p><p>$\theta_j^{‘} &#x3D; \theta_j + \frac{1}{m}\sum_{i&#x3D;1}^{m}(y^i - h_\theta(x^i))x_j^i$</p><p>缺点：处理大型数据缓慢，易导致内存溢出； 更新快慢由学习率决定，在非凸曲面中可能会趋于局部最优； </p></li><li><p>随机梯度下降法（SGD）</p><p>利用单个样本的损失函数对θ求偏导得到对应的梯度，来更新θ</p><p>$\theta_j^{‘} &#x3D; \theta_j + (y^i - h_\theta(x^i))x_j^i$</p><p>缺点：噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向，； 当数据较多时，训练缓慢。</p></li><li><p>小批量梯度下降法（Mini-Batch GD）</p><p>利用部分样本的损失函数对θ求偏导得到对应的梯度，来更新θ</p><p>for k &#x3D; 1, 11, 21, ..,,99 do</p><p>$\theta_j^{‘} &#x3D; \theta_j + \frac{1}{10}\sum_{i&#x3D;k}^{k+9}(y^i - h_\theta(x^i))x_j^i$</p><p>优点：能减少参数更新的波动，获得更好和更稳定的收敛</p></li></ol><p><img src="/2024/06/09/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/image-20240609204358288.png" alt="image-20240609204358288"></p><p><strong>优化方法：</strong></p><p><img src="/2024/06/09/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/image-20240610181701007.png" alt="image-20240610181701007"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>循环神经网络</title>
    <link href="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="递归神经网络RNN"><a href="#递归神经网络RNN" class="headerlink" title="递归神经网络RNN"></a>递归神经网络RNN</h2><p>RNN按照时间序列展开：</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609180740711.png" alt="image-20240609180740711"></p><h3 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h3><p>$t$时刻中间隐层输入$s_t &#x3D; Ux_t + Wh_{t-1}$</p><p>$t$​时刻中间隐层输出$h_t &#x3D; f(s_t)$​（其中$f$为sigmoid函数）</p><p>$t$时刻输出层输出$o_t &#x3D; g(Vh_t)$（其中$g$为softmax函数）</p><p>损失函数为$L_t &#x3D; -[y_tlogo_t + (1-y_t)log(1-o_t)]$</p><p>所有时间的损失为$L &#x3D; sum_{t&#x3D;1}^{T}L_t$</p><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>首先对$V$求导，直接得到：</p><p>$\frac{\partial L}{\partial V} &#x3D; \sum_{t&#x3D;1}^T \frac{\partial L_t}{\partial V} &#x3D; \sum_{t&#x3D;1}^{T} \frac{\partial L_t}{\partial o_t} \frac{\partial o_t}{\partial V} &#x3D;  \sum_{t&#x3D;1}^{T} -(\frac{y_t}{o_t} + \frac{y_t-1}{1-o_t}o_t(1-o_t)h_t^T) &#x3D;  \sum_{t&#x3D;1}^{T} (o_t - y_t)h_t^T$</p><p>对$W$求梯度，</p><p>$\frac{\partial L}{\partial W} &#x3D; \sum_{t&#x3D;1}^T \frac{\partial L_t}{\partial W}  &#x3D;\sum_{t&#x3D;1}^T \frac{\partial L_t}{\partial o_t} \sum_{k&#x3D;1}^t \frac{\partial o_t}{\partial h_t} \frac{\partial h_t}{\partial h_k} \frac{\partial h_k}{\partial W} &#x3D; \sum_{t&#x3D;1}^T \sum_{k&#x3D;1}^t \frac{\partial L_t}{\partial o_t}\frac{\partial o_t}{\partial h_t}(\prod_{j&#x3D;k+1}^t \frac{\partial h_j}{\partial h_{j-1}})\frac{\partial h_k}{\partial W}   $</p><p>依此类推出：</p><p>$\frac{\partial L}{\partial U} &#x3D; \sum_{t&#x3D;1}^T \sum_{k&#x3D;1}^t \frac{\partial L_t}{\partial o_t}\frac{\partial o_t}{\partial h_t}(\prod_{j&#x3D;k+1}^t \frac{\partial h_j}{\partial h_{j-1}})\frac{\partial h_k}{\partial U} $ </p><h3 id="弊端"><a href="#弊端" class="headerlink" title="弊端"></a>弊端</h3><p>传统RNN都采用反向传播时间算法（BPTT），随着时间流逝，网络层数增加，会产生梯度消失或者梯度爆炸的问题。</p><p>以$W$的梯度更新举例，使用激活函数假如是$tanh$</p><p>$\frac{\partial h_j}{\partial h_{j-1}}  &#x3D; W^T \odot tanh^{‘}， tanh^{‘} \in [0,1]$</p><p>👉梯度消失：如果$W$也是大于0小于1的数，当$t$很大时，$W^T \odot tanh^{‘} &lt;1 $，连乘起来就会趋于0.</p><p>👉梯度爆炸：如果梯度比较大的话（$\frac{\partial h_j}{\partial h_{j-1}} &gt; 1$），经过多层迭代，又会导致梯度大的不得了，比如$1,01^{100}$。</p><p>梯度消失和爆炸实际上导致了网络只能学习到<strong>短周期的依赖关系</strong>。</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609184920246.png" alt="image-20240609184920246"></p><p>随着时间的推移，对于 t&gt;1 时刻的产生的影响会越来越小，由图中的颜色的深浅代表信号的大小。这种衰减会导致 RNN 无法处理长期依赖。</p><h2 id="LSTM（长短时记忆神经网络Long-short-term-memory"><a href="#LSTM（长短时记忆神经网络Long-short-term-memory" class="headerlink" title="LSTM（长短时记忆神经网络Long short-term memory)"></a>LSTM（长短时记忆神经网络Long short-term memory)</h2><h3 id="与RNN的区别"><a href="#与RNN的区别" class="headerlink" title="与RNN的区别"></a>与RNN的区别</h3><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609185538836.png" alt="image-20240609185538836"></p><p><strong>·</strong> 原始RNN的隐藏层只有一个状态，即$h$，它对于短期输入非常敏感</p><p><strong>·</strong> 再增加一个状态$c$，来保存长期的状态，称为单元状态或者内部记忆单元，记录了当前时刻为止的所有历史信息。</p><h3 id="内部记忆单元C"><a href="#内部记忆单元C" class="headerlink" title="内部记忆单元C"></a>内部记忆单元C</h3><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609190147525.png" alt="image-20240609190147525"></p><ol><li><p>第一个开关，负责控制继续保存内部状态C（遗忘门）</p><p>遗忘门可以保存很久很久之前的信息</p><p>它决定了上一时刻的单元内部状态$c_{t-1}$有多少保留到当前内部时刻内部状态$c_t$</p></li><li><p>第二个开关，负责控制把当前内部候选状态输入到当前状态C（输入门）</p><p>它决定了当前时刻网络的输入$x_t$有多少保存到当前单元内部状态$c_t$</p></li><li><p>第三个开关，负责控制是否把内部状态C作为当前LSTM的输出（输出门）</p><p>它决定了内部状态$c_t$有多少输出到LSTM的当前输出值$h_t$</p></li></ol><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609190718664.png" alt="image-20240609190718664"></p><p><strong>核心思想</strong>：LSTM的关键在于细胞的状态整个(绿色的图表示的是一个cell)，和穿过细胞的那条水平线。</p><p>细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609190919991.png" alt="image-20240609190919991"></p><p>若只有上面的那条水平线是没办法实现添加或者删除信息的。而是通过一种叫做 门（gates） 的结构来实现的。</p><p>门可以实现选择性地让信息通过，主要是通过一个 sigmoid 的神经层 和一个逐点相乘的操作来实现的。</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609191001406.png" alt="image-20240609191001406"></p><h3 id="LSTM的3个门"><a href="#LSTM的3个门" class="headerlink" title="LSTM的3个门"></a>LSTM的3个门</h3><ol><li><p>遗忘门（控制内部记忆单元遗忘哪些历史信息）$f_t$</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609191204577.png" alt="image-20240609191204577"></p><p>$f^t &#x3D; \sigma(W_f \cdot h^{t-1} + U_f \cdot x^t + b_f)$</p></li><li><p>输入门（控制内部记忆单元加入多少新信息）</p><p><strong>Part1</strong> </p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609191533333.png" alt="image-20240609191533333"></p><p>$i^t  &#x3D; \sigma (W_i h^{t-1} + U_ix^t + b_i)$</p><p>$\tilde{c}^t  &#x3D;  tanh(W_c h^{t-1} + U_c x^t + b_c)$</p><p>Step1: 通过输入门的sigmoid层决定加入哪些新信息</p><p>Step2: 再由tanh层通过$X$和$h$值，生成一个候选记忆向量。 </p><p><strong>Part2</strong></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609192027548.png" alt="image-20240609192027548"></p><p>$c^t &#x3D; f^t * c^{t-1} + i^t * \tilde{c}^t$</p><p>新的内部记忆单元包括两部分<br>1 经过遗忘门过滤的旧状态信息<br>2 候选记忆向量与通过输入门决定的 $𝑖^𝑡$的乘积</p></li><li><p>输出门</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609192228601.png" alt="image-20240609192228601"></p><p>$o^t &#x3D; \sigma(W_o h^{t-1} + U_o x^t + b_o)$</p><p>$h^t &#x3D; o^t * tanh(c^t)$</p><p>$c^t$通过tanh函数，将输出信息控制在-1到1之间。</p><p><strong>总结：</strong></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609193920008.png" alt="image-20240609193920008"></p></li></ol><h3 id="缓解梯度消失和爆炸"><a href="#缓解梯度消失和爆炸" class="headerlink" title="缓解梯度消失和爆炸"></a>缓解梯度消失和爆炸</h3><p>由正向传播公式：$c^t &#x3D; f^t * c^{t-1} + i^t * \tilde{c}^t$</p><p>得到 $\frac{\partial c^{t+1}}{\partial c^t} &#x3D; f^t + …$</p><p>可以看到当$f^t &#x3D;1$时，就算其余项很小，梯度仍可以很好的导到上一时刻，此时即使层数较沈也不会发生梯度消失；当$f^t &#x3D; 0$时，即上一时刻的信号不影响当前时刻，梯度也不会传回去。</p><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>误差使用交叉熵函数</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609194737135.png" alt="image-20240609194737135"></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609200610067.png" alt="image-20240609200610067"></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609200711616.png" alt="image-20240609200711616"></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609201125891-17179350868121.png" alt="image-20240609201125891"></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609201155275.png" alt="image-20240609201155275"></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609201223375.png" alt="image-20240609201223375"></p><h2 id="LSTM其他变体"><a href="#LSTM其他变体" class="headerlink" title="LSTM其他变体"></a>LSTM其他变体</h2><ol><li><p>合并遗忘门和输入门 $i_t + f_t$ &#x3D;1</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609201410947.png" alt="image-20240609201410947"></p></li><li><p>GRU</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609201547466.png" alt="image-20240609201547466"></p><p>GRU只有两个门：更新门z和重置门r</p><p>更新门：遗忘多少历史信息和接受多少新信息。</p><p>重置门：候选状态中有多少信息是从历史信息中得到的。</p><p>与LSTM的比较：</p><ol><li>GRU少了一个门，也少了一个细胞状态$c_t$</li><li>GRU只需要重置门来控制是否要保留原来隐藏状态的信息，单步在限制当前信息的传入。</li><li>在 LSTM 中，虽然得到了新的细胞状态$ 𝒄_𝒕$，但是还不能直接输出，而是需要经过一个过滤的处理:；同样，在GRU 中, 虽然我们也得到了新的隐藏状态$\tilde{h}_t$， 但是还不能直接输出，而是通过更新门来控制最后的输出。</li></ol></li></ol><h1 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h1><ol><li>RNN构造</li><li>RNN的梯度消失和梯度爆炸产生原因</li><li>LSTM结构及核心思想</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>卷积神经网络</title>
    <link href="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>卷积经常用在信号处理中，用于计算信号的延迟累积。</p><h3 id="一维卷积"><a href="#一维卷积" class="headerlink" title="一维卷积"></a>一维卷积</h3><p>时刻t收到的信号$y_t$ 为当前时刻产生的信息和以前时刻延迟信息的叠加: $y_t &#x3D; 1 \times x_t + 1&#x2F;2 \times x_{t-1} + 1&#x2F;4 \times x_{t-2}&#x3D; w_1x_t + w_2x_{t-1}+w_3 x_{t-2} &#x3D; \sum_{k&#x3D;1}^{3}w_k x_{t-k+1}$</p><p>$w_k$称为滤波器或者卷积核</p><p>给定一个输入信号序列$x$和滤波器$w$，卷积的输出：$y_t &#x3D; \sum_{k&#x3D;1}^K w_k x_{t-k+1}$</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608183211215.png" alt="image-20240608183211215"></p><p>不同的滤波器来提取信号序列中的不同特性</p><p>👉低通滤波：滤波器$[1&#x2F;3, 1&#x2F;3, 1&#x2F;3]$可以检测信号序列中的低频信息（如均值）</p><p>👉高通滤波：滤波器$[1, -2, 1]$可以检测信号中的高频信息（如边缘）</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608183710865.png" alt="image-20240608183710865"></p><p>引入滑动步长$S$和零填充$P$:</p><p>$S&#x3D;2$即卷积核每隔2步做一次卷积运算</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608183937333.png" alt="image-20240608183937333"></p><p>$P&#x3D;1$即原数据左右两边各填充一个0（避免重要信息的丢失）</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608184031995.png" alt="image-20240608184031995"></p><p><strong>卷积类型</strong>：按照输出长度可分为3类</p><p>$M$为输入序列的长度，$K$为窗口大小</p><ol><li>宽卷积：步长$S&#x3D;1$，两边不补零$P&#x3D;0$，卷积后输出长度为$M-K+1$</li><li>窄卷积：步长$S&#x3D;1$，两边补零$P&#x3D;K-1$，卷积输出长度为$M+K-1$</li><li>等宽卷积：步长$S&#x3D;1$，两端补零$P&#x3D;(K-1)&#x2F;2$，卷积后输出长度为$M$</li></ol><p>对一个步长为$S$，填充为$P$的卷积，输出的长度为：$|\frac{M+2P-K}{S}| + 1$</p><h3 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h3><p>在图像处理中，图像是以二维矩阵的形式输入到神经网络中，因此我们需要二维卷积。</p><p>一个输入信息$X$和滤波器$W$的二维卷积定义为：$Y&#x3D;W * X$</p><p>$y_{ij} &#x3D; \sum_{u&#x3D;1}^{U}\sum_{v&#x3D;1}^{V} w_{uv}x_{i-u+1, j-v+1}$</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608185512974.png" alt="image-20240608185512974"></p><p>步长和填充不同的情况：</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608190629458.png" alt="image-20240608190629458"></p><h2 id="卷积神经网络-1"><a href="#卷积神经网络-1" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>网络结构：</p><p>👉数据输入层（Input layer）</p><p>👉<a href="#convolution">卷积计算层（CONV layer）</a></p><ol><li>局部特征提取</li><li>训练中参数学习</li><li>每个卷积核提取特定模式的特征</li></ol><p>👉ReLU激励层（ReLU layer）</p><p>👉<a href="#pool">池化层（Pooling layer）</a></p><ol><li>降低数据维度，避免过拟合</li><li>增强局部感受野</li><li>提高平移不变性</li></ol><p>👉全连接层（FC layer）：特征提取到分类的桥梁</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608191126721.png" alt="image-20240608191126721"></p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p><span id="convolution">卷积</span>代替全连接层  $h^{l+1} &#x3D; f(w * h^l + b)$</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608191945394.png" alt="image-20240608191945394"><strong>局部连接，权重共享，空间或者时间上的次采样</strong></p><p><strong>卷积与协相关</strong>：</p><p>协相关：核核输入数据对应相乘再求和</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608192448907.png" alt="image-20240608192448907"></p><p>卷积：核先翻转180度，再做协相关</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608192458655.png" alt="image-20240608192458655"></p><p>除非特殊声明，计算结果按照协相关来。</p><p><strong>多个卷积核</strong>：使用多个卷积核能增强卷积层的提取不同特征的能力</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608193340547.png" alt="image-20240608193340547"></p><p>输入$[7 \times 7 \times3] \rightarrow $ 经过卷积核$w_0[3 \times 3 \times 3]+b_0, w_1[3 \times 3 \times 3]+b_1 \rightarrow$ 结果$[3 \times 3 \times 2]$</p><p>(两个卷积核的大小必须一致)</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608194027267.png" alt="image-20240608194027267"></p><p>3通道图像的卷积层：经过多少个卷积核，就会产生多少通道特征图</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608194917125.png" alt="image-20240608194917125"></p><h3 id="池化层或汇聚层（Pooling-Layers）"><a href="#池化层或汇聚层（Pooling-Layers）" class="headerlink" title="池化层或汇聚层（Pooling Layers）"></a>池化层或汇聚层（Pooling Layers）</h3><p>卷积层虽然可以显著减少连接的个数，但是每一个特征映射的神经元个数并没有显著减少。</p><p>下面是最大<span id="pool">池化是</span>的作用方式：</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608195911957.png" alt="image-20240608195911957"></p><p>卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成的</p><p>经典结构如下：一个卷积块为连续$M$个卷积层和$b$个汇聚层。一个卷积网络可以堆叠$N$个连续的卷积块，然后在接着$K$个全连接层。</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608200153512.png" alt="image-20240608200153512"></p><h2 id="误差反向传播"><a href="#误差反向传播" class="headerlink" title="误差反向传播"></a>误差反向传播</h2><h3 id="卷积层的反向传播"><a href="#卷积层的反向传播" class="headerlink" title="卷积层的反向传播"></a>卷积层的反向传播</h3><ol><li>误差的传播</li></ol><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608201429497.png" alt="image-20240608201429497"></p><ol start="2"><li><p>权重梯度的计算</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608201549172.png" alt="image-20240608201549172"></p></li></ol><h3 id="池化层的反向传播"><a href="#池化层的反向传播" class="headerlink" title="池化层的反向传播"></a>池化层的反向传播</h3><p>误差的传播</p><p>对最大池化，下一层的误差项会原封不动的传给上一层对应区块的最大值所对应的神经元，而其他神经元的误差项都是0。（平均池化就是每个误差项系数为1&#x2F;K）</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608201707278.png" alt="image-20240608201707278"></p><h2 id="其他卷积种类"><a href="#其他卷积种类" class="headerlink" title="其他卷积种类"></a>其他卷积种类</h2><h3 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h3><p>通过给卷积核插入“空洞”来变相地增加其大小，以增加输出单元的感受野</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608203139129.png" alt="image-20240608203139129"></p><h3 id="转置卷积-微步卷积"><a href="#转置卷积-微步卷积" class="headerlink" title="转置卷积&#x2F;微步卷积"></a>转置卷积&#x2F;微步卷积</h3><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608203709599.png" alt="image-20240608203709599"></p><h3 id="可分离卷积"><a href="#可分离卷积" class="headerlink" title="可分离卷积"></a>可分离卷积</h3><p>👉空间可分离卷积</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608203931153.png" alt="image-20240608203931153"></p><p>👉深度可分离卷积</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608204002547.png" alt="image-20240608204002547"></p><h2 id="经典卷积网络"><a href="#经典卷积网络" class="headerlink" title="经典卷积网络"></a>经典卷积网络</h2><p><strong>残差网络</strong></p><p>残差网络（Residual Network，ResNet）是通过给非线性的卷积层增加<strong>直连边</strong>的方式来提高信息的传播效率。</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608204534887.png"></p><p><strong>好处：</strong></p><ol><li>能够解决梯度消失问题，适用于深层结构。</li><li>加速了模型训练收敛速度，使得模型更容易地学习到恒等映射（identity mapping）或者近似于恒等映射的变换。</li><li>即学习输入与目标之间的差异，而不是直接学习输出。这样的设计使得网络可以更好地适应复杂的数据分布，提高了网络的表达能力</li></ol><h2 id="CNN的优缺点"><a href="#CNN的优缺点" class="headerlink" title="CNN的优缺点"></a>CNN的优缺点</h2><p><strong>优点：</strong></p><ol><li>共享卷积核，对高维数据处理无压力</li><li>无需手动提取特征，训练好合适的权重，可以获取好的特征</li><li>分类效果好</li></ol><p><strong>缺点：</strong></p><ol><li>需要调参，还需要大样本，训练要GPU</li><li>物理含义不明确</li></ol><h2 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h2><ol><li>卷积网络的三个结构特征：共享权值，局部连接，时间或空间上的次采样</li><li>卷积的定义，以及各式各样的卷积</li><li>卷积网络结构（卷积块的构成+全连接层）</li><li>理解残差网络的机理</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>基于图片生成古诗</title>
    <link href="/2024/06/07/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E5%8F%A4%E8%AF%97/"/>
    <url>/2024/06/07/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E5%8F%A4%E8%AF%97/</url>
    
    <content type="html"><![CDATA[<h1 id="基于图片生成古诗"><a href="#基于图片生成古诗" class="headerlink" title="基于图片生成古诗"></a>基于图片生成古诗</h1><h2 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h2><p>我们在看图说话时，一般都是先看看图片上有什么内容，比如一张图里有山有水还有很多的植物，那我们对这张图的第一印象就是一张风景图，然后我们再根据图片上某一个具体的景物做详细的描述。</p><p>想让机器看图生成古诗，可以表示为如下步骤：</p><ol><li>对图像分类，比如这个图像是风景图，再确切一点，是有湖、骛、荷花的风景图。这样就可以得到这个图的关键词。</li><li>由上一步得到的关键词，通过古诗生成模型，来生成有关图片的古诗。</li></ol><p>下面，将把这个项目分成以下几个部分：图片分类、古诗生成、网页搭建。</p><h3 id="图片分类"><a href="#图片分类" class="headerlink" title="图片分类"></a>图片分类</h3><p>自己重新训练47%准确率</p><p>采用ResNet50预训练模型，微调后准确率在65%左右</p><p>图片——&gt;标签——&gt;关键词</p><p>如：输入图像</p><p><img src="/2024/06/07/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E5%8F%A4%E8%AF%97/image-20240616202651925.png" alt="image-20240616202651925"></p><p>得到类别和具体标签：</p><p><img src="/2024/06/07/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E5%8F%A4%E8%AF%97/image-20240616202721506.png" alt="image-20240616202721506"></p><h2 id="诗句生成"><a href="#诗句生成" class="headerlink" title="诗句生成"></a>诗句生成</h2><h3 id="关键词的构造"><a href="#关键词的构造" class="headerlink" title="关键词的构造"></a>关键词的构造</h3><p>在上一节得到了具体类别后，要建立一个标签（英文）到关键词（字或者词）的映射，根据语义手动构造即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">write_label_key_dict</span>():<br>    fileHandle = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;label_key_dict.txt&#x27;</span>, <span class="hljs-string">&#x27;wb&#x27;</span>)<br>    <span class="hljs-comment"># 图片标签与key_word对应关系</span><br>    l1 = [<span class="hljs-string">&#x27;江海湖&#x27;</span>, <span class="hljs-string">&#x27;鱼游水&#x27;</span>, <span class="hljs-string">&#x27;鱼&#x27;</span>]<br>    l2 = [<span class="hljs-string">&#x27;春花&#x27;</span>, <span class="hljs-string">&#x27;秋落&#x27;</span>, <span class="hljs-string">&#x27;春红&#x27;</span>, <span class="hljs-string">&#x27;春风&#x27;</span>, <span class="hljs-string">&#x27;秋凉&#x27;</span>, <span class="hljs-string">&#x27;枝&#x27;</span>, <span class="hljs-string">&#x27;桃&#x27;</span>]<br>    l3 = [<span class="hljs-string">&#x27;杯&#x27;</span>, <span class="hljs-string">&#x27;盘&#x27;</span>]<br>    l4 = [<span class="hljs-string">&#x27;桃&#x27;</span>, <span class="hljs-string">&#x27;枝&#x27;</span>]<br>    l5 = [<span class="hljs-string">&#x27;金&#x27;</span>, <span class="hljs-string">&#x27;盘&#x27;</span>]<br>    l6 = [<span class="hljs-string">&#x27;飞&#x27;</span>, <span class="hljs-string">&#x27;飞高&#x27;</span>]<br>    l7 = [<span class="hljs-string">&#x27;山&#x27;</span>, <span class="hljs-string">&#x27;野&#x27;</span>]<br>    l8 = [<span class="hljs-string">&#x27;楼&#x27;</span>, <span class="hljs-string">&#x27;高&#x27;</span>, <span class="hljs-string">&#x27;阁&#x27;</span>]<br>    l9 = [<span class="hljs-string">&#x27;树&#x27;</span>, <span class="hljs-string">&#x27;林&#x27;</span>, <span class="hljs-string">&#x27;枝&#x27;</span>]<br>    l10 = [<span class="hljs-string">&#x27;车&#x27;</span>, <span class="hljs-string">&#x27;车路&#x27;</span>]<br>    label_key_dict = &#123;<span class="hljs-string">&#x27;beaver&#x27;</span>: l1, <span class="hljs-string">&#x27;dolphin&#x27;</span>: l1, <span class="hljs-string">&#x27;otter&#x27;</span>: l1, <span class="hljs-string">&#x27;seal&#x27;</span>: l1, <span class="hljs-string">&#x27;aquarium_fish&#x27;</span>: l1, <span class="hljs-string">&#x27;flatfish&#x27;</span>: l1,<br>                        <span class="hljs-string">&#x27;ray&#x27;</span>: l1, <span class="hljs-string">&#x27;shark&#x27;</span>: l1, <span class="hljs-string">&#x27;trout&#x27;</span>: l1, <span class="hljs-string">&#x27;whale&#x27;</span>: l1, <span class="hljs-string">&#x27;orchid&#x27;</span>: l2, <span class="hljs-string">&#x27;poppie&#x27;</span>: l2, <span class="hljs-string">&#x27;rose&#x27;</span>: l2,<br>                        <span class="hljs-string">&#x27;sunflower&#x27;</span>: l2, <span class="hljs-string">&#x27;tulip&#x27;</span>: l2, <span class="hljs-string">&#x27;bottle&#x27;</span>: l3, <span class="hljs-string">&#x27;bowl&#x27;</span>: l3, <span class="hljs-string">&#x27;can&#x27;</span>: l3, <span class="hljs-string">&#x27;cup&#x27;</span>: l3,<br>                        <span class="hljs-string">&#x27;plate&#x27;</span>: l3, <span class="hljs-string">&#x27;apple&#x27;</span>: l4, <span class="hljs-string">&#x27;mushroom&#x27;</span>: l4, <span class="hljs-string">&#x27;orange&#x27;</span>: l4, <span class="hljs-string">&#x27;pear&#x27;</span>: l4, <span class="hljs-string">&#x27;sweet_pepper&#x27;</span>: l4,<br>                        <span class="hljs-string">&#x27;clock&#x27;</span>: l5, <span class="hljs-string">&#x27;computer keyboard&#x27;</span>: l5, <span class="hljs-string">&#x27;lamp&#x27;</span>: l5, <span class="hljs-string">&#x27;telephone&#x27;</span>: l5, <span class="hljs-string">&#x27;television&#x27;</span>: l5,<br>                        <span class="hljs-string">&#x27;bed&#x27;</span>: l5, <span class="hljs-string">&#x27;chair&#x27;</span>: l5, <span class="hljs-string">&#x27;couch&#x27;</span>: l5, <span class="hljs-string">&#x27;table&#x27;</span>: l5, <span class="hljs-string">&#x27;wardrobe&#x27;</span>: l5,<br>                        <span class="hljs-string">&#x27;bee&#x27;</span>: l6, <span class="hljs-string">&#x27;beetle&#x27;</span>: l6, <span class="hljs-string">&#x27;butterfly&#x27;</span>: l6, <span class="hljs-string">&#x27;caterpillar&#x27;</span>: l6, <span class="hljs-string">&#x27;cockroach&#x27;</span>: l6,<br>                        <span class="hljs-string">&#x27;bear&#x27;</span>: l7, <span class="hljs-string">&#x27;leopard&#x27;</span>: l7, <span class="hljs-string">&#x27;lion&#x27;</span>: l7, <span class="hljs-string">&#x27;tiger&#x27;</span>: l7, <span class="hljs-string">&#x27;wolf&#x27;</span>: l7,<br>                        <span class="hljs-string">&#x27;bridge&#x27;</span>: [<span class="hljs-string">&#x27;桥&#x27;</span>], <span class="hljs-string">&#x27;castle&#x27;</span>: l8, <span class="hljs-string">&#x27;house&#x27;</span>: l8, <span class="hljs-string">&#x27;road&#x27;</span>: [<span class="hljs-string">&#x27;路&#x27;</span>, <span class="hljs-string">&#x27;道&#x27;</span>], <span class="hljs-string">&#x27;skyscraper&#x27;</span>: l8,<br>                        <span class="hljs-string">&#x27;cloud&#x27;</span>: [<span class="hljs-string">&#x27;云&#x27;</span>], <span class="hljs-string">&#x27;forest&#x27;</span>: [<span class="hljs-string">&#x27;林&#x27;</span>], <span class="hljs-string">&#x27;mountain&#x27;</span>: [<span class="hljs-string">&#x27;山&#x27;</span>], <span class="hljs-string">&#x27;plain&#x27;</span>: [<span class="hljs-string">&#x27;平&#x27;</span>], <span class="hljs-string">&#x27;sea&#x27;</span>: [<span class="hljs-string">&#x27;海&#x27;</span>],<br>                        <span class="hljs-string">&#x27;camel&#x27;</span>: [<span class="hljs-string">&#x27;沙&#x27;</span>], <span class="hljs-string">&#x27;cattle&#x27;</span>: l7, <span class="hljs-string">&#x27;chimpanzee&#x27;</span>: l7, <span class="hljs-string">&#x27;elephant&#x27;</span>: l7, <span class="hljs-string">&#x27;kangaroo&#x27;</span>: l7,<br>                        <span class="hljs-string">&#x27;fox,&#x27;</span>: l7, <span class="hljs-string">&#x27;porcupine&#x27;</span>: l7, <span class="hljs-string">&#x27;possum&#x27;</span>: l7, <span class="hljs-string">&#x27;raccoon&#x27;</span>: l7, <span class="hljs-string">&#x27;skunk&#x27;</span>: l7,<br>                        <span class="hljs-string">&#x27;crab&#x27;</span>: [<span class="hljs-string">&#x27;鱼&#x27;</span>], <span class="hljs-string">&#x27;lobster&#x27;</span>: [<span class="hljs-string">&#x27;鱼&#x27;</span>], <span class="hljs-string">&#x27;snail&#x27;</span>: [<span class="hljs-string">&#x27;地&#x27;</span>], <span class="hljs-string">&#x27;spider&#x27;</span>: [<span class="hljs-string">&#x27;地&#x27;</span>], <span class="hljs-string">&#x27;worm&#x27;</span>: [<span class="hljs-string">&#x27;地&#x27;</span>],<br>                        <span class="hljs-string">&#x27;baby&#x27;</span>: [<span class="hljs-string">&#x27;儿&#x27;</span>, <span class="hljs-string">&#x27;小子&#x27;</span>], <span class="hljs-string">&#x27;man&#x27;</span>: [<span class="hljs-string">&#x27;夫&#x27;</span>], <span class="hljs-string">&#x27;womam&#x27;</span>: [<span class="hljs-string">&#x27;女&#x27;</span>], <span class="hljs-string">&#x27;boy&#x27;</span>: [<span class="hljs-string">&#x27;郎&#x27;</span>], <span class="hljs-string">&#x27;girl&#x27;</span>: [<span class="hljs-string">&#x27;女&#x27;</span>],<br>                        <span class="hljs-string">&#x27;crocodile&#x27;</span>: l7, <span class="hljs-string">&#x27;dinosaur&#x27;</span>: l7, <span class="hljs-string">&#x27;lizard&#x27;</span>: l7, <span class="hljs-string">&#x27;snake&#x27;</span>: l7, <span class="hljs-string">&#x27;turtle&#x27;</span>: l7,<br>                        <span class="hljs-string">&#x27;hamster&#x27;</span>: l7, <span class="hljs-string">&#x27;mouse&#x27;</span>: l7, <span class="hljs-string">&#x27;rabbit&#x27;</span>: l7, <span class="hljs-string">&#x27;shrew&#x27;</span>: l7, <span class="hljs-string">&#x27;squirrel&#x27;</span>: l7,<br>                        <span class="hljs-string">&#x27;maple_tree&#x27;</span>: l9, <span class="hljs-string">&#x27;oak_tree&#x27;</span>: l9, <span class="hljs-string">&#x27;palm_tree&#x27;</span>: l9, <span class="hljs-string">&#x27;pine_tree&#x27;</span>: l9, <span class="hljs-string">&#x27;willow_tree&#x27;</span>: l9,<br>                        <span class="hljs-string">&#x27;bicycle&#x27;</span>: l10, <span class="hljs-string">&#x27;bus&#x27;</span>: l10, <span class="hljs-string">&#x27;motorcycle&#x27;</span>: l10, <span class="hljs-string">&#x27;pickup truck&#x27;</span>: l10, <span class="hljs-string">&#x27;train&#x27;</span>: l10,<br>                        <span class="hljs-string">&#x27;lawn_mower&#x27;</span>: l10, <span class="hljs-string">&#x27;rocket&#x27;</span>: l10, <span class="hljs-string">&#x27;streetcar&#x27;</span>: l10, <span class="hljs-string">&#x27;tank&#x27;</span>: l10, <span class="hljs-string">&#x27;tractor&#x27;</span>: l10&#125;<br>    pickle.dump(label_key_dict, fileHandle)<br>    fileHandle.close()  <br></code></pre></td></tr></table></figure><h3 id="数据预处理"><a href="#数据预处理" class="headerlink" title="数据预处理"></a>数据预处理</h3><p>样本全部选择的是五言绝句。</p><p>把样本全部处理成形如（[千山鸟飞绝，万径人踪灭，孤舟蓑笠翁，独钓寒江雪]）的形式</p><p>提取诗句中的所有字，统计频率，按照出现次数递减排序，给这些字从0开始编号</p><p>word ——&gt;id ——&gt;word</p><p>由此也可以得到诗句的向量（对于诗句，开头字符BOS定为’[‘（id &#x3D;2），结尾字符EOS定为’]’（id &#x3D;3））都是五言绝句，不需要填充</p><p>使用word2vec对字向量训练，输入一个关键字能得到与其语义相近的关键字</p><p>上一节得到的图像标签”castle”—(label_key_dict)—&gt;[‘楼’, ‘高’, ‘阁’]， 可通过word2vec得到其相近的关键字</p><h3 id="数据输入与输出"><a href="#数据输入与输出" class="headerlink" title="数据输入与输出"></a>数据输入与输出</h3><p>X（Decoder输入：诗句）</p><p>Y（Decoder输出：诗句）</p><p>Z（Encoder输入：关键字构成的向量）<br>下图是X和Y的构造方法</p><p><img src="/2024/06/07/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E5%8F%A4%E8%AF%97/image-20240616205650612.png" alt="image-20240616205650612"></p><p>对关键字向量Z，每个诗句对应固定容量为4的字向量，通过图片得到的标签可再经过word2vec得到与其词义相近的字，组成关键字向量。</p><h3 id="模型构建"><a href="#模型构建" class="headerlink" title="模型构建"></a>模型构建</h3><p>采用transformer模型</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>大模型的高效微调</title>
    <link href="/2024/06/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/"/>
    <url>/2024/06/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/</url>
    
    <content type="html"><![CDATA[<h1 id="参数高效微调"><a href="#参数高效微调" class="headerlink" title="参数高效微调"></a>参数高效微调</h1><h1 id="Prompt"><a href="#Prompt" class="headerlink" title="Prompt-"></a>Prompt-</h1>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数据结构-最小生成树的生成</title>
    <link href="/2024/06/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/"/>
    <url>/2024/06/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/</url>
    
    <content type="html"><![CDATA[<h1 id="最小生成树的生成"><a href="#最小生成树的生成" class="headerlink" title="最小生成树的生成"></a>最小生成树的生成</h1><p>定义：把无向图的全部n个顶点和n-1条边构成的生成树，其边的权值最小的树称为该图的最小生成树。</p><p>实现方法：</p><ol><li>prim方法：从任意一个起点开始，以贪心的策略，每次只选择一个距离当前联通部分最小的点加入生成树，直至所有点都被加入该树为止。</li><li>kruskal方法：首先把所有的边按权重从小到大排序，然后从权重最小的边开始添加，只要不会形成环就添加这条边，直到所有的点都连接起来。</li></ol><h2 id="Prim方法"><a href="#Prim方法" class="headerlink" title="Prim方法"></a>Prim方法</h2><p><img src="/2024/06/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/image-20240605185023203.png" alt="image-20240605185023203"></p><p>首先，我们需要任意选择一个起点，这里选节点1作为起点。dist[1] &#x3D; 0，其余的dist全为0x3f</p><p>然后，从所有节点的dist数组里面找到最小的节点加到联通部分。</p><p>更新所有与该节点相连的点的dist数组。</p><p>一次更新过程如下图所示</p><p><img src="/2024/06/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/image-20240605191803958.png" alt="image-20240605191803958"></p><p>下一次更新：</p><p><img src="/2024/06/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/image-20240605192122070.png" alt="image-20240605192122070"></p><p>下面是代码实现：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;algorithm&gt;</span></span><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">510</span>;<br><span class="hljs-type">int</span> g[N][N];<span class="hljs-comment">//存储图</span><br><span class="hljs-type">int</span> dist[N];<span class="hljs-comment">//存储每个节点到当前联通部分的距离</span><br><span class="hljs-type">int</span> state[N];<span class="hljs-comment">//存储每个点是否被加入联通部分中</span><br><span class="hljs-type">int</span> pre[N];<span class="hljs-comment">//每个点联通时的前置节点</span><br><span class="hljs-type">int</span> n,m;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">getpath</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> res = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//先将所有节点的前置节点置-1</span><br>    <span class="hljs-built_in">memset</span>(pre, <span class="hljs-number">-1</span>, <span class="hljs-built_in">sizeof</span>(pre));<br>    <span class="hljs-built_in">memset</span>(dist, <span class="hljs-number">0x3f</span>, <span class="hljs-built_in">sizeof</span>(dist));<br>    <span class="hljs-comment">//将1节点的距离置0</span><br>    dist[<span class="hljs-number">1</span>] = <span class="hljs-number">0</span>;<br>    <br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> t= <span class="hljs-number">0</span>; t&lt;n; t++)&#123;<br>        <span class="hljs-comment">//查找所有节点中距离联通部分最近的点，加入联通部分</span><br>        <span class="hljs-type">int</span> midist = <span class="hljs-number">0x3f</span>;<br>        <span class="hljs-type">int</span> index = <span class="hljs-number">-1</span>;<br>        <span class="hljs-comment">//设置t=-1是为了保留孤立点</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>; i&lt;=n; i++)&#123;<br>            <span class="hljs-keyword">if</span>((index==<span class="hljs-number">-1</span> || dist[i]&lt;dist[index]) &amp;&amp; state[i]==<span class="hljs-number">0</span>)&#123;<br>                midist = dist[i];<br>                index = i;<br>            &#125;<br>            <br>        &#125;<br>        <span class="hljs-comment">//如果是孤立点 无法产生最小生成树</span><br>        <span class="hljs-keyword">if</span>(dist[index] == <span class="hljs-number">0x3f3f3f3f</span>)&#123;<br>            cout&lt;&lt;<span class="hljs-string">&quot;impossible&quot;</span>&lt;&lt;endl;<br>            <span class="hljs-keyword">return</span>;<br>        &#125;<br>        <span class="hljs-comment">//此时找到index节点，将其加入联通部分</span><br>        state[index] = <span class="hljs-number">1</span>;<br>        <span class="hljs-comment">//cout&lt;&lt;&quot;index: &quot;&lt;&lt;index&lt;&lt;&quot; midist: &quot;&lt;&lt;midist&lt;&lt;endl;</span><br>        res += midist;<br>        <span class="hljs-comment">//更新所有与indx节点相连的点 到联通部分的距离</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>; i&lt;=n; i++)&#123;<br>            <span class="hljs-comment">//cout&lt;&lt;&quot;dist[i]&quot;&lt;&lt;dist[i]&lt;&lt;&quot; g[i][index]&quot;&lt;&lt;g[i][index]&lt;&lt;endl;</span><br>            <span class="hljs-keyword">if</span>(state[i]==<span class="hljs-number">0</span> &amp;&amp; (dist[i] &gt; g[i][index]))&#123;<br>                dist[i] = <span class="hljs-built_in">min</span>(dist[i], g[i][index]);<br>                <span class="hljs-comment">//cout&lt;&lt;&quot;i: &quot;&lt;&lt;i&lt;&lt;&quot;dist[i]&quot;&lt;&lt;dist[i]&lt;&lt;endl;</span><br>                <br>            &#125;<br>        &#125;<br>    <br>    &#125;<br>    cout&lt;&lt;res&lt;&lt;endl;<br>    <br>    <br>    <br>    <br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <br>    <span class="hljs-built_in">memset</span>(g, <span class="hljs-number">0x3f</span>, <span class="hljs-built_in">sizeof</span>(g));<br>    <br>    cin&gt;&gt;n&gt;&gt;m;<br>    <span class="hljs-type">int</span> a,b,w;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;m; i++)&#123;<br>        cin&gt;&gt;a&gt;&gt;b&gt;&gt;w;<br>        g[a][b] = g[b][a] =<span class="hljs-built_in">min</span>(w, g[a][b]);<br>    &#125;<br>    <br>    <span class="hljs-built_in">getpath</span>();<br>    <br>    <br>    <br>&#125;<br></code></pre></td></tr></table></figure><h2 id="kruskal方法"><a href="#kruskal方法" class="headerlink" title="kruskal方法"></a>kruskal方法</h2><p><strong>方法</strong>：将所有边按从小到大的顺序排序，一条一条边的尝试加入结果集，如果正在尝试加入的这条边加入后会产生环路，则放弃该边，选择下一条边，直至选完n-1条边。</p><p><img src="/2024/06/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/image-20240605202553078.png" alt="image-20240605202553078"></p><p><strong>判断环路产生</strong>：使用并查集的方法。使用一个代表数组存储数据，如p&#x3D;[0,0,0,3,3]的意思是原数组a[0]&#x3D;a[1]&#x3D;a[2]，他们的代表元素是a[0]；a[3]&#x3D;a[4],他们的代表元素是a[3]。</p><ol><li><strong>并</strong>：如把[1,2,3]和[4,5]合并，只需要把p数组的p[3]修改成0即可</li><li><strong>查</strong>：修改后，如果想查a[4]，通过查找p数组，发现a[4]&#x3D;a[3], 再查找p数组a[3]&#x3D; a[0]，由此就查到了a[4]&#x3D; a[0]</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs C++"># 找到a的代表元素<br><span class="hljs-meta"># p[0,0,0,1,3] 如果想找到4的代表元素，p[4]= p[3]=p[1]=p[0]</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">find</span><span class="hljs-params">(a)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">if</span>(a!=p[a])<br>    &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">find</span>(p[a]);<br>    &#125;<br>    <span class="hljs-keyword">return</span> p[a];<br>&#125;<br></code></pre></td></tr></table></figure><p>代码实现：</p><p>边的存储：使用结构体struct E{int a,  b,  w;}</p><p>排序边：构造compare(const E &amp; a, const E &amp; b)函数比较</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstring&gt;</span></span><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Edge</span>&#123;<br>    <span class="hljs-type">int</span> a,b,w;<br>&#125;edg[<span class="hljs-number">200010</span>];<br><br><span class="hljs-type">int</span> p[<span class="hljs-number">200010</span>];<br><span class="hljs-type">int</span> n,m;<br><br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">compare</span><span class="hljs-params">(Edge a, Edge b)</span></span>&#123;<br>    <span class="hljs-keyword">return</span> a.w &lt; b.w;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">findp</span><span class="hljs-params">(<span class="hljs-type">int</span> a)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(a != p[a])&#123;<br>        p[a] = <span class="hljs-built_in">findp</span>(p[a]);<br>    &#125;<br>    <span class="hljs-keyword">return</span> p[a];<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">kruskal</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> res = <span class="hljs-number">0</span>;<br>    <span class="hljs-type">int</span> cnt =<span class="hljs-number">0</span> ;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i =<span class="hljs-number">1</span>; i&lt;=m; i++)&#123;<br>        <span class="hljs-type">int</span> pa = <span class="hljs-built_in">findp</span>(edg[i].a);<br>        <span class="hljs-type">int</span> pb = <span class="hljs-built_in">findp</span>(edg[i].b);<br>        <span class="hljs-keyword">if</span>(pa!=pb)&#123;<br>            res += edg[i].w;<br>            p[pa] = pb;<br>            cnt ++;<br>            <br>        &#125;<br>       <br>        <br>    &#125;<br>    <br>    <span class="hljs-keyword">if</span>(cnt&lt;n<span class="hljs-number">-1</span>)&#123;<br>        cout&lt;&lt;<span class="hljs-string">&quot;impossible&quot;</span>&lt;&lt;endl;<br>       <br>    &#125;<br>    <span class="hljs-keyword">else</span> &#123;<br>        cout&lt;&lt;res&lt;&lt;endl;<br>        <br>    &#125;<br>&#125;<br>    <br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin&gt;&gt;n&gt;&gt;m;<br>    <br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>; i&lt;=m; i++)&#123;<br>        p[i] = i;<br>    &#125;<br>    <br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>; i&lt;=m; i++)&#123;<br>        cin&gt;&gt;edg[i].a&gt;&gt;edg[i].b&gt;&gt;edg[i].w;<br>    &#125;<br>    <br>    <span class="hljs-built_in">sort</span>(edg+<span class="hljs-number">1</span>, edg+m+<span class="hljs-number">1</span>, compare);<br>    <br>    <span class="hljs-built_in">kruskal</span>();<br>    <br>    <br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>大语言模型介绍（一）</title>
    <link href="/2024/06/04/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2024/06/04/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="大语言模型"><a href="#大语言模型" class="headerlink" title="大语言模型"></a>大语言模型</h1><h1 id="语言模型的发展历程"><a href="#语言模型的发展历程" class="headerlink" title="语言模型的发展历程"></a>语言模型的发展历程</h1><p>语言模型旨在对于人类语言的内在规律进行建模，从而准确预测 词序列中未来（或缺失）词或词元（Token）的概率。根据所采用技术方法的不同， 针对语言模型的研究工作可以分为以下四个主要发展阶段：</p><ol><li><p>**统计语言模型（SLM)**：统计语言模型使用马尔可夫假设（MarkovAssumption）来建立语言序列的预测模型，通常是根据词序 列中若干个连续的上下文单词来预测下一个词的出现概率，即根据一个固定长度的前缀来预测目标单词。</p><p>如：N-gram模型，<strong>某一个词语出现的概率只由其前面的n−1个词语所决定</strong>。</p><p>$p(S)&#x3D;p(w_1w_2⋯w_n)&#x3D;p(w_1)p(w_2∣w_1)⋯p(w_n∣w_n−1)$</p><p>$p(冰激凌|我要去吃)&#x3D;p(冰激凌|吃)$ </p><p>对于高阶统计语言模型来说，随着阶数𝑛的增加，需要估计的转移概率项数将会指数级增长，经常会受到“维数灾难”（CurseofDimensionality） 的困扰。</p></li><li><p>**神经语言模型（NLM)**：神经语言模型使用神经网络来建模文本序列的生成，如循环神经网络（RecurrentNeuralNetworks,RNN）。通过引入WordEmbedding技术，能够有效克服统计语言模型中的数据稀疏问题，稠密向量的非零表征对于复杂语言模型的搭建非常友好。</p><p>Word2vec 是一个两层神经网络，通过“向量化”单词来处理文本。它的<strong>输入是一个文本语料库</strong>，它的<strong>输出是一组向量</strong>：表示该语料库中单词的特征向量。</p><p><img src="/2024/06/04/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D%EF%BC%88%E4%B8%80%EF%BC%89/image-20240604214251379.png" alt="image-20240604214251379"></p></li><li><p><strong>预训练语言模型（PLM)<strong>：与早期的词嵌入模型相 比，预训练语言模型在</strong>训练架构</strong>与<strong>训练数据</strong>两个方面进行了改进与创新。ELMo使用大量的无标注数据训练双向LST网络预训练完成后所得到的biLSTM可以用来学 习上下文感知的单词表示。进一 步，ELMo可以根据下游任务数据对biLSTM网络进行微调（Fine-Tuning），从而实现面向特定任务的模型优化。在2017 年，谷歌提出了基于自注意力机制（Self-Attention）的Transformer模型，通过自注意力机制建模长程序列关系。基于Transformer架构，谷歌进一步提出了预训练语言模型BERT，采用了仅有编码器的Transformer架构，并通过在大规模无标注数据上使用专门设计的预训练任务来学习双向语言模型。GPT-1采用了仅有解码器的Transformer架构，以及基于下一个词元预测的预训练任务进行模型的训练。</p><p>以ELMo、BERT、GPT-1为代表的预训练语言模型确立了“<strong>预训练-微调</strong>”这一任务求解范式。其中，预训练阶段旨在通过大规模无标注文本建立模型 的基础能力，而微调阶段则使用有标注数据对于模型进行特定任务的适配，从而更好地解决下游的自然语言处理任务。</p><p><img src="/2024/06/04/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D%EF%BC%88%E4%B8%80%EF%BC%89/image-20240604214907303.png" alt="image-20240604214907303"></p></li><li><p><strong>大语言模型（LLM)<strong>： 研究人员发现，通过规模扩展 （如增加模型参数规模或数据规模）通常会带来下游任务的模型性能提升，这种现象通常被称为“</strong>扩展法则</strong>”（ScalingLaw）。一些研究工作尝试训练更大的预 训练语言模型（例如175B参数的GPT-3和540B参数的PaLM）来探索扩展语言 模型所带来的性能极限。这些大规模的预训练语言模型在解决复杂任务时表现出 了与小型预训练语言模型（例如330M参数的BERT和1.5B参数的GPT-2）不同 的行为。</p></li></ol><h2 id="大语言模型的构建过程"><a href="#大语言模型的构建过程" class="headerlink" title="大语言模型的构建过程"></a>大语言模型的构建过程</h2><p>神经网络是一种具有特定模型结构的函数形式，而大语言模型则是一种基于 Transformer 结构的神经网络模型。因此，可以将大语言模型看作一种拥有大规模参数的函数，它的构建过程就是使用训练数据对于模型参数的拟合过程。  大语言模型的优化目标更加泛化，不仅仅是为了解决某一种或者某一类特定任务，而是希望能够<strong>作为通用任务的求解器</strong>。为了实现这一宏大的目标，大语言模型的构建过程需要更为复杂、精细的训练方法。一般来说，这个训练过程可以分为<strong>大规模预训练</strong>和<strong>指令微调与人类对齐</strong>两个阶段 。</p><h3 id="大规模预训练"><a href="#大规模预训练" class="headerlink" title="大规模预训练"></a>大规模预训练</h3><p>预训练是指使用与下游任务无关的大规模数据进行模型参数的初始训练，可以认为是<strong>为模型参数找到一个较好的“初值点”</strong>。 大规模预训练本质上是在做一个世界知识的压缩，从而能够学习到一个<strong>编码世界知识</strong>的参数模型，这个模型能够通过解压缩所需要的知识来解决真实世界的任务。  </p><p>为了预训练大语言模型，需要准备大规模的文本数据，并且进行严格的清洗，去除掉可能包含有毒有害的内容，最后将清洗后的数据进行词元化（Tokenization）流，并且切分成批次（Batch），用于大语言模型的预训练。  </p><h3 id="指令微调与人类对齐"><a href="#指令微调与人类对齐" class="headerlink" title="指令微调与人类对齐"></a>指令微调与人类对齐</h3><p>但是由于预训练任务形式所限，这些模型更擅长于文本补全，并不适合直接解决具体的任务。比较广泛使用的微调技术是“指令微调”（也叫做有监督微调， Supervised Fine-tuning, SFT），通过使用任务输入与输出的配对数据进行模型训练，可以使得语言模型较好地掌握通过问答形式进行任务求解的能力。 与预训练相比，指令微调通常来说需要的指令实例数据规模要小的多。  </p><p> 除了提升任务的解决能力外，还需要将大语言模型与人类的期望、需求以及价值观对齐（Alignment），这对于大模型的部署与应用具有重要的意义。OpenAI在 2022 年初发布了 InstructGPT 的学术论文，系统地介绍了如何将语言模型进行人类对齐。具体来说，主要引入了基于人类反馈的强化学习对齐方法 RLHF （Reinforcement Learning from Human Feedback），在指令微调后使用强化学习加强模型的对齐能力。在 RLHF 算法中，需要训练一个符合人类价值观的奖励模型（Reward Model）。    </p><h3 id="扩展法则"><a href="#扩展法则" class="headerlink" title="扩展法则"></a>扩展法则</h3><p>  大语言模型采用了与小型预训练语言模型相似的神经网络结构（基于注意力机制的 Transformer 架构）和预训练方法（如语言建模）。但是通过<strong>扩展参数规模、数据规模和计算算力</strong>，大语言模型的能力显著超越了小型语言模型的能力。有趣的是，这种通过扩展所带来的性能提升通常显著高于通过改进架构、算法等方面所带来的改进。  </p><p>当算力 𝐶 给定的情况下，最优的模型参数规模和数据规模由指数系数 𝑎 和 𝑏 分别确定。 𝑎 和 𝑏 决定了参数规模和数据规模的资源分配优先级：当 𝑎 &gt; 𝑏时，应该用更多的算力去提高参数规模；当 𝑏 &gt; 𝑎 时，应该用更多的算力去提高数据规模。尽管 KM 扩展法则和 Chinchilla 扩展法则具有相似的公式形式，但是在模型规模和数据规模的扩展上存在一定的差异。随着算力预算的增加， KM 扩展法则（𝑎 ≈ 0.73, 𝑏 ≈ 0.27  ）倾向于将更大的预算分配给模型规模的增加，而不是分配给数据规模的增加；而 Chinchilla 扩展法则主张两种规模参数应该以等比例关系增加（𝑎 ≈ 0.46, 𝑏 ≈ 0.54 ）。</p><p>越来越多的工作表明，现有的预训练语言模型对于数据的需求量远高于这些扩展法则中所给出的估计规模。例如， LLaMA-2 (7B) 的模型就使用了 2T 的词元进行训练，很多更小的模型也能够通过使用超大规模的预训练数据获得较大的模型性能提升。  </p><h3 id="涌现能力"><a href="#涌现能力" class="headerlink" title="涌现能力"></a>涌现能力</h3><p>大语言模型的涌现能力被非形式化定义为“在小型模型中不存在但在大模型中出现的能力”，具体是指当模型扩展到一定规模时，模型的特定任务性能突然出现显著跃升的趋势，远超过随机水平。  </p><ol><li><p>上下文学习</p><p>上下文学习能力在 GPT-3 的论文中被正式提出。具体方式为，在提示中为语言模型提供自然语言指令和多个任务示例（Demonstration），无需显式的训练或梯度更新，仅输入文本的单词序列就能为测试样本生成预期的输出。  </p></li><li><p>指令遵循</p><p>指令遵循能力是指大语言模型能够按照自然语言指令来执行对应的任务。为了获得这一能力，通常需要使用自然语言描述的多任务示例数据集进行微调，称为指令微调（Instruction Tuning）或监督微调（Supervised Fine-tuning）。通过指令微调，大语言模型可以在没有使用显式示例的情况下按照任务指令完成新任务，有效提升了模型的泛化能力。  </p></li><li><p>逐步推理</p><p>对于小型语言模型而言，通常很难解决涉及多个推理步骤的复杂任务（如数学应用题），而大语言模型则可以利用思维链（Chain-of-Thought, CoT）提示策略来加强推理性能。</p></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数据结构-图的创建与遍历</title>
    <link href="/2024/06/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%81%8D%E5%8E%86/"/>
    <url>/2024/06/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%81%8D%E5%8E%86/</url>
    
    <content type="html"><![CDATA[<h1 id="有向图的拓扑排序"><a href="#有向图的拓扑排序" class="headerlink" title="# 有向图的拓扑排序"></a># 有向图的拓扑排序</h1><h2 id="图的创建与遍历"><a href="#图的创建与遍历" class="headerlink" title="图的创建与遍历"></a>图的创建与遍历</h2><p>给定节点数和边数，以及构成每条边的两个节点。</p><p><strong>构造图</strong>：</p><p><img src="/2024/06/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%81%8D%E5%8E%86/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240603193735331.png" alt="image-20240603193735331"></p><p>要存储上面的图，需要h[N], e[N], ne[N]来表示</p><span id="more"></span><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstring&gt;</span></span><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-type">int</span> h[N];<span class="hljs-comment">//存放节点a第一个邻接节点的索引idx</span><br><span class="hljs-type">int</span> e[N];<span class="hljs-comment">//存放索引为idx的边的指向的节点值a</span><br><span class="hljs-type">int</span> ne[N];<span class="hljs-comment">//存放索引idx的与其共享同一个起始点的边的索引idx</span><br><span class="hljs-type">int</span> idx;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-type">int</span> a, <span class="hljs-type">int</span> b)</span></span>&#123;<br>    e[idx] = b;<br>    ne[idx] = h[a];<br>    h[a] = idx++;<br>    <br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">memset</span>(h, <span class="hljs-number">-1</span>, <span class="hljs-built_in">sizeof</span>(h));<br>    <span class="hljs-type">int</span> n,m;<br>    cin&gt;&gt;n&gt;&gt;m;<br>    <span class="hljs-type">int</span> a,b;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;m; i++)&#123;<br>        cin&gt;&gt;a&gt;&gt;b;<br>        <span class="hljs-built_in">add</span>(a,b);<br>    &#125;<br>    <br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>; i&lt;=n; i++)&#123;<br>        <span class="hljs-comment">//对点i求其所连接的边</span><br>        cout&lt;&lt;<span class="hljs-string">&quot;以节点&quot;</span>&lt;&lt;i&lt;&lt;<span class="hljs-string">&quot;为起始点，与其相邻的边有：&quot;</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = h[i]; j!=<span class="hljs-number">-1</span>; j = ne[j])&#123;<br>            <span class="hljs-type">int</span> eb = e[j];<br>            cout&lt;&lt;eb&lt;&lt;<span class="hljs-string">&quot; &quot;</span>;<br>            <br>        &#125;<br>        cout&lt;&lt;endl;<br>        <br>    &#125; <br>    <br>    <br>&#125;<br></code></pre></td></tr></table></figure><p><img src="/2024/06/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%81%8D%E5%8E%86/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240603193821500.png" alt="image-20240603193821500"></p><h2 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h2><p><strong>拓扑排序</strong>：</p><p>若一个由图中所有点构成的序列 A 满足：对于图中的每条边 (𝑥,𝑦)，𝑥 在 𝐴 中都出现在 𝑦 之前，则称 𝐴 是该图的一个拓扑序列。</p><p>如：下面的图，能找到序列：1 —— 2 —— 4 —— 3 ——5构成一条拓扑序列</p><p><img src="/2024/06/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%81%8D%E5%8E%86/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240603194203015.png" alt="image-20240603194203015"></p><p><strong>判断方法</strong>：</p><ol><li>删除一个入度为0的节点，加入结果集合</li><li>将上述节点对应的有向边都删除，重复第一步</li><li>直到最后没有节点，则该图可找出拓扑序列</li></ol><p><strong>代码实现重点</strong>：</p><ol><li>现把入度为0的节点加到集合的，后面删除该节点对应的边时要先删除：可以使用队列存储</li><li>删除节点a对应的有向边：可以遍历a为起始的连接边，将这些对应的点的入度-1即可</li><li>直到最后没有节点：队列为空时，结果集为整个图的节点个数</li></ol><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>用一个d[N]数组来存储每个点的入度数</p><p>队列q存储所有入度为0的节点</p><p>数组v存储答案</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;queue&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;algorithm&gt;</span></span><br><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br>queue&lt;<span class="hljs-type">int</span>&gt; q;<br>vector&lt;<span class="hljs-type">int</span>&gt; v;<br><br><span class="hljs-type">int</span> e[N], ne[N],h[N];<br><span class="hljs-type">int</span> d[N];<span class="hljs-comment">//计算每个点的入度数</span><br><span class="hljs-type">int</span> idx;<br><span class="hljs-type">int</span> n,m;<br><span class="hljs-type">int</span> sum;<br><br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-type">int</span> a, <span class="hljs-type">int</span> b)</span></span>&#123;<br>    e[idx] = b;<br>    ne[idx] = h[a];<br>    h[a] = idx++;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">topsort</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-comment">//找度为0的点</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i =<span class="hljs-number">1</span>; i&lt;=n; i++)&#123;<br>        <span class="hljs-keyword">if</span>(d[i]==<span class="hljs-number">0</span>)&#123;<br>            q.<span class="hljs-built_in">push</span>(i);<br>        &#125;<br>    &#125;<br>    <br>    <span class="hljs-keyword">while</span>(!q.<span class="hljs-built_in">empty</span>())&#123;<br>        <span class="hljs-type">int</span> u = q.<span class="hljs-built_in">front</span>();<br>        v.<span class="hljs-built_in">push_back</span>(u);<br>        sum++;<br>        q.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = h[u]; j!=<span class="hljs-number">-1</span>; j=ne[j])&#123;<br>            <span class="hljs-type">int</span> c = e[j];<br>            d[c]--;<br>            <span class="hljs-keyword">if</span>(d[c]==<span class="hljs-number">0</span>)&#123;<br>                q.<span class="hljs-built_in">push</span>(c);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <br>    <span class="hljs-keyword">if</span>(sum == n)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> i : v)&#123;<br>            cout&lt;&lt;i&lt;&lt;<span class="hljs-string">&quot; &quot;</span>;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">else</span>&#123;<br>        cout&lt;&lt;<span class="hljs-number">-1</span>;<br>    &#125;<br>    <br>    <br>    <br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">memset</span>(h, <span class="hljs-number">-1</span>, <span class="hljs-built_in">sizeof</span>(h));<br>    cin&gt;&gt;n&gt;&gt;m;<br>    <span class="hljs-type">int</span> a,b;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i =<span class="hljs-number">0</span> ;i&lt;m ;i++)&#123;<br>        cin&gt;&gt;a&gt;&gt;b;<br>        <span class="hljs-built_in">add</span>(a,b);<br>        d[b]++;<br>    &#125;<br>    <span class="hljs-built_in">topsort</span>();<br>    <br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>前馈神经网络</title>
    <link href="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h1><h2 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h2><h3 id="单层感知机"><a href="#单层感知机" class="headerlink" title="单层感知机"></a>单层感知机</h3><p>输入层：感知层，n个神经节点，无信息处理能力，只负责引入外部信息X。</p><p>处理层：m个神经接点，每节点均有信息处理能力，m个节点向外部处理输出信息，构成输出列向量Y。</p><p>两层间连接权值用权值列向量$W_j$表示，m个权向量构成单层感知器的权值矩阵W。</p><span id="more"></span><p>$W_j&#x3D;[w_{1j} w_{2j} …w{ij}…w_{nj}]^T $</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180125854.png" alt="image-20240608180125854"></p><p>离散型单计算层感知器采用符号型转移函数，则j节点输出为：</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180228313.png" alt="image-20240608180228313"></p><h4 id="单计算节点感知机"><a href="#单计算节点感知机" class="headerlink" title="单计算节点感知机"></a>单计算节点感知机</h4><p>单计算节点感知器实际上就是一个M-P神经元模型。</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180257486.png" alt="image-20240608180257486"></p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180319370.png" alt="image-20240608180319370"></p><p><strong>功能：</strong></p><p>输入向量$X&#x3D;[x_1 x_2 …x_n]^T$,则n个输入分量构成几何n维空间，</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180355265.png" alt="image-20240608180355265"></p><p>该超平面将样本分成2类。</p><p>一个简单的单计算节点感知器具有分类功能，其分类原理是将分类知识存储于感知器的权向量（包括阈值）中，由权向量确定的分类判决界面（线），可将输入模式分为两类。</p><p>举例：</p><ol><li><p>功能”<strong>与</strong>“</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180432426.png" alt="image-20240608180432426"></p></li><li><p>功能”<strong>或</strong>“</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180443134.png" alt="image-20240608180443134"></p></li><li><p>功能”<strong>异或</strong>”</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180456415.png" alt="image-20240608180456415"></p><p>确定的分类判决方程是线性方程,因而只能解决线性可分问题的分类,不能解决线性不可分问题.<br>这称为单计算层感知器的局限性.</p></li></ol><h3 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h3><p>单计算层感知器只能解决线性可分问题,多层感知器可解决线性不可分问题。</p><p>输出层节点以隐层两节点y1,y2的输出作为输入,其结构也相当于一个符号单元。</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180510130.png" alt="image-20240608180510130"></p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180521772.png" alt="image-20240608180521772"></p><p>1、2两符号单元确定两条分界直线s1和s2,可构成开放式凸域。</p><p>双隐层感知器足以解决任何复杂分类问题，为提高感知器分类能力，可采用<strong>非线性连续函数</strong>作为神经元节点的转移函数，使区域边界变成曲线，可形成连续光滑曲线域。</p><h2 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h2><p>单个神经细胞只有两种状态：兴奋和抑制</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240607175753581.png" alt="image-20240607175753581"></p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p><strong>·</strong> 激活函数的性质：</p><ol><li>连续并可导的非线性函数（允许在少数点上不可导）</li><li>激活函数及其导数要尽可能的简单</li><li>激活函数的导数的值域要在有关合适的区间内</li><li>单调递增</li></ol><p><strong>·</strong> 常见的激活函数：</p><ol><li><p>S型</p><p>$\sigma(x) &#x3D; \frac{1}{1+e^{-x}}$</p><p>$\tanh(x)&#x3D;\frac{e^x - e^{-x}}{e^x + e^{-x}} &#x3D; 2\sigma(2x)-1$</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240607180731494.png" alt="image-20240607180731494"></p><p><strong>饱和函数：</strong>当$x$趋向无穷大时，其导数置趋于0</p><p>S型激活函数时饱和函数。（可能会导致梯度消失问题）</p><p>Tanh函数时零中心化的，logistic函数的输出恒大于0</p></li><li><p>斜坡型</p><p>$ReLU(x) &#x3D; \begin{cases}    x, &amp; \text{if } x &gt; 0 \    0, &amp; \text{if } x \leq 0 \end{cases}&#x3D;max(x,0)$</p><p>具有单侧抑制，宽兴奋边界，能在一定程度是缓解梯度消失问题 （ReLU 在正区间（x &gt; 0）上的梯度恒为 1，这意味着在正区间内，梯度不会消失。相比之下，一些传统的激活函数（如 Sigmoid 和 Tanh）在输入值较大或较小时会接近饱和，导致梯度消失，使得网络的训练速度变慢。）</p><p>$LeakyReLU(x) &#x3D; \begin{cases}    x, &amp; \text{if } x &gt; 0 \    \gamma x, &amp; \text{if } x \leq 0 \end{cases} &#x3D; max(0,x) + \gamma min(0,x)$</p><p>$PReLU(x) &#x3D; \begin{cases}    x, &amp; \text{if } x &gt; 0 \    \gamma_i x, &amp; \text{if } x \leq 0 \end{cases} &#x3D; max(0,x) + \gamma_i min(0,x)$</p><p>近似零中心化的非线性函数</p><p>$ELU(x) &#x3D; \begin{cases}    x, &amp; \text{if } x &gt; 0 \    \gamma (e^x-1), &amp; \text{if } x \leq 0 \end{cases} &#x3D; max(0,x) +  min(0,\gamma(e^x-1))$</p><p>ReLU的平滑版本：</p><p>$softplus(x) &#x3D; log(1+e^x)$</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240607182142681.png" alt="image-20240607182142681"></p><p><strong>死亡ReLU问题：</strong>一些神经元的输出始终为零，导致这些神经元对于网络的训练没有贡献，失去了激活的能力。可使用LeakyReLU缓解</p></li><li><p>复合函数</p><p>自门控激活函数</p><p>$swish(x) &#x3D; x\sigma(\beta x)$</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240607182656843.png" alt="image-20240607182656843"></p></li></ol><p>常见激活函数及其导数</p><table><thead><tr><th>激活函数</th><th>函数</th><th>导数</th></tr></thead><tbody><tr><td>Logistic函数</td><td>$f(x)&#x3D;\frac{1}{1+e^{-x}}$</td><td>$f^{‘}(x) &#x3D; f(x)(1-f(x)) $</td></tr><tr><td>Tanh函数</td><td>$f(x)&#x3D;\frac{e^x - e^{-x}}{e^x + e^{-x}}$</td><td>$f^{‘}(x) &#x3D; 1 - f^2(x)$</td></tr><tr><td>ReLU函数</td><td>$f(x) &#x3D; max(0,x)$</td><td>$f^{‘}(x) &#x3D; I(x&gt;0)$</td></tr><tr><td>ELU函数</td><td>$f(x) &#x3D; max(0,x)+min(0, \gamma(e^x-1))$</td><td>$f^{‘}(x) &#x3D; I(x&gt;0) + I(x\leq0)\gamma e^x$</td></tr><tr><td>SoftPlus函数</td><td>$f(x) &#x3D; log(1+ e^x)$</td><td>$f^{‘}(x) &#x3D; \frac{1}{1+e^{-x}}$</td></tr></tbody></table><h3 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h3><p>人工神经网络主要由大量的神经元以及它们之间的有向连接构成。</p><p>需要考虑三方面：</p><ol><li><p>神经元的激活规则</p></li><li><p>网络的拓扑结构</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240607184301479.png" alt="image-20240607184301479"></p></li><li><p>学习算法</p></li></ol><h2 id="前馈神经网络-1"><a href="#前馈神经网络-1" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h2><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>各神经元分别属于不同层，层内无连接</p><p>相邻两层之间的神经元全部两两连接</p><p>网络中无反馈，信号从输入层想输出层单向传播，可用一个有向无环图表示。</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240607184828035.png" alt="image-20240607184828035"></p><h3 id="信息传递过程"><a href="#信息传递过程" class="headerlink" title="信息传递过程"></a>信息传递过程</h3><p>对第$l$层，其净输入$z^l &#x3D; W^la^{l-1}+b^l$，其输出为$a^l &#x3D; f(z^l)$</p><p>前馈计算为：$x &#x3D; a^0 \rightarrow  z^1 \rightarrow a^1 \rightarrow z^2 \rightarrow a^{L-1} \rightarrow z^L \rightarrow a^L $</p><h3 id="通用近似定理"><a href="#通用近似定理" class="headerlink" title="通用近似定理"></a>通用近似定理</h3><p>也叫<strong>万能逼近定理</strong></p><p>对于具有线性输出层和至少一个使用“挤压”性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够，它可以以任意的精度来近似任何从一个定义在实数空间中的有界闭集函数。</p><h3 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h3><p>如果使用Softmax回归分类器，相当于网络最后一层设置C 个神经元，其输出经过Softmax函数进行归一化后可以作为每个类的条件概率。</p><p>$\hat{y} &#x3D; softmax(z^L)$</p><p>采用交叉熵损失函数，对样本$(x,y)$，其损失函数为：</p><p>$L(y,\hat{y}) &#x3D; -y^Tlog\hat{y}$</p><p>给定训练集为$𝐷 &#x3D; {(𝒙^{(𝑛)}, 𝑦^{(𝑛)} )}_{𝑛&#x3D;1}^𝑁$ ，将每个样本输入给前馈神经网络，得到网络输出$\hat{𝑦}^{(𝑛)}$ ，其在数据集D上的结构化风险函数为：</p><p>$R(W,b) &#x3D; \frac{1}{N}\sum_{n&#x3D;1}^{N}L(y^n, \hat{y}^n)+ \frac{1}{2} \lambda ||W||^2$</p><p>梯度下降：</p><p>$W^l \leftarrow W^l - \alpha\frac{\partial R(W,b)}{\partial W^l} $</p><p>$b^l \leftarrow b^l - \alpha\frac{\partial R(W,b)}{\partial b^l} $</p><h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p><strong>Step1:</strong>  顺序表示梯度公式</p><p>由公式$z^l &#x3D; W^la^{l-1}+b^l$得：</p><p>$\frac{\partial L(y,\hat{y})}{\partial w_{ij}^l} &#x3D; \frac{\partial L(y,\hat{y})}{\partial \mathbf{z}^l } \frac{\partial \mathbf{z}^l}{\partial w_{ij}^l} $ , $\frac{\partial L(y,\hat{y})}{\partial \mathbf{b}^l} &#x3D; \frac{\partial L(y,\hat{y})}{\partial \mathbf{z}^l } \frac{\partial \mathbf{z}^l}{\partial \mathbf{b}^l} $</p><p>这里的$\mathbf{z}, \mathbf{b}$都是m维的向量</p><p>$\frac{\partial \mathbf{z}^l}{\partial w_{ij}^l} &#x3D; [ \frac{\partial z^l_1}{\partial w_{ij}^l}, \frac{\partial z^l_2}{\partial w_{ij}^l},…,\frac{\partial z^l_{i}}{\partial w_{ij}^l},…,\frac{\partial z^l_m}{\partial w_{ij}^l}] &#x3D; [0,0,…,\frac{\partial a^{l-1} \mathbf{w}_{i:}^l+b^{l}<em>i}{\partial w</em>{ij}^l},..,0] &#x3D; [0,0,…,a^{l-1}_j,0]$</p><p>同理，将$a_j^{l-1}$看作$1$，可得：</p><p>$\frac{\partial \mathbf{z}^l}{\partial \mathbf{b}^l} &#x3D; \mathbf{I}_m$</p><p>再令，$ \frac{\partial L(y,\hat{y})}{\partial \mathbf{z}^l }  &#x3D;  \mathbf{\delta}^l$，则损失函数对参数z，b求导最后可以写作：</p><p>$\frac{\partial L(y,\hat{y})}{\partial w_{ij}^l} &#x3D; \delta^l_i a_j^{l-1} $</p><p>$\frac{\partial L(y,\hat{y})}{\partial \mathbf{b}^l} &#x3D; \bold{\delta}^l$</p><p><strong>Step2:</strong>  根据递推公式表示$\bold{\delta}^l$</p><p>$\mathbf{z}^{l+1} &#x3D; \mathbf{W}^{l+1} \mathbf{a}^{l} + \mathbf{b}^{l+1}$</p><p>$\mathbf{a}^l &#x3D; f(\mathbf{z}^l)$</p><p>即$\mathbf{z}^l \rightarrow \mathbf{a}^l \rightarrow \mathbf{z}^{l+1}$</p><p>$\bold{\delta}^l &#x3D; \frac{\partial L(y,\hat{y})}{\partial \mathbf{z}^l }  &#x3D; \frac{\partial L(y,\hat{y})}{\partial \mathbf{z}^{l+1} } \frac{\partial \mathbf{z}^{l+1}}{\partial \mathbf{a}^l } \frac{\partial \mathbf{a}^l}{\partial \mathbf{z}^l }  &#x3D; \mathbf{\delta^{l+1}} (\mathbf{W}^{l+1})^T diag(f^{‘}(\mathbf{z}^l)) &#x3D; f^{‘}(\mathbf{z}^l) \odot \mathbf{\delta^{l+1}} (\mathbf{W}^{l+1})^T$</p><p>随机梯度下降训练过程：</p><p>输入：训练数据，验证数据</p><p>首先 随机初始化参数$\mathbf{W}, \mathbf{b}$</p><p>重复以下操作：</p><p>  对样本随机重排</p><p>  for n &#x3D;1 .. N do:</p><ol><li><p>选择一个样本$(\mathbf{x}^n, y^n)$，计算前馈的每一层的净输入$\mathbf{z}^l$和激活值$\mathbf{a}^l$，并计算每一层的误差$\bold{\delta}^l$，这个可以通过step2的公式计算</p></li><li><p>反向传播，计算每层的偏导，可用step1的公式计算</p><p>$\frac{\partial L(y,\hat{y})}{\partial w_{ij}^l} &#x3D; \delta^l_i a_j^{l-1} $</p><p>$\frac{\partial L(y,\hat{y})}{\partial \mathbf{b}^l} &#x3D; \bold{\delta}^l$</p></li><li><p>更新参数</p><p>$W^l \leftarrow W^l - \alpha\frac{\partial R(W,b)}{\partial W^l} $</p><p>$b^l \leftarrow b^l - \alpha\frac{\partial R(W,b)}{\partial b^l} $</p></li></ol><p>  end</p><p>直至模型在验证集上的错误率不在下降</p><p>举例：</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608170939640.png" alt="image-20240608170939640"></p><p>先按传播顺序计算出各个净输入和激活值</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608171752988.png" alt="image-20240608171752988"></p><ol><li><p>计算输出层</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/56d0197d29a72eaa8310d7fedd501891-17178392786981.png" alt="img"></p><p>计算这层的误差：$\mathbf{\delta}^l &#x3D; \hat{y} (1- \hat{y}) (\hat{y} - y)$</p><p>更新权值：$\frac{\partial L(y,\hat{y})}{\partial w_{ij}^l} &#x3D; \delta^l_i a_j^{l-1}$</p></li><li><p>计算隐藏层</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/56d0197d29a72eaa8310d7fedd501891.png" alt="img"></p><p>计算这层的误差：$\mathbf{\delta}^l &#x3D; \hat{y} (1- \hat{y}) \sum(\mathbf{\delta}^{l+1}<em>j)w</em>{ij}$</p><p>更新权值：$\frac{\partial L(y,\hat{y})}{\partial w_{ij}^l} &#x3D; \delta^l_i a_j^{l-1}$</p></li></ol><h2 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h2><ol><li>前馈神经网络特点</li><li>激活函数的定义及其导数</li><li>反向传播算法</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>预训练模型</title>
    <link href="/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h1><h2 id="预训练简介"><a href="#预训练简介" class="headerlink" title="预训练简介"></a>预训练简介</h2><p><strong>预训练</strong>：通过子监督学习从大规模数据里获取与具体任务无关的预训练模型的过程。</p><p>预训练任务：</p><ol><li><p>掩码语言模型（编码器）：将一些位置的token替换成特殊的[MASK]字符，预测这些被替换的字符</p><p><img src="/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240602212830234.png" alt="image-20240602212830234"></p><p>只计算掩码部分的loss，其余部分不计算loss</p><span id="more"></span></li><li><p>因果语言模型（解码器）：输入完整序列，基于上文预测当前token</p><p><img src="/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240602213105457.png" alt="image-20240602213105457"></p><p>eos代表句子结束</p></li><li><p>序列到序列模型：编码器解码器方式，预测部分放到解码器里面</p><p>只计算解码器的loss，不计算解码器部分</p></li></ol><h2 id="文本摘要"><a href="#文本摘要" class="headerlink" title="文本摘要"></a>文本摘要</h2><p><strong>文本摘要：</strong>输入长文本，将长文本转成简短的摘要</p><p>任务类别：单文档单语言摘要</p><p>评价指标：Rouge-1(基于1-gram) Rouge-2(基于2-gram) Rouge-L(基于LCS)</p><table><thead><tr><th>原始文本</th><th>1-gram</th><th>2-gram</th></tr></thead><tbody><tr><td>今天不错</td><td>今 天 不 错</td><td>今天 天不 不错</td></tr><tr><td>今天太阳不错</td><td>今 天 太 阳 不 错</td><td>今天 天太 太阳 阳不 不错</td></tr></tbody></table><p>Rouge-1   P &#x3D; 4&#x2F;4   R&#x3D;4&#x2F;6  F &#x3D; 2PR(P+R)</p><p>Rouge-2   P &#x3D; 2&#x2F;3   R&#x3D;2&#x2F;5</p><p>LCS(最长公共子序列)   P &#x3D; 4&#x2F;4   R&#x3D;4&#x2F;6</p><h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>input和labels分开处理，labels最后一定要有eos_token</p><p>labels不仅是标签，还是解码器的输入</p><p>数据集：</p><h2 id="对话机器人"><a href="#对话机器人" class="headerlink" title="对话机器人"></a>对话机器人</h2><h1 id="参数微调fine-tuning"><a href="#参数微调fine-tuning" class="headerlink" title="参数微调fine-tuning"></a>参数微调fine-tuning</h1><h2 id="beat-fit"><a href="#beat-fit" class="headerlink" title="beat-fit"></a>beat-fit</h2><p>只对bias求梯度，其他的参数冻结</p><h2 id="prompt-Tuning"><a href="#prompt-Tuning" class="headerlink" title="prompt-Tuning"></a>prompt-Tuning</h2><p><img src="/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/image-20240605211808432.png" alt="image-20240605211808432"></p><p>冻结主模型所有参数，在训练数据前加入一小段prompt，只训练prompt的embedding层。</p><p>hard prompt：指定prompt</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, prompt_tuning_init=PromptTuningInit.TEXT, <br>                            prompt_tuning_init_text = <span class="hljs-string">&quot;下面是一段人与机器人的对话。&quot;</span>,<br>                            num_virtual_tokens  = <span class="hljs-built_in">len</span>(tokenizer(<span class="hljs-string">&quot;下面是一段人与机器人的对话。&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]), <br>                            tokenizer_name_or_path = <span class="hljs-string">&quot;Langboat/bloom-389m-zh&quot;</span>)<br><br><br></code></pre></td></tr></table></figure><p>soft prompt：不指定prompt，让模型自行学习  ，对模型需要进行适配多轮才能有好效果</p><h2 id="P-Tuning"><a href="#P-Tuning" class="headerlink" title="P-Tuning"></a>P-Tuning</h2><p><img src="/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/image-20240605212049075.png" alt="image-20240605212049075"></p><p>在Prompt-Tuning基础上，对prompt部分进行进一步的编码计算，加速收敛。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> PromptEncoderConfig, TaskType, get_peft_model, PromptEncoderReparameterizationType<br><br>config = PromptEncoderConfig(task_type = TaskType.CAUSAL_LM, num_virtual_tokens = <span class="hljs-number">10</span>, <br>                             encoder_reparameterization_type = PromptEncoderReparameterizationType.LSTM,<br>                             encoder_dropout = <span class="hljs-number">0.1</span>, encoder_num_layers = <span class="hljs-number">2</span>, encoder_hidden_size =<span class="hljs-number">1024</span><br>                             )<br>config<br></code></pre></td></tr></table></figure><h2 id="Prefix-Tuning"><a href="#Prefix-Tuning" class="headerlink" title="Prefix-Tuning"></a>Prefix-Tuning</h2><p>past_key_values：Transformer模型中历史计算过的key和value的结果，会存在重复计算，可将结果缓存。</p><p>通过past_key_values的形式将可学习的部分放到了模型中的每一层，这部分内容又称为前缀。</p><p><img src="/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/image-20240605214006018.png" alt="image-20240605214006018"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Transformer</title>
    <link href="/2024/06/02/Transformer/"/>
    <url>/2024/06/02/Transformer/</url>
    
    <content type="html"><![CDATA[<h1 id="Transformer的理解"><a href="#Transformer的理解" class="headerlink" title="Transformer的理解"></a>Transformer的理解</h1><h1 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h1><p><img src="https://img-blog.csdnimg.cn/direct/8a8c78c6941948a1827a013fe1da2bb3.png" alt="在这里插入图片描述"></p><h1 id="ENCODER"><a href="#ENCODER" class="headerlink" title="ENCODER"></a>ENCODER</h1><p><img src="https://img-blog.csdnimg.cn/direct/e93387aeabd749789fcb9379ef913203.png" alt="在这里插入图片描述"></p><h2 id="输入部分"><a href="#输入部分" class="headerlink" title="输入部分"></a>输入部分</h2><p>对拆分后的语句x &#x3D; [batch_size, seq_len]进行以下操作</p><span id="more"></span><ol><li>Embedding<br>将离散的输入（如单词索引或其他类别特征）转换为稠密的实数向量，以便可以在神经网络中使用。</li><li>位置编码<br>与RNN相比，RNN是一个字一个字的输入，自然每个字的顺序关系信息就会保留下来。但在Encoder中，一个句子的每一个字（词）是并行计算的（下一节解释），所以我们在输入的时候需要提前引入位置信息。<br>位置信息由： pos（一句话中的第几个字） 和 i （这个字编码成向量后的第i维) 来确定<br>下面是Positional Encoding的公式：<br>i为 偶 数 时 ,  $PE_{pos, i}&#x3D; sin( pos&#x2F; 10000^{2i&#x2F; d_{model}})$<br>i为 奇 数 时 ,  $PE_{pos, i}&#x3D; cos( pos&#x2F; 10000^{2i&#x2F; d_{model}})$<br>$d_{model}$指想用多长的 vector 来表达一个词(embedding_dim)</li></ol><p>通过输入部分<br>x： [batch_size, seq_len, embedding_dim]</p><p><img src="https://img-blog.csdnimg.cn/direct/2cd5cdb5641248179485166cd2044e44.png" alt="在这里插入图片描述"></p><h2 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h2><ol><li>单头注意力机制<br>对一句话中第i个字的字向量$a_i$，产生三个矩阵Q, K ，V<br>Q,K,V的维度都为[batch_size, seq_len, embedding_dim]</li></ol><p>将$a_i$分别与上面三个矩阵相乘，得到三个向量$q_i, k_i, v_i$<br>如果要计算第1个字向量与句子中所有字向量的注意力：<br>将查询向量$q_1$与 所有的字向量的键向量$k_i$相乘得到$alpha_{10}, alpha_{11},…,alpha_{1,seqlen}$<br>将这写数值进行softmax处理后， 分别与$v_i$相乘再合加得到最终结果$b_1$</p><p><img src="https://img-blog.csdnimg.cn/direct/1e55284965a44375b70d7f35a5869142.png" alt="在这里插入图片描述"></p><ol start="2"><li>多头注意力机制<br>把$Q,K,V$三个大矩阵变成n个小矩阵（seq_len, embedding_dim&#x2F;n)     n&#x3D;8<br>用上节相同的方式计算8个矩阵，然后把每一个head-Attention计算出来的b矩阵拼在一起，作为输出</li></ol><h2 id="Add-LN"><a href="#Add-LN" class="headerlink" title="Add&amp;LN"></a>Add&amp;LN</h2><p>Add是用了残差神经网络的思想，也就是把Multi-Head Attention的输入的a矩阵直接加上Multi-Head Attention的输出b矩阵（好处是可以让网络训练的更深）得到的和 $\bar{b}$矩阵</p><p>再在经过Layer normalization（归一化，作用加快训练速度，加速收敛）把<br> 每一行（也就是每个句子）做归一为标准正态分布，最后得到$\hat{b}$<br>BN 和 LN：</p><ol><li>LN： 在一个样本内做归一化 适于RNN,transformer</li><li>BN： 对batch_size里面的样本按对应的特征做归一化  适于CNN<br><img src="https://img-blog.csdnimg.cn/direct/8f699727d3214959addb03e90822d558.png" alt="在这里插入图片描述"></li></ol><h2 id="Feed-forward前馈神经网络"><a href="#Feed-forward前馈神经网络" class="headerlink" title="Feed_forward前馈神经网络"></a>Feed_forward前馈神经网络</h2><p>把Add &amp; Layer normalization输出$\hat{b}$，经过两个全连接层，再经过Add &amp; Layer normalization得到最后输出 o 矩阵</p><h1 id="DECODER"><a href="#DECODER" class="headerlink" title="DECODER"></a>DECODER</h1><h2 id="masked-多头注意力机制"><a href="#masked-多头注意力机制" class="headerlink" title="masked_多头注意力机制"></a>masked_多头注意力机制</h2><p> 比如我们在中英文翻译时候，会先把”我是学生”整个句子输入到Encoder中，得到最后一层的输出后，才会在Decoder输入”S I am a student”（s表示开始）,但是”S I am a student”这个句子我们不会一起输入，而是在T0时刻先输入”S”预测，预测第一个词”I”；在下一个T1时刻，同时输入”S”和”I”到Decoder预测下一个单词”am”；然后在T2时刻把”S,I,am”同时输入到Decoder预测下一个单词”a”,依次把整个句子输入到Decoder,预测出”I am a student E”</p><h2 id="多头注意力机制-1"><a href="#多头注意力机制-1" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h2><p>Decoder 的 Multi-Head Attention 的输入来自两部分，<br>K，V 矩阵来自Encoder的输出，<br>Q 矩阵来自 Masked Multi-Head Attention 的输出<br><img src="https://img-blog.csdnimg.cn/direct/3f06bddb5c124448b71e5df6a4c2429d.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>first-blog</title>
    <link href="/2024/06/02/first-blog/"/>
    <url>/2024/06/02/first-blog/</url>
    
    <content type="html"><![CDATA[<h1 id="数据结构复习篇1——哈夫曼树"><a href="#数据结构复习篇1——哈夫曼树" class="headerlink" title="数据结构复习篇1——哈夫曼树"></a>数据结构复习篇1——哈夫曼树</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><ol><li><p>结点的带权路径：从根结点到该结点之间的路径长度与该结点的权的乘积</p></li><li><p>树的带权路径：所有叶子结点的带权路径长度之和</p><p>举例：</p><p><img src="/2024/06/02/first-blog/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240602203936732.png" alt="image-20240602203936732"></p></li><li><p>哈夫曼树：将n个权值作为二叉树的n个叶子结点，若树的带权路径长度达到最小，则这棵树被称为哈夫曼树</p></li></ol><span id="more"></span><h2 id="构造哈夫曼树"><a href="#构造哈夫曼树" class="headerlink" title="构造哈夫曼树"></a>构造哈夫曼树</h2><p>给我们n个结点，如何构造出一颗哈夫曼树呢？</p><p>在这里，我们可以尝试使用贪心的策略，如果要是的WPL最小，那权值大的点应该放在深度很小的地方，权值小的点应该放到底层，即深度很大的地方。所以每次构造时选择两个权值最小的结点进行构造。</p><p>举例：给定4，7，9，10这四个结点，如何构造一个是的WPL最小的二叉树呢？</p><p><strong>·Step1:</strong> 从这个结点集合中选择权值最小的两个点，组成一个新结点。</p><p>在这个例子里面，选择 4 和 7 ，构造成新的点11</p><p><strong>·Step2:</strong> 把选过的结点从结点集合排出，新结点11加入结点集合，重复step1，直至结点集合里面只有一个结点为止。</p><p>根据上述步骤，最后组成的哈夫曼树如图所示。</p><p><img src="/2024/06/02/first-blog/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240602204852758.png" alt="image-20240602204852758"></p><h2 id="代码实践"><a href="#代码实践" class="headerlink" title="代码实践"></a>代码实践</h2><p>对于上述的方法，很容易想到如果要构造一个哈夫曼树可以使用最小堆（priority_queue）实现。每次从堆中弹出的两个结点即是我们要取的构造新结点的组成结点。</p><p>这里，我们将二叉树扩展到K叉树，即每个父结点由K个字节的组成。</p><h3 id="荷马史诗"><a href="#荷马史诗" class="headerlink" title="荷马史诗"></a>荷马史诗</h3><p>引入问题：（该问题来自acwing<a href="https://www.acwing.com/problem/content/151/">149. 荷马史诗 - AcWing题库</a>)</p><p>一部《荷马史诗》中有 n 种不同的单词，从 1到 n 进行编号。其中第 i𝑖 种单词出现的总次数为 𝑤𝑖。</p><p>达达想要用 𝑘 进制串 𝑠𝑖 来替换第 𝑖 种单词，使得其满足如下要求:</p><p>对于任意的 1≤i,j≤n，i≠j1≤𝑖,𝑗≤𝑛，𝑖≠𝑗，都有：𝑠𝑖 不是 𝑠𝑗 的前缀。</p><p>现在达达想要知道，如何选择 𝑠𝑖，才能使替换以后得到的新的《荷马史诗》长度最小。</p><p>在确保总长度最小的情况下，达达还想知道最长的 𝑠𝑖 的最短长度是多少？</p><p>一个字符串被称为 k𝑘 进制字符串，当且仅当它的每个字符是 0 到 𝑘−1 之间（包括 0 和 𝑘−1）的整数。</p><p>字符串 𝑆𝑡𝑟1 被称为字符串 𝑆𝑡𝑟2 的前缀，当且仅当：存在 1≤t≤m1≤𝑡≤𝑚，使得 𝑆𝑡𝑟1&#x3D;𝑆𝑡𝑟2[1..𝑡]。</p><p>其中，𝑚 是字符串𝑆𝑡𝑟2 的长度，𝑆𝑡𝑟2[1..𝑡] 表示𝑆𝑡𝑟2 的前 𝑡 个字符组成的字符串。</p><p><strong>注意</strong>: 请使用 64 位整数进行输入输出、储存和计算。</p><p>输出文件包括 2 行。</p><p>第 1 行输出 1 个整数，为《荷马史诗》经过重新编码以后的最短长度。</p><p>第 2 行输出 1 个整数，为保证最短总长度的情况下，最长字符串 𝑠𝑖 的最短长度。</p><p>对于该问题，很明显可以看出需要我们构造一个k叉树使其$WPL$达到最小，另外，对于权值相同的结点，要优先考虑深度小的来构造，避免树的深度过大。</p><p>下面是代码实现。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;queue&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-keyword">typedef</span> <span class="hljs-type">long</span> <span class="hljs-type">long</span>  LL;<br>priority_queue&lt;pair&lt;LL, <span class="hljs-type">int</span>&gt;, vector&lt;pair&lt;LL, <span class="hljs-type">int</span>&gt;&gt;, greater&lt;pair&lt;LL, <span class="hljs-type">int</span>&gt;&gt;&gt; h;<br><br><span class="hljs-type">int</span> n,k;<br><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin&gt;&gt;n&gt;&gt;k;<br>    LL a;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;n; i++)&#123;<br>        cin&gt;&gt;a;<br>        h.<span class="hljs-built_in">push</span>(&#123;a,<span class="hljs-number">0</span>&#125;);<br>    &#125;<br>    <span class="hljs-comment">//对于填不满k叉树的结点，补0来处理</span><br>    <span class="hljs-keyword">while</span>((n<span class="hljs-number">-1</span>)%(k<span class="hljs-number">-1</span>))&#123;<br>        h.<span class="hljs-built_in">push</span>(&#123;<span class="hljs-number">0</span>,<span class="hljs-number">0</span>&#125;);<br>        n++;<br>    &#125;<br>    <br>    LL res = <span class="hljs-number">0</span>;<br>    <br>    <span class="hljs-keyword">while</span>(h.<span class="hljs-built_in">size</span>()&gt;<span class="hljs-number">1</span>)&#123;<br>        <br>        <span class="hljs-type">int</span> depth = <span class="hljs-number">0</span>;<br>        LL s = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;k; i++)&#123;<br>            <span class="hljs-keyword">auto</span> p = h.<span class="hljs-built_in">top</span>();<br>            s += p.first;<br>            depth = <span class="hljs-built_in">max</span>(depth, p.second);<br>            h.<span class="hljs-built_in">pop</span>();<br>        &#125;<br>        res += s;<br>        h.<span class="hljs-built_in">push</span>(&#123;s, depth+<span class="hljs-number">1</span>&#125;);<br>        <br>        <br>    &#125;<br>    cout&lt;&lt;res&lt;&lt;endl;<br>    cout&lt;&lt;h.<span class="hljs-built_in">top</span>().second&lt;&lt;endl;<br>    <br>    <br>    <br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/06/02/hello-world/"/>
    <url>/2024/06/02/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
