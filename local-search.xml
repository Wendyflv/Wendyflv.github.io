<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>网络优化与正则化</title>
    <link href="/2024/06/09/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/"/>
    <url>/2024/06/09/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/</url>
    
    <content type="html"><![CDATA[<h1 id="网络优化与正则化"><a href="#网络优化与正则化" class="headerlink" title="网络优化与正则化"></a>网络优化与正则化</h1><h2 id="网络优化"><a href="#网络优化" class="headerlink" title="网络优化"></a>网络优化</h2><p>网络优化的难点：</p><ol><li>不同网络的结构差异大，没有通用的优化算法，超参数多</li><li>非凸优化问题，如何继续参数初始化和逃离局部最优</li><li>梯度消失和梯度爆炸问题</li></ol><p>网络优化的方法</p><p>梯度下降法（GD）</p><ol><li><p>批量梯度下降法（BGD)</p><p>批量梯度下降得到的是一个所有训练数据上的全局最优解，每一次的参数更新都用到了所有的训练数据，如果训练数据非常多的话，执行效率较低。</p><p>$\theta_j^{‘} &#x3D; \theta_j + \frac{1}{m}\sum_{i&#x3D;1}^{m}(y^i - h_\theta(x^i))x_j^i$</p><p>缺点：处理大型数据缓慢，易导致内存溢出； 更新快慢由学习率决定，在非凸曲面中可能会趋于局部最优； </p></li><li><p>随机梯度下降法（SGD）</p><p>利用单个样本的损失函数对θ求偏导得到对应的梯度，来更新θ</p><p>$\theta_j^{‘} &#x3D; \theta_j + (y^i - h_\theta(x^i))x_j^i$</p><p>缺点：噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向，； 当数据较多时，训练缓慢。</p></li><li><p>小批量梯度下降法（Mini-Batch GD）</p><p>利用部分样本的损失函数对θ求偏导得到对应的梯度，来更新θ</p><p>for k &#x3D; 1, 11, 21, ..,,99 do</p><p>$\theta_j^{‘} &#x3D; \theta_j + \frac{1}{10}\sum_{i&#x3D;k}^{k+9}(y^i - h_\theta(x^i))x_j^i$</p><p>优点：能减少参数更新的波动，获得更好和更稳定的收敛</p></li></ol><p><img src="/2024/06/09/%E7%BD%91%E7%BB%9C%E4%BC%98%E5%8C%96%E4%B8%8E%E6%AD%A3%E5%88%99%E5%8C%96/image-20240609204358288.png" alt="image-20240609204358288"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>循环神经网络</title>
    <link href="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="递归神经网络RNN"><a href="#递归神经网络RNN" class="headerlink" title="递归神经网络RNN"></a>递归神经网络RNN</h2><p>RNN按照时间序列展开：</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609180740711.png" alt="image-20240609180740711"></p><h3 id="正向传播"><a href="#正向传播" class="headerlink" title="正向传播"></a>正向传播</h3><p>$t$时刻中间隐层输入$s_t &#x3D; Ux_t + Wh_{t-1}$</p><p>$t$​时刻中间隐层输出$h_t &#x3D; f(s_t)$​（其中$f$为sigmoid函数）</p><p>$t$时刻输出层输出$o_t &#x3D; g(Vh_t)$（其中$g$为softmax函数）</p><p>损失函数为$L_t &#x3D; -[y_tlogo_t + (1-y_t)log(1-o_t)]$</p><p>所有时间的损失为$L &#x3D; sum_{t&#x3D;1}^{T}L_t$</p><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>首先对$V$求导，直接得到：</p><p>$\frac{\partial L}{\partial V} &#x3D; \sum_{t&#x3D;1}^T \frac{\partial L_t}{\partial V} &#x3D; \sum_{t&#x3D;1}^{T} \frac{\partial L_t}{\partial o_t} \frac{\partial o_t}{\partial V} &#x3D;  \sum_{t&#x3D;1}^{T} -(\frac{y_t}{o_t} + \frac{y_t-1}{1-o_t}o_t(1-o_t)h_t^T) &#x3D;  \sum_{t&#x3D;1}^{T} (o_t - y_t)h_t^T$</p><p>对$W$求梯度，</p><p>$\frac{\partial L}{\partial W} &#x3D; \sum_{t&#x3D;1}^T \frac{\partial L_t}{\partial W}  &#x3D;\sum_{t&#x3D;1}^T \frac{\partial L_t}{\partial o_t} \sum_{k&#x3D;1}^t \frac{\partial o_t}{\partial h_t} \frac{\partial h_t}{\partial h_k} \frac{\partial h_k}{\partial W} &#x3D; \sum_{t&#x3D;1}^T \sum_{k&#x3D;1}^t \frac{\partial L_t}{\partial o_t}\frac{\partial o_t}{\partial h_t}(\prod_{j&#x3D;k+1}^t \frac{\partial h_j}{\partial h_{j-1}})\frac{\partial h_k}{\partial W}   $</p><p>依此类推出：</p><p>$\frac{\partial L}{\partial U} &#x3D; \sum_{t&#x3D;1}^T \sum_{k&#x3D;1}^t \frac{\partial L_t}{\partial o_t}\frac{\partial o_t}{\partial h_t}(\prod_{j&#x3D;k+1}^t \frac{\partial h_j}{\partial h_{j-1}})\frac{\partial h_k}{\partial U} $ </p><h3 id="弊端"><a href="#弊端" class="headerlink" title="弊端"></a>弊端</h3><p>传统RNN都采用反向传播时间算法（BPTT），随着时间流逝，网络层数增加，会产生梯度消失或者梯度爆炸的问题。</p><p>以$W$的梯度更新举例，使用激活函数假如是$tanh$</p><p>$\frac{\partial h_j}{\partial h_{j-1}}  &#x3D; W^T \odot tanh^{‘}， tanh^{‘} \in [0,1]$</p><p>👉梯度消失：如果$W$也是大于0小于1的数，当$t$很大时，$W^T \odot tanh^{‘} &lt;1 $，连乘起来就会趋于0.</p><p>👉梯度爆炸：如果梯度比较大的话（$\frac{\partial h_j}{\partial h_{j-1}} &gt; 1$），经过多层迭代，又会导致梯度大的不得了，比如$1,01^{100}$。</p><p>梯度消失和爆炸实际上导致了网络只能学习到<strong>短周期的依赖关系</strong>。</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609184920246.png" alt="image-20240609184920246"></p><p>随着时间的推移，对于 t&gt;1 时刻的产生的影响会越来越小，由图中的颜色的深浅代表信号的大小。这种衰减会导致 RNN 无法处理长期依赖。</p><h2 id="LSTM（长短时记忆神经网络Long-short-term-memory"><a href="#LSTM（长短时记忆神经网络Long-short-term-memory" class="headerlink" title="LSTM（长短时记忆神经网络Long short-term memory)"></a>LSTM（长短时记忆神经网络Long short-term memory)</h2><h3 id="与RNN的区别"><a href="#与RNN的区别" class="headerlink" title="与RNN的区别"></a>与RNN的区别</h3><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609185538836.png" alt="image-20240609185538836"></p><p><strong>·</strong> 原始RNN的隐藏层只有一个状态，即$h$，它对于短期输入非常敏感</p><p><strong>·</strong> 再增加一个状态$c$，来保存长期的状态，称为单元状态或者内部记忆单元，记录了当前时刻为止的所有历史信息。</p><h3 id="内部记忆单元C"><a href="#内部记忆单元C" class="headerlink" title="内部记忆单元C"></a>内部记忆单元C</h3><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609190147525.png" alt="image-20240609190147525"></p><ol><li><p>第一个开关，负责控制继续保存内部状态C（遗忘门）</p><p>遗忘门可以保存很久很久之前的信息</p><p>它决定了上一时刻的单元内部状态$c_{t-1}$有多少保留到当前内部时刻内部状态$c_t$</p></li><li><p>第二个开关，负责控制把当前内部候选状态输入到当前状态C（输入门）</p><p>它决定了当前时刻网络的输入$x_t$有多少保存到当前单元内部状态$c_t$</p></li><li><p>第三个开关，负责控制是否把内部状态C作为当前LSTM的输出（输出门）</p><p>它决定了内部状态$c_t$有多少输出到LSTM的当前输出值$h_t$</p></li></ol><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609190718664.png" alt="image-20240609190718664"></p><p><strong>核心思想</strong>：LSTM的关键在于细胞的状态整个(绿色的图表示的是一个cell)，和穿过细胞的那条水平线。</p><p>细胞状态类似于传送带。直接在整个链上运行，只有一些少量的线性交互。信息在上面流传保持不变会很容易。</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609190919991.png" alt="image-20240609190919991"></p><p>若只有上面的那条水平线是没办法实现添加或者删除信息的。而是通过一种叫做 门（gates） 的结构来实现的。</p><p>门可以实现选择性地让信息通过，主要是通过一个 sigmoid 的神经层 和一个逐点相乘的操作来实现的。</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609191001406.png" alt="image-20240609191001406"></p><h3 id="LSTM的3个门"><a href="#LSTM的3个门" class="headerlink" title="LSTM的3个门"></a>LSTM的3个门</h3><ol><li><p>遗忘门（控制内部记忆单元遗忘哪些历史信息）$f_t$</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609191204577.png" alt="image-20240609191204577"></p><p>$f^t &#x3D; \sigma(W_f \cdot h^{t-1} + U_f \cdot x^t + b_f)$</p></li><li><p>输入门（控制内部记忆单元加入多少新信息）</p><p><strong>Part1</strong> </p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609191533333.png" alt="image-20240609191533333"></p><p>$i^t  &#x3D; \sigma (W_i h^{t-1} + U_ix^t + b_i)$</p><p>$\tilde{c}^t  &#x3D;  tanh(W_c h^{t-1} + U_c x^t + b_c)$</p><p>Step1: 通过输入门的sigmoid层决定加入哪些新信息</p><p>Step2: 再由tanh层通过$X$和$h$值，生成一个候选记忆向量。 </p><p><strong>Part2</strong></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609192027548.png" alt="image-20240609192027548"></p><p>$c^t &#x3D; f^t * c^{t-1} + i^t * \tilde{c}^t$</p><p>新的内部记忆单元包括两部分<br>1 经过遗忘门过滤的旧状态信息<br>2 候选记忆向量与通过输入门决定的 $𝑖^𝑡$的乘积</p></li><li><p>输出门</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609192228601.png" alt="image-20240609192228601"></p><p>$o^t &#x3D; \sigma(W_o h^{t-1} + U_o x^t + b_o)$</p><p>$h^t &#x3D; o^t * tanh(c^t)$</p><p>$c^t$通过tanh函数，将输出信息控制在-1到1之间。</p><p><strong>总结：</strong></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609193920008.png" alt="image-20240609193920008"></p></li></ol><h3 id="缓解梯度消失和爆炸"><a href="#缓解梯度消失和爆炸" class="headerlink" title="缓解梯度消失和爆炸"></a>缓解梯度消失和爆炸</h3><p>由正向传播公式：$c^t &#x3D; f^t * c^{t-1} + i^t * \tilde{c}^t$</p><p>得到 $\frac{\partial c^{t+1}}{\partial c^t} &#x3D; f^t + …$</p><p>可以看到当$f^t &#x3D;1$时，就算其余项很小，梯度仍可以很好的导到上一时刻，此时即使层数较沈也不会发生梯度消失；当$f^t &#x3D; 0$时，即上一时刻的信号不影响当前时刻，梯度也不会传回去。</p><h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p>误差使用交叉熵函数</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609194737135.png" alt="image-20240609194737135"></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609200610067.png" alt="image-20240609200610067"></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609200711616.png" alt="image-20240609200711616"></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609201125891-17179350868121.png" alt="image-20240609201125891"></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609201155275.png" alt="image-20240609201155275"></p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609201223375.png" alt="image-20240609201223375"></p><h2 id="LSTM其他变体"><a href="#LSTM其他变体" class="headerlink" title="LSTM其他变体"></a>LSTM其他变体</h2><ol><li><p>合并遗忘门和输入门 $i_t + f_t$ &#x3D;1</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609201410947.png" alt="image-20240609201410947"></p></li><li><p>GRU</p><p><img src="/2024/06/09/%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240609201547466.png" alt="image-20240609201547466"></p><p>GRU只有两个门：更新门z和重置门r</p><p>更新门：遗忘多少历史信息和接受多少新信息。</p><p>重置门：候选状态中有多少信息是从历史信息中得到的。</p><p>与LSTM的比较：</p><ol><li>GRU少了一个门，也少了一个细胞状态$c_t$</li><li>GRU只需要重置门来控制是否要保留原来隐藏状态的信息，单步在限制当前信息的传入。</li><li>在 LSTM 中，虽然得到了新的细胞状态$ 𝒄_𝒕$，但是还不能直接输出，而是需要经过一个过滤的处理:；同样，在GRU 中, 虽然我们也得到了新的隐藏状态$\tilde{h}_t$， 但是还不能直接输出，而是通过更新门来控制最后的输出。</li></ol></li></ol><h1 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h1><ol><li>RNN构造</li><li>RNN的梯度消失和梯度爆炸产生原因</li><li>LSTM结构及核心思想</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>卷积神经网络</title>
    <link href="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>卷积经常用在信号处理中，用于计算信号的延迟累积。</p><h3 id="一维卷积"><a href="#一维卷积" class="headerlink" title="一维卷积"></a>一维卷积</h3><p>时刻t收到的信号$y_t$ 为当前时刻产生的信息和以前时刻延迟信息的叠加: $y_t &#x3D; 1 \times x_t + 1&#x2F;2 \times x_{t-1} + 1&#x2F;4 \times x_{t-2}&#x3D; w_1x_t + w_2x_{t-1}+w_3 x_{t-2} &#x3D; \sum_{k&#x3D;1}^{3}w_k x_{t-k+1}$</p><p>$w_k$称为滤波器或者卷积核</p><p>给定一个输入信号序列$x$和滤波器$w$，卷积的输出：$y_t &#x3D; \sum_{k&#x3D;1}^K w_k x_{t-k+1}$</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608183211215.png" alt="image-20240608183211215"></p><p>不同的滤波器来提取信号序列中的不同特性</p><p>👉低通滤波：滤波器$[1&#x2F;3, 1&#x2F;3, 1&#x2F;3]$可以检测信号序列中的低频信息（如均值）</p><p>👉高通滤波：滤波器$[1, -2, 1]$可以检测信号中的高频信息（如边缘）</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608183710865.png" alt="image-20240608183710865"></p><p>引入滑动步长$S$和零填充$P$:</p><p>$S&#x3D;2$即卷积核每隔2步做一次卷积运算</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608183937333.png" alt="image-20240608183937333"></p><p>$P&#x3D;1$即原数据左右两边各填充一个0（避免重要信息的丢失）</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608184031995.png" alt="image-20240608184031995"></p><p><strong>卷积类型</strong>：按照输出长度可分为3类</p><p>$M$为输入序列的长度，$K$为窗口大小</p><ol><li>宽卷积：步长$S&#x3D;1$，两边不补零$P&#x3D;0$，卷积后输出长度为$M-K+1$</li><li>窄卷积：步长$S&#x3D;1$，两边补零$P&#x3D;K-1$，卷积输出长度为$M+K-1$</li><li>等宽卷积：步长$S&#x3D;1$，两端补零$P&#x3D;(K-1)&#x2F;2$，卷积后输出长度为$M$</li></ol><p>对一个步长为$S$，填充为$P$的卷积，输出的长度为：$|\frac{M+2P-K}{2}| + 1$</p><h3 id="二维卷积"><a href="#二维卷积" class="headerlink" title="二维卷积"></a>二维卷积</h3><p>在图像处理中，图像是以二维矩阵的形式输入到神经网络中，因此我们需要二维卷积。</p><p>一个输入信息$X$和滤波器$W$的二维卷积定义为：$Y&#x3D;W * X$</p><p>$y_{ij} &#x3D; \sum_{u&#x3D;1}^{U}\sum_{v&#x3D;1}^{V} w_{uv}x_{i-u+1, j-v+1}$</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608185512974.png" alt="image-20240608185512974"></p><p>步长和填充不同的情况：</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608190629458.png" alt="image-20240608190629458"></p><h2 id="卷积神经网络-1"><a href="#卷积神经网络-1" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h2><p>网络结构：</p><p>👉数据输入层（Input layer）</p><p>👉<a href="#convolution">卷积计算层（CONV layer）</a></p><ol><li>局部特征提取</li><li>训练中参数学习</li><li>每个卷积核提取特定模式的特征</li></ol><p>👉ReLU激励层（ReLU layer）</p><p>👉<a href="#pool">池化层（Pooling layer）</a></p><ol><li>降低数据维度，避免过拟合</li><li>增强局部感受野</li><li>提高平移不变性</li></ol><p>👉全连接层（FC layer）：特征提取到分类的桥梁</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608191126721.png" alt="image-20240608191126721"></p><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p><span id="convolution">卷积</span>代替全连接层  $h^{l+1} &#x3D; f(w * h^l + b)$</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608191945394.png" alt="image-20240608191945394"><strong>局部连接，权重共享，空间或者时间上的次采样</strong></p><p><strong>卷积与协相关</strong>：</p><p>协相关：核核输入数据对应相乘再求和</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608192448907.png" alt="image-20240608192448907"></p><p>卷积：核先翻转180度，再做协相关</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608192458655.png" alt="image-20240608192458655"></p><p>除非特殊声明，计算结果按照协相关来。</p><p><strong>多个卷积核</strong>：使用多个卷积核能增强卷积层的提取不同特征的能力</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608193340547.png" alt="image-20240608193340547"></p><p>输入$[7 \times 7 \times3] \rightarrow $ 经过卷积核$w_0[3 \times 3 \times 3]+b_0, w_1[3 \times 3 \times 3]+b_1 \rightarrow$ 结果$[3 \times 3 \times 2]$</p><p>(两个卷积核的大小必须一致)</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608194027267.png" alt="image-20240608194027267"></p><p>3通道图像的卷积层：经过多少个卷积核，就会产生多少通道特征图</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608194917125.png" alt="image-20240608194917125"></p><h3 id="池化层或汇聚层（Pooling-Layers）"><a href="#池化层或汇聚层（Pooling-Layers）" class="headerlink" title="池化层或汇聚层（Pooling Layers）"></a>池化层或汇聚层（Pooling Layers）</h3><p>卷积层虽然可以显著减少连接的个数，但是每一个特征映射的神经元个数并没有显著减少。</p><p>下面是最大<span id="pool">池化是</span>的作用方式：</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608195911957.png" alt="image-20240608195911957"></p><p>卷积网络是由卷积层、汇聚层、全连接层交叉堆叠而成的</p><p>经典结构如下：一个卷积块为连续$M$个卷积层和$b$个汇聚层。一个卷积网络可以堆叠$N$个连续的卷积块，然后在接着$K$个全连接层。</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608200153512.png" alt="image-20240608200153512"></p><h2 id="误差反向传播"><a href="#误差反向传播" class="headerlink" title="误差反向传播"></a>误差反向传播</h2><h3 id="卷积层的反向传播"><a href="#卷积层的反向传播" class="headerlink" title="卷积层的反向传播"></a>卷积层的反向传播</h3><ol><li>误差的传播</li></ol><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608201429497.png" alt="image-20240608201429497"></p><ol start="2"><li><p>权重梯度的计算</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608201549172.png" alt="image-20240608201549172"></p></li></ol><h3 id="池化层的反向传播"><a href="#池化层的反向传播" class="headerlink" title="池化层的反向传播"></a>池化层的反向传播</h3><p>误差的传播</p><p>对最大池化，下一层的误差项会原封不动的传给上一层对应区块的最大值所对应的神经元，而其他神经元的误差项都是0。（平均池化就是每个误差项系数为1&#x2F;K）</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608201707278.png" alt="image-20240608201707278"></p><h2 id="其他卷积种类"><a href="#其他卷积种类" class="headerlink" title="其他卷积种类"></a>其他卷积种类</h2><h3 id="空洞卷积"><a href="#空洞卷积" class="headerlink" title="空洞卷积"></a>空洞卷积</h3><p>通过给卷积核插入“空洞”来变相地增加其大小，以增加输出单元的感受野</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608203139129.png" alt="image-20240608203139129"></p><h3 id="转置卷积-微步卷积"><a href="#转置卷积-微步卷积" class="headerlink" title="转置卷积&#x2F;微步卷积"></a>转置卷积&#x2F;微步卷积</h3><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608203709599.png" alt="image-20240608203709599"></p><h3 id="可分离卷积"><a href="#可分离卷积" class="headerlink" title="可分离卷积"></a>可分离卷积</h3><p>👉空间可分离卷积</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608203931153.png" alt="image-20240608203931153"></p><p>👉深度可分离卷积</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608204002547.png" alt="image-20240608204002547"></p><h2 id="经典卷积网络"><a href="#经典卷积网络" class="headerlink" title="经典卷积网络"></a>经典卷积网络</h2><p><strong>残差网络</strong></p><p>残差网络（Residual Network，ResNet）是通过给非线性的卷积层增加<strong>直连边</strong>的方式来提高信息的传播效率。</p><p><img src="/2024/06/08/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608204534887.png"></p><p><strong>好处：</strong></p><ol><li>能够解决梯度消失问题，适用于深层结构。</li><li>加速了模型训练收敛速度，使得模型更容易地学习到恒等映射（identity mapping）或者近似于恒等映射的变换。</li><li>即学习输入与目标之间的差异，而不是直接学习输出。这样的设计使得网络可以更好地适应复杂的数据分布，提高了网络的表达能力</li></ol><h2 id="CNN的优缺点"><a href="#CNN的优缺点" class="headerlink" title="CNN的优缺点"></a>CNN的优缺点</h2><p><strong>优点：</strong></p><ol><li>共享卷积核，对高维数据处理无压力</li><li>无需手动提取特征，训练好合适的权重，可以获取好的特征</li><li>分类效果好</li></ol><p><strong>缺点：</strong></p><ol><li>需要调参，还需要大样本，训练要GPU</li><li>物理含义不明确</li></ol><h2 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h2><ol><li>卷积网络的三个结构特征：共享权值，局部连接，时间或空间上的次采样</li><li>卷积的定义，以及各式各样的卷积</li><li>卷积网络结构（卷积块的构成+全连接层）</li><li>理解残差网络的机理</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>基于图片生成古诗</title>
    <link href="/2024/06/07/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E5%8F%A4%E8%AF%97/"/>
    <url>/2024/06/07/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%89%87%E7%94%9F%E6%88%90%E5%8F%A4%E8%AF%97/</url>
    
    <content type="html"><![CDATA[<h1 id="基于图片生成古诗"><a href="#基于图片生成古诗" class="headerlink" title="基于图片生成古诗"></a>基于图片生成古诗</h1><h2 id="实现方法"><a href="#实现方法" class="headerlink" title="实现方法"></a>实现方法</h2><p>我们在看图说话时，一般都是先看看图片上有什么内容，比如一张图里有山有水还有很多的植物，那我们对这张图的第一印象就是一张风景图，然后我们再根据图片上某一个具体的景物做详细的描述。</p><p>想让机器看图生成古诗，可以表示为如下步骤：</p><ol><li>对图像分类，比如这个图像是风景图，再确切一点，是有湖、骛、荷花的风景图。这样就可以得到这个图的关键词。</li><li>由上一步得到的关键词，通过古诗生成模型，来生成有关图片的古诗。</li></ol><p>下面，将把这个项目分成以下几个部分：图片分类、古诗生成、网页搭建。</p><h3 id="图片分类"><a href="#图片分类" class="headerlink" title="图片分类"></a>图片分类</h3>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>大模型的高效微调</title>
    <link href="/2024/06/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/"/>
    <url>/2024/06/05/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%AB%98%E6%95%88%E5%BE%AE%E8%B0%83/</url>
    
    <content type="html"><![CDATA[<h1 id="参数高效微调"><a href="#参数高效微调" class="headerlink" title="参数高效微调"></a>参数高效微调</h1><h1 id="Prompt"><a href="#Prompt" class="headerlink" title="Prompt-"></a>Prompt-</h1>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数据结构-最小生成树的生成</title>
    <link href="/2024/06/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/"/>
    <url>/2024/06/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/</url>
    
    <content type="html"><![CDATA[<h1 id="最小生成树的生成"><a href="#最小生成树的生成" class="headerlink" title="最小生成树的生成"></a>最小生成树的生成</h1><p>定义：把无向图的全部n个顶点和n-1条边构成的生成树，其边的权值最小的树称为该图的最小生成树。</p><p>实现方法：</p><ol><li>prim方法：从任意一个起点开始，以贪心的策略，每次只选择一个距离当前联通部分最小的点加入生成树，直至所有点都被加入该树为止。</li><li>kruskal方法：首先把所有的边按权重从小到大排序，然后从权重最小的边开始添加，只要不会形成环就添加这条边，直到所有的点都连接起来。</li></ol><h2 id="Prim方法"><a href="#Prim方法" class="headerlink" title="Prim方法"></a>Prim方法</h2><p><img src="/2024/06/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/image-20240605185023203.png" alt="image-20240605185023203"></p><p>首先，我们需要任意选择一个起点，这里选节点1作为起点。dist[1] &#x3D; 0，其余的dist全为0x3f</p><p>然后，从所有节点的dist数组里面找到最小的节点加到联通部分。</p><p>更新所有与该节点相连的点的dist数组。</p><p>一次更新过程如下图所示</p><p><img src="/2024/06/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/image-20240605191803958.png" alt="image-20240605191803958"></p><p>下一次更新：</p><p><img src="/2024/06/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/image-20240605192122070.png" alt="image-20240605192122070"></p><p>下面是代码实现：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;algorithm&gt;</span></span><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">510</span>;<br><span class="hljs-type">int</span> g[N][N];<span class="hljs-comment">//存储图</span><br><span class="hljs-type">int</span> dist[N];<span class="hljs-comment">//存储每个节点到当前联通部分的距离</span><br><span class="hljs-type">int</span> state[N];<span class="hljs-comment">//存储每个点是否被加入联通部分中</span><br><span class="hljs-type">int</span> pre[N];<span class="hljs-comment">//每个点联通时的前置节点</span><br><span class="hljs-type">int</span> n,m;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">getpath</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> res = <span class="hljs-number">0</span>;<br>    <span class="hljs-comment">//先将所有节点的前置节点置-1</span><br>    <span class="hljs-built_in">memset</span>(pre, <span class="hljs-number">-1</span>, <span class="hljs-built_in">sizeof</span>(pre));<br>    <span class="hljs-built_in">memset</span>(dist, <span class="hljs-number">0x3f</span>, <span class="hljs-built_in">sizeof</span>(dist));<br>    <span class="hljs-comment">//将1节点的距离置0</span><br>    dist[<span class="hljs-number">1</span>] = <span class="hljs-number">0</span>;<br>    <br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> t= <span class="hljs-number">0</span>; t&lt;n; t++)&#123;<br>        <span class="hljs-comment">//查找所有节点中距离联通部分最近的点，加入联通部分</span><br>        <span class="hljs-type">int</span> midist = <span class="hljs-number">0x3f</span>;<br>        <span class="hljs-type">int</span> index = <span class="hljs-number">-1</span>;<br>        <span class="hljs-comment">//设置t=-1是为了保留孤立点</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>; i&lt;=n; i++)&#123;<br>            <span class="hljs-keyword">if</span>((index==<span class="hljs-number">-1</span> || dist[i]&lt;dist[index]) &amp;&amp; state[i]==<span class="hljs-number">0</span>)&#123;<br>                midist = dist[i];<br>                index = i;<br>            &#125;<br>            <br>        &#125;<br>        <span class="hljs-comment">//如果是孤立点 无法产生最小生成树</span><br>        <span class="hljs-keyword">if</span>(dist[index] == <span class="hljs-number">0x3f3f3f3f</span>)&#123;<br>            cout&lt;&lt;<span class="hljs-string">&quot;impossible&quot;</span>&lt;&lt;endl;<br>            <span class="hljs-keyword">return</span>;<br>        &#125;<br>        <span class="hljs-comment">//此时找到index节点，将其加入联通部分</span><br>        state[index] = <span class="hljs-number">1</span>;<br>        <span class="hljs-comment">//cout&lt;&lt;&quot;index: &quot;&lt;&lt;index&lt;&lt;&quot; midist: &quot;&lt;&lt;midist&lt;&lt;endl;</span><br>        res += midist;<br>        <span class="hljs-comment">//更新所有与indx节点相连的点 到联通部分的距离</span><br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>; i&lt;=n; i++)&#123;<br>            <span class="hljs-comment">//cout&lt;&lt;&quot;dist[i]&quot;&lt;&lt;dist[i]&lt;&lt;&quot; g[i][index]&quot;&lt;&lt;g[i][index]&lt;&lt;endl;</span><br>            <span class="hljs-keyword">if</span>(state[i]==<span class="hljs-number">0</span> &amp;&amp; (dist[i] &gt; g[i][index]))&#123;<br>                dist[i] = <span class="hljs-built_in">min</span>(dist[i], g[i][index]);<br>                <span class="hljs-comment">//cout&lt;&lt;&quot;i: &quot;&lt;&lt;i&lt;&lt;&quot;dist[i]&quot;&lt;&lt;dist[i]&lt;&lt;endl;</span><br>                <br>            &#125;<br>        &#125;<br>    <br>    &#125;<br>    cout&lt;&lt;res&lt;&lt;endl;<br>    <br>    <br>    <br>    <br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <br>    <span class="hljs-built_in">memset</span>(g, <span class="hljs-number">0x3f</span>, <span class="hljs-built_in">sizeof</span>(g));<br>    <br>    cin&gt;&gt;n&gt;&gt;m;<br>    <span class="hljs-type">int</span> a,b,w;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;m; i++)&#123;<br>        cin&gt;&gt;a&gt;&gt;b&gt;&gt;w;<br>        g[a][b] = g[b][a] =<span class="hljs-built_in">min</span>(w, g[a][b]);<br>    &#125;<br>    <br>    <span class="hljs-built_in">getpath</span>();<br>    <br>    <br>    <br>&#125;<br></code></pre></td></tr></table></figure><h2 id="kruskal方法"><a href="#kruskal方法" class="headerlink" title="kruskal方法"></a>kruskal方法</h2><p><strong>方法</strong>：将所有边按从小到大的顺序排序，一条一条边的尝试加入结果集，如果正在尝试加入的这条边加入后会产生环路，则放弃该边，选择下一条边，直至选完n-1条边。</p><p><img src="/2024/06/05/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E6%9C%80%E5%B0%8F%E7%94%9F%E6%88%90%E6%A0%91%E7%9A%84%E7%94%9F%E6%88%90/image-20240605202553078.png" alt="image-20240605202553078"></p><p><strong>判断环路产生</strong>：使用并查集的方法。使用一个代表数组存储数据，如p&#x3D;[0,0,0,3,3]的意思是原数组a[0]&#x3D;a[1]&#x3D;a[2]，他们的代表元素是a[0]；a[3]&#x3D;a[4],他们的代表元素是a[3]。</p><ol><li><strong>并</strong>：如把[1,2,3]和[4,5]合并，只需要把p数组的p[3]修改成0即可</li><li><strong>查</strong>：修改后，如果想查a[4]，通过查找p数组，发现a[4]&#x3D;a[3], 再查找p数组a[3]&#x3D; a[0]，由此就查到了a[4]&#x3D; a[0]</li></ol><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs C++"># 找到a的代表元素<br><span class="hljs-meta"># p[0,0,0,1,3] 如果想找到4的代表元素，p[4]= p[3]=p[1]=p[0]</span><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">find</span><span class="hljs-params">(a)</span></span><br><span class="hljs-function"></span>&#123;<br>    <span class="hljs-keyword">if</span>(a!=p[a])<br>    &#123;<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">find</span>(p[a]);<br>    &#125;<br>    <span class="hljs-keyword">return</span> p[a];<br>&#125;<br></code></pre></td></tr></table></figure><p>代码实现：</p><p>边的存储：使用结构体struct E{int a,  b,  w;}</p><p>排序边：构造compare(const E &amp; a, const E &amp; b)函数比较</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;algorithm&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstring&gt;</span></span><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-keyword">struct</span> <span class="hljs-title class_">Edge</span>&#123;<br>    <span class="hljs-type">int</span> a,b,w;<br>&#125;edg[<span class="hljs-number">200010</span>];<br><br><span class="hljs-type">int</span> p[<span class="hljs-number">200010</span>];<br><span class="hljs-type">int</span> n,m;<br><br><span class="hljs-function"><span class="hljs-type">bool</span> <span class="hljs-title">compare</span><span class="hljs-params">(Edge a, Edge b)</span></span>&#123;<br>    <span class="hljs-keyword">return</span> a.w &lt; b.w;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">findp</span><span class="hljs-params">(<span class="hljs-type">int</span> a)</span></span>&#123;<br>    <span class="hljs-keyword">if</span>(a != p[a])&#123;<br>        p[a] = <span class="hljs-built_in">findp</span>(p[a]);<br>    &#125;<br>    <span class="hljs-keyword">return</span> p[a];<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">kruskal</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-type">int</span> res = <span class="hljs-number">0</span>;<br>    <span class="hljs-type">int</span> cnt =<span class="hljs-number">0</span> ;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i =<span class="hljs-number">1</span>; i&lt;=m; i++)&#123;<br>        <span class="hljs-type">int</span> pa = <span class="hljs-built_in">findp</span>(edg[i].a);<br>        <span class="hljs-type">int</span> pb = <span class="hljs-built_in">findp</span>(edg[i].b);<br>        <span class="hljs-keyword">if</span>(pa!=pb)&#123;<br>            res += edg[i].w;<br>            p[pa] = pb;<br>            cnt ++;<br>            <br>        &#125;<br>       <br>        <br>    &#125;<br>    <br>    <span class="hljs-keyword">if</span>(cnt&lt;n<span class="hljs-number">-1</span>)&#123;<br>        cout&lt;&lt;<span class="hljs-string">&quot;impossible&quot;</span>&lt;&lt;endl;<br>       <br>    &#125;<br>    <span class="hljs-keyword">else</span> &#123;<br>        cout&lt;&lt;res&lt;&lt;endl;<br>        <br>    &#125;<br>&#125;<br>    <br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin&gt;&gt;n&gt;&gt;m;<br>    <br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>; i&lt;=m; i++)&#123;<br>        p[i] = i;<br>    &#125;<br>    <br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>; i&lt;=m; i++)&#123;<br>        cin&gt;&gt;edg[i].a&gt;&gt;edg[i].b&gt;&gt;edg[i].w;<br>    &#125;<br>    <br>    <span class="hljs-built_in">sort</span>(edg+<span class="hljs-number">1</span>, edg+m+<span class="hljs-number">1</span>, compare);<br>    <br>    <span class="hljs-built_in">kruskal</span>();<br>    <br>    <br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>大语言模型介绍（一）</title>
    <link href="/2024/06/04/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2024/06/04/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h1 id="大语言模型"><a href="#大语言模型" class="headerlink" title="大语言模型"></a>大语言模型</h1><h1 id="语言模型的发展历程"><a href="#语言模型的发展历程" class="headerlink" title="语言模型的发展历程"></a>语言模型的发展历程</h1><p>语言模型旨在对于人类语言的内在规律进行建模，从而准确预测 词序列中未来（或缺失）词或词元（Token）的概率。根据所采用技术方法的不同， 针对语言模型的研究工作可以分为以下四个主要发展阶段：</p><ol><li><p>**统计语言模型（SLM)**：统计语言模型使用马尔可夫假设（MarkovAssumption）来建立语言序列的预测模型，通常是根据词序 列中若干个连续的上下文单词来预测下一个词的出现概率，即根据一个固定长度的前缀来预测目标单词。</p><p>如：N-gram模型，<strong>某一个词语出现的概率只由其前面的n−1个词语所决定</strong>。</p><p>$p(S)&#x3D;p(w_1w_2⋯w_n)&#x3D;p(w_1)p(w_2∣w_1)⋯p(w_n∣w_n−1)$</p><p>$p(冰激凌|我要去吃)&#x3D;p(冰激凌|吃)$ </p><p>对于高阶统计语言模型来说，随着阶数𝑛的增加，需要估计的转移概率项数将会指数级增长，经常会受到“维数灾难”（CurseofDimensionality） 的困扰。</p></li><li><p>**神经语言模型（NLM)**：神经语言模型使用神经网络来建模文本序列的生成，如循环神经网络（RecurrentNeuralNetworks,RNN）。通过引入WordEmbedding技术，能够有效克服统计语言模型中的数据稀疏问题，稠密向量的非零表征对于复杂语言模型的搭建非常友好。</p><p>Word2vec 是一个两层神经网络，通过“向量化”单词来处理文本。它的<strong>输入是一个文本语料库</strong>，它的<strong>输出是一组向量</strong>：表示该语料库中单词的特征向量。</p><p><img src="/2024/06/04/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D%EF%BC%88%E4%B8%80%EF%BC%89/image-20240604214251379.png" alt="image-20240604214251379"></p></li><li><p><strong>预训练语言模型（PLM)<strong>：与早期的词嵌入模型相 比，预训练语言模型在</strong>训练架构</strong>与<strong>训练数据</strong>两个方面进行了改进与创新。ELMo使用大量的无标注数据训练双向LST网络预训练完成后所得到的biLSTM可以用来学 习上下文感知的单词表示。进一 步，ELMo可以根据下游任务数据对biLSTM网络进行微调（Fine-Tuning），从而实现面向特定任务的模型优化。在2017 年，谷歌提出了基于自注意力机制（Self-Attention）的Transformer模型，通过自注意力机制建模长程序列关系。基于Transformer架构，谷歌进一步提出了预训练语言模型BERT，采用了仅有编码器的Transformer架构，并通过在大规模无标注数据上使用专门设计的预训练任务来学习双向语言模型。GPT-1采用了仅有解码器的Transformer架构，以及基于下一个词元预测的预训练任务进行模型的训练。</p><p>以ELMo、BERT、GPT-1为代表的预训练语言模型确立了“<strong>预训练-微调</strong>”这一任务求解范式。其中，预训练阶段旨在通过大规模无标注文本建立模型 的基础能力，而微调阶段则使用有标注数据对于模型进行特定任务的适配，从而更好地解决下游的自然语言处理任务。</p><p><img src="/2024/06/04/%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E4%BB%8B%E7%BB%8D%EF%BC%88%E4%B8%80%EF%BC%89/image-20240604214907303.png" alt="image-20240604214907303"></p></li><li><p><strong>大语言模型（LLM)<strong>： 研究人员发现，通过规模扩展 （如增加模型参数规模或数据规模）通常会带来下游任务的模型性能提升，这种现象通常被称为“</strong>扩展法则</strong>”（ScalingLaw）。一些研究工作尝试训练更大的预 训练语言模型（例如175B参数的GPT-3和540B参数的PaLM）来探索扩展语言 模型所带来的性能极限。这些大规模的预训练语言模型在解决复杂任务时表现出 了与小型预训练语言模型（例如330M参数的BERT和1.5B参数的GPT-2）不同 的行为。</p></li></ol><h2 id="大语言模型的构建过程"><a href="#大语言模型的构建过程" class="headerlink" title="大语言模型的构建过程"></a>大语言模型的构建过程</h2><p>神经网络是一种具有特定模型结构的函数形式，而大语言模型则是一种基于 Transformer 结构的神经网络模型。因此，可以将大语言模型看作一种拥有大规模参数的函数，它的构建过程就是使用训练数据对于模型参数的拟合过程。  大语言模型的优化目标更加泛化，不仅仅是为了解决某一种或者某一类特定任务，而是希望能够<strong>作为通用任务的求解器</strong>。为了实现这一宏大的目标，大语言模型的构建过程需要更为复杂、精细的训练方法。一般来说，这个训练过程可以分为<strong>大规模预训练</strong>和<strong>指令微调与人类对齐</strong>两个阶段 。</p><h3 id="大规模预训练"><a href="#大规模预训练" class="headerlink" title="大规模预训练"></a>大规模预训练</h3><p>预训练是指使用与下游任务无关的大规模数据进行模型参数的初始训练，可以认为是<strong>为模型参数找到一个较好的“初值点”</strong>。 大规模预训练本质上是在做一个世界知识的压缩，从而能够学习到一个<strong>编码世界知识</strong>的参数模型，这个模型能够通过解压缩所需要的知识来解决真实世界的任务。  </p><p>为了预训练大语言模型，需要准备大规模的文本数据，并且进行严格的清洗，去除掉可能包含有毒有害的内容，最后将清洗后的数据进行词元化（Tokenization）流，并且切分成批次（Batch），用于大语言模型的预训练。  </p><h3 id="指令微调与人类对齐"><a href="#指令微调与人类对齐" class="headerlink" title="指令微调与人类对齐"></a>指令微调与人类对齐</h3><p>但是由于预训练任务形式所限，这些模型更擅长于文本补全，并不适合直接解决具体的任务。比较广泛使用的微调技术是“指令微调”（也叫做有监督微调， Supervised Fine-tuning, SFT），通过使用任务输入与输出的配对数据进行模型训练，可以使得语言模型较好地掌握通过问答形式进行任务求解的能力。 与预训练相比，指令微调通常来说需要的指令实例数据规模要小的多。  </p><p> 除了提升任务的解决能力外，还需要将大语言模型与人类的期望、需求以及价值观对齐（Alignment），这对于大模型的部署与应用具有重要的意义。OpenAI在 2022 年初发布了 InstructGPT 的学术论文，系统地介绍了如何将语言模型进行人类对齐。具体来说，主要引入了基于人类反馈的强化学习对齐方法 RLHF （Reinforcement Learning from Human Feedback），在指令微调后使用强化学习加强模型的对齐能力。在 RLHF 算法中，需要训练一个符合人类价值观的奖励模型（Reward Model）。    </p><h3 id="扩展法则"><a href="#扩展法则" class="headerlink" title="扩展法则"></a>扩展法则</h3><p>  大语言模型采用了与小型预训练语言模型相似的神经网络结构（基于注意力机制的 Transformer 架构）和预训练方法（如语言建模）。但是通过<strong>扩展参数规模、数据规模和计算算力</strong>，大语言模型的能力显著超越了小型语言模型的能力。有趣的是，这种通过扩展所带来的性能提升通常显著高于通过改进架构、算法等方面所带来的改进。  </p><p>当算力 𝐶 给定的情况下，最优的模型参数规模和数据规模由指数系数 𝑎 和 𝑏 分别确定。 𝑎 和 𝑏 决定了参数规模和数据规模的资源分配优先级：当 𝑎 &gt; 𝑏时，应该用更多的算力去提高参数规模；当 𝑏 &gt; 𝑎 时，应该用更多的算力去提高数据规模。尽管 KM 扩展法则和 Chinchilla 扩展法则具有相似的公式形式，但是在模型规模和数据规模的扩展上存在一定的差异。随着算力预算的增加， KM 扩展法则（𝑎 ≈ 0.73, 𝑏 ≈ 0.27  ）倾向于将更大的预算分配给模型规模的增加，而不是分配给数据规模的增加；而 Chinchilla 扩展法则主张两种规模参数应该以等比例关系增加（𝑎 ≈ 0.46, 𝑏 ≈ 0.54 ）。</p><p>越来越多的工作表明，现有的预训练语言模型对于数据的需求量远高于这些扩展法则中所给出的估计规模。例如， LLaMA-2 (7B) 的模型就使用了 2T 的词元进行训练，很多更小的模型也能够通过使用超大规模的预训练数据获得较大的模型性能提升。  </p><h3 id="涌现能力"><a href="#涌现能力" class="headerlink" title="涌现能力"></a>涌现能力</h3><p>大语言模型的涌现能力被非形式化定义为“在小型模型中不存在但在大模型中出现的能力”，具体是指当模型扩展到一定规模时，模型的特定任务性能突然出现显著跃升的趋势，远超过随机水平。  </p><ol><li><p>上下文学习</p><p>上下文学习能力在 GPT-3 的论文中被正式提出。具体方式为，在提示中为语言模型提供自然语言指令和多个任务示例（Demonstration），无需显式的训练或梯度更新，仅输入文本的单词序列就能为测试样本生成预期的输出。  </p></li><li><p>指令遵循</p><p>指令遵循能力是指大语言模型能够按照自然语言指令来执行对应的任务。为了获得这一能力，通常需要使用自然语言描述的多任务示例数据集进行微调，称为指令微调（Instruction Tuning）或监督微调（Supervised Fine-tuning）。通过指令微调，大语言模型可以在没有使用显式示例的情况下按照任务指令完成新任务，有效提升了模型的泛化能力。  </p></li><li><p>逐步推理</p><p>对于小型语言模型而言，通常很难解决涉及多个推理步骤的复杂任务（如数学应用题），而大语言模型则可以利用思维链（Chain-of-Thought, CoT）提示策略来加强推理性能。</p></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>数据结构-图的创建与遍历</title>
    <link href="/2024/06/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%81%8D%E5%8E%86/"/>
    <url>/2024/06/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%81%8D%E5%8E%86/</url>
    
    <content type="html"><![CDATA[<h1 id="有向图的拓扑排序"><a href="#有向图的拓扑排序" class="headerlink" title="# 有向图的拓扑排序"></a># 有向图的拓扑排序</h1><h2 id="图的创建与遍历"><a href="#图的创建与遍历" class="headerlink" title="图的创建与遍历"></a>图的创建与遍历</h2><p>给定节点数和边数，以及构成每条边的两个节点。</p><p><strong>构造图</strong>：</p><p><img src="/2024/06/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%81%8D%E5%8E%86/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240603193735331.png" alt="image-20240603193735331"></p><p>要存储上面的图，需要h[N], e[N], ne[N]来表示</p><span id="more"></span><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;cstring&gt;</span></span><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br><span class="hljs-type">int</span> h[N];<span class="hljs-comment">//存放节点a第一个邻接节点的索引idx</span><br><span class="hljs-type">int</span> e[N];<span class="hljs-comment">//存放索引为idx的边的指向的节点值a</span><br><span class="hljs-type">int</span> ne[N];<span class="hljs-comment">//存放索引idx的与其共享同一个起始点的边的索引idx</span><br><span class="hljs-type">int</span> idx;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-type">int</span> a, <span class="hljs-type">int</span> b)</span></span>&#123;<br>    e[idx] = b;<br>    ne[idx] = h[a];<br>    h[a] = idx++;<br>    <br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">memset</span>(h, <span class="hljs-number">-1</span>, <span class="hljs-built_in">sizeof</span>(h));<br>    <span class="hljs-type">int</span> n,m;<br>    cin&gt;&gt;n&gt;&gt;m;<br>    <span class="hljs-type">int</span> a,b;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;m; i++)&#123;<br>        cin&gt;&gt;a&gt;&gt;b;<br>        <span class="hljs-built_in">add</span>(a,b);<br>    &#125;<br>    <br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">1</span>; i&lt;=n; i++)&#123;<br>        <span class="hljs-comment">//对点i求其所连接的边</span><br>        cout&lt;&lt;<span class="hljs-string">&quot;以节点&quot;</span>&lt;&lt;i&lt;&lt;<span class="hljs-string">&quot;为起始点，与其相邻的边有：&quot;</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = h[i]; j!=<span class="hljs-number">-1</span>; j = ne[j])&#123;<br>            <span class="hljs-type">int</span> eb = e[j];<br>            cout&lt;&lt;eb&lt;&lt;<span class="hljs-string">&quot; &quot;</span>;<br>            <br>        &#125;<br>        cout&lt;&lt;endl;<br>        <br>    &#125; <br>    <br>    <br>&#125;<br></code></pre></td></tr></table></figure><p><img src="/2024/06/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%81%8D%E5%8E%86/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240603193821500.png" alt="image-20240603193821500"></p><h2 id="拓扑排序"><a href="#拓扑排序" class="headerlink" title="拓扑排序"></a>拓扑排序</h2><p><strong>拓扑排序</strong>：</p><p>若一个由图中所有点构成的序列 A 满足：对于图中的每条边 (𝑥,𝑦)，𝑥 在 𝐴 中都出现在 𝑦 之前，则称 𝐴 是该图的一个拓扑序列。</p><p>如：下面的图，能找到序列：1 —— 2 —— 4 —— 3 ——5构成一条拓扑序列</p><p><img src="/2024/06/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84-%E5%9B%BE%E7%9A%84%E5%88%9B%E5%BB%BA%E4%B8%8E%E9%81%8D%E5%8E%86/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240603194203015.png" alt="image-20240603194203015"></p><p><strong>判断方法</strong>：</p><ol><li>删除一个入度为0的节点，加入结果集合</li><li>将上述节点对应的有向边都删除，重复第一步</li><li>直到最后没有节点，则该图可找出拓扑序列</li></ol><p><strong>代码实现重点</strong>：</p><ol><li>现把入度为0的节点加到集合的，后面删除该节点对应的边时要先删除：可以使用队列存储</li><li>删除节点a对应的有向边：可以遍历a为起始的连接边，将这些对应的点的入度-1即可</li><li>直到最后没有节点：队列为空时，结果集为整个图的节点个数</li></ol><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>用一个d[N]数组来存储每个点的入度数</p><p>队列q存储所有入度为0的节点</p><p>数组v存储答案</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;queue&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span><span class="hljs-string">&lt;cstring&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;algorithm&gt;</span></span><br><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-type">const</span> <span class="hljs-type">int</span> N = <span class="hljs-number">100010</span>;<br>queue&lt;<span class="hljs-type">int</span>&gt; q;<br>vector&lt;<span class="hljs-type">int</span>&gt; v;<br><br><span class="hljs-type">int</span> e[N], ne[N],h[N];<br><span class="hljs-type">int</span> d[N];<span class="hljs-comment">//计算每个点的入度数</span><br><span class="hljs-type">int</span> idx;<br><span class="hljs-type">int</span> n,m;<br><span class="hljs-type">int</span> sum;<br><br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">add</span><span class="hljs-params">(<span class="hljs-type">int</span> a, <span class="hljs-type">int</span> b)</span></span>&#123;<br>    e[idx] = b;<br>    ne[idx] = h[a];<br>    h[a] = idx++;<br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">void</span> <span class="hljs-title">topsort</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-comment">//找度为0的点</span><br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i =<span class="hljs-number">1</span>; i&lt;=n; i++)&#123;<br>        <span class="hljs-keyword">if</span>(d[i]==<span class="hljs-number">0</span>)&#123;<br>            q.<span class="hljs-built_in">push</span>(i);<br>        &#125;<br>    &#125;<br>    <br>    <span class="hljs-keyword">while</span>(!q.<span class="hljs-built_in">empty</span>())&#123;<br>        <span class="hljs-type">int</span> u = q.<span class="hljs-built_in">front</span>();<br>        v.<span class="hljs-built_in">push_back</span>(u);<br>        sum++;<br>        q.<span class="hljs-built_in">pop</span>();<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> j = h[u]; j!=<span class="hljs-number">-1</span>; j=ne[j])&#123;<br>            <span class="hljs-type">int</span> c = e[j];<br>            d[c]--;<br>            <span class="hljs-keyword">if</span>(d[c]==<span class="hljs-number">0</span>)&#123;<br>                q.<span class="hljs-built_in">push</span>(c);<br>            &#125;<br>        &#125;<br>    &#125;<br>    <br>    <span class="hljs-keyword">if</span>(sum == n)&#123;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-keyword">auto</span> i : v)&#123;<br>            cout&lt;&lt;i&lt;&lt;<span class="hljs-string">&quot; &quot;</span>;<br>        &#125;<br>    &#125;<br>    <span class="hljs-keyword">else</span>&#123;<br>        cout&lt;&lt;<span class="hljs-number">-1</span>;<br>    &#125;<br>    <br>    <br>    <br>&#125;<br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    <span class="hljs-built_in">memset</span>(h, <span class="hljs-number">-1</span>, <span class="hljs-built_in">sizeof</span>(h));<br>    cin&gt;&gt;n&gt;&gt;m;<br>    <span class="hljs-type">int</span> a,b;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i =<span class="hljs-number">0</span> ;i&lt;m ;i++)&#123;<br>        cin&gt;&gt;a&gt;&gt;b;<br>        <span class="hljs-built_in">add</span>(a,b);<br>        d[b]++;<br>    &#125;<br>    <span class="hljs-built_in">topsort</span>();<br>    <br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>前馈神经网络</title>
    <link href="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <url>/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
    
    <content type="html"><![CDATA[<h1 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h1><h2 id="感知机模型"><a href="#感知机模型" class="headerlink" title="感知机模型"></a>感知机模型</h2><h3 id="单层感知机"><a href="#单层感知机" class="headerlink" title="单层感知机"></a>单层感知机</h3><p>输入层：感知层，n个神经节点，无信息处理能力，只负责引入外部信息X。</p><p>处理层：m个神经接点，每节点均有信息处理能力，m个节点向外部处理输出信息，构成输出列向量Y。</p><p>两层间连接权值用权值列向量$W_j$表示，m个权向量构成单层感知器的权值矩阵W。</p><span id="more"></span><p>$W_j&#x3D;[w_{1j} w_{2j} …w{ij}…w_{nj}]^T $</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180125854.png" alt="image-20240608180125854"></p><p>离散型单计算层感知器采用符号型转移函数，则j节点输出为：</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180228313.png" alt="image-20240608180228313"></p><h4 id="单计算节点感知机"><a href="#单计算节点感知机" class="headerlink" title="单计算节点感知机"></a>单计算节点感知机</h4><p>单计算节点感知器实际上就是一个M-P神经元模型。</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180257486.png" alt="image-20240608180257486"></p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180319370.png" alt="image-20240608180319370"></p><p><strong>功能：</strong></p><p>输入向量$X&#x3D;[x_1 x_2 …x_n]^T$,则n个输入分量构成几何n维空间，</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180355265.png" alt="image-20240608180355265"></p><p>该超平面将样本分成2类。</p><p>一个简单的单计算节点感知器具有分类功能，其分类原理是将分类知识存储于感知器的权向量（包括阈值）中，由权向量确定的分类判决界面（线），可将输入模式分为两类。</p><p>举例：</p><ol><li><p>功能”<strong>与</strong>“</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180432426.png" alt="image-20240608180432426"></p></li><li><p>功能”<strong>或</strong>“</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180443134.png" alt="image-20240608180443134"></p></li><li><p>功能”<strong>异或</strong>”</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180456415.png" alt="image-20240608180456415"></p><p>确定的分类判决方程是线性方程,因而只能解决线性可分问题的分类,不能解决线性不可分问题.<br>这称为单计算层感知器的局限性.</p></li></ol><h3 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h3><p>单计算层感知器只能解决线性可分问题,多层感知器可解决线性不可分问题。</p><p>输出层节点以隐层两节点y1,y2的输出作为输入,其结构也相当于一个符号单元。</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180510130.png" alt="image-20240608180510130"></p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608180521772.png" alt="image-20240608180521772"></p><p>1、2两符号单元确定两条分界直线s1和s2,可构成开放式凸域。</p><p>双隐层感知器足以解决任何复杂分类问题，为提高感知器分类能力，可采用<strong>非线性连续函数</strong>作为神经元节点的转移函数，使区域边界变成曲线，可形成连续光滑曲线域。</p><h2 id="神经元"><a href="#神经元" class="headerlink" title="神经元"></a>神经元</h2><p>单个神经细胞只有两种状态：兴奋和抑制</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240607175753581.png" alt="image-20240607175753581"></p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p><strong>·</strong> 激活函数的性质：</p><ol><li>连续并可导的非线性函数（允许在少数点上不可导）</li><li>激活函数及其导数要尽可能的简单</li><li>激活函数的导数的值域要在有关合适的区间内</li><li>单调递增</li></ol><p><strong>·</strong> 常见的激活函数：</p><ol><li><p>S型</p><p>$\sigma(x) &#x3D; \frac{1}{1+e^{-x}}$</p><p>$\tanh(x)&#x3D;\frac{e^x - e^{-x}}{e^x + e^{-x}} &#x3D; 2\sigma(2x)-1$</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240607180731494.png" alt="image-20240607180731494"></p><p><strong>饱和函数：</strong>当$x$趋向无穷大时，其导数置趋于0</p><p>S型激活函数时饱和函数。（可能会导致梯度消失问题）</p><p>Tanh函数时零中心化的，logistic函数的输出恒大于0</p></li><li><p>斜坡型</p><p>$ReLU(x) &#x3D; \begin{cases}    x, &amp; \text{if } x &gt; 0 \    0, &amp; \text{if } x \leq 0 \end{cases}&#x3D;max(x,0)$</p><p>具有单侧抑制，宽兴奋边界，能在一定程度是缓解梯度消失问题 （ReLU 在正区间（x &gt; 0）上的梯度恒为 1，这意味着在正区间内，梯度不会消失。相比之下，一些传统的激活函数（如 Sigmoid 和 Tanh）在输入值较大或较小时会接近饱和，导致梯度消失，使得网络的训练速度变慢。）</p><p>$LeakyReLU(x) &#x3D; \begin{cases}    x, &amp; \text{if } x &gt; 0 \    \gamma x, &amp; \text{if } x \leq 0 \end{cases} &#x3D; max(0,x) + \gamma min(0,x)$</p><p>$PReLU(x) &#x3D; \begin{cases}    x, &amp; \text{if } x &gt; 0 \    \gamma_i x, &amp; \text{if } x \leq 0 \end{cases} &#x3D; max(0,x) + \gamma_i min(0,x)$</p><p>近似零中心化的非线性函数</p><p>$ELU(x) &#x3D; \begin{cases}    x, &amp; \text{if } x &gt; 0 \    \gamma (e^x-1), &amp; \text{if } x \leq 0 \end{cases} &#x3D; max(0,x) +  min(0,\gamma(e^x-1))$</p><p>ReLU的平滑版本：</p><p>$softplus(x) &#x3D; log(1+e^x)$</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240607182142681.png" alt="image-20240607182142681"></p><p><strong>死亡ReLU问题：</strong>一些神经元的输出始终为零，导致这些神经元对于网络的训练没有贡献，失去了激活的能力。可使用LeakyReLU缓解</p></li><li><p>复合函数</p><p>自门控激活函数</p><p>$swish(x) &#x3D; x\sigma(\beta x)$</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240607182656843.png" alt="image-20240607182656843"></p></li></ol><p>常见激活函数及其导数</p><table><thead><tr><th>激活函数</th><th>函数</th><th>导数</th></tr></thead><tbody><tr><td>Logistic函数</td><td>$f(x)&#x3D;\frac{1}{1+e^{-x}}$</td><td>$f^{‘}(x) &#x3D; f(x)(1-f(x)) $</td></tr><tr><td>Tanh函数</td><td>$f(x)&#x3D;\frac{e^x - e^{-x}}{e^x + e^{-x}}$</td><td>$f^{‘}(x) &#x3D; 1 - f^2(x)$</td></tr><tr><td>ReLU函数</td><td>$f(x) &#x3D; max(0,x)$</td><td>$f^{‘}(x) &#x3D; I(x&gt;0)$</td></tr><tr><td>ELU函数</td><td>$f(x) &#x3D; max(0,x)+min(0, \gamma(e^x-1))$</td><td>$f^{‘}(x) &#x3D; I(x&gt;0) + I(x\leq0)\gamma e^x$</td></tr><tr><td>SoftPlus函数</td><td>$f(x) &#x3D; log(1+ e^x)$</td><td>$f^{‘}(x) &#x3D; \frac{1}{1+e^{-x}}$</td></tr></tbody></table><h3 id="人工神经网络"><a href="#人工神经网络" class="headerlink" title="人工神经网络"></a>人工神经网络</h3><p>人工神经网络主要由大量的神经元以及它们之间的有向连接构成。</p><p>需要考虑三方面：</p><ol><li><p>神经元的激活规则</p></li><li><p>网络的拓扑结构</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240607184301479.png" alt="image-20240607184301479"></p></li><li><p>学习算法</p></li></ol><h2 id="前馈神经网络-1"><a href="#前馈神经网络-1" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h2><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>各神经元分别属于不同层，层内无连接</p><p>相邻两层之间的神经元全部两两连接</p><p>网络中无反馈，信号从输入层想输出层单向传播，可用一个有向无环图表示。</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240607184828035.png" alt="image-20240607184828035"></p><h3 id="信息传递过程"><a href="#信息传递过程" class="headerlink" title="信息传递过程"></a>信息传递过程</h3><p>对第$l$层，其净输入$z^l &#x3D; W^la^{l-1}+b^l$，其输出为$a^l &#x3D; f(z^l)$</p><p>前馈计算为：$x &#x3D; a^0 \rightarrow  z^1 \rightarrow a^1 \rightarrow z^2 \rightarrow a^{L-1} \rightarrow z^L \rightarrow a^L $</p><h3 id="通用近似定理"><a href="#通用近似定理" class="headerlink" title="通用近似定理"></a>通用近似定理</h3><p>也叫<strong>万能逼近定理</strong></p><p>对于具有线性输出层和至少一个使用“挤压”性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够，它可以以任意的精度来近似任何从一个定义在实数空间中的有界闭集函数。</p><h3 id="参数学习"><a href="#参数学习" class="headerlink" title="参数学习"></a>参数学习</h3><p>如果使用Softmax回归分类器，相当于网络最后一层设置C 个神经元，其输出经过Softmax函数进行归一化后可以作为每个类的条件概率。</p><p>$\hat{y} &#x3D; softmax(z^L)$</p><p>采用交叉熵损失函数，对样本$(x,y)$，其损失函数为：</p><p>$L(y,\hat{y}) &#x3D; -y^Tlog\hat{y}$</p><p>给定训练集为$𝐷 &#x3D; {(𝒙^{(𝑛)}, 𝑦^{(𝑛)} )}_{𝑛&#x3D;1}^𝑁$ ，将每个样本输入给前馈神经网络，得到网络输出$\hat{𝑦}^{(𝑛)}$ ，其在数据集D上的结构化风险函数为：</p><p>$R(W,b) &#x3D; \frac{1}{N}\sum_{n&#x3D;1}^{N}L(y^n, \hat{y}^n)+ \frac{1}{2} \lambda ||W||^2$</p><p>梯度下降：</p><p>$W^l \leftarrow W^l - \alpha\frac{\partial R(W,b)}{\partial W^l} $</p><p>$b^l \leftarrow b^l - \alpha\frac{\partial R(W,b)}{\partial b^l} $</p><h3 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h3><p><strong>Step1:</strong>  顺序表示梯度公式</p><p>由公式$z^l &#x3D; W^la^{l-1}+b^l$得：</p><p>$\frac{\partial L(y,\hat{y})}{\partial w_{ij}^l} &#x3D; \frac{\partial L(y,\hat{y})}{\partial \mathbf{z}^l } \frac{\partial \mathbf{z}^l}{\partial w_{ij}^l} $ , $\frac{\partial L(y,\hat{y})}{\partial \mathbf{b}^l} &#x3D; \frac{\partial L(y,\hat{y})}{\partial \mathbf{z}^l } \frac{\partial \mathbf{z}^l}{\partial \mathbf{b}^l} $</p><p>这里的$\mathbf{z}, \mathbf{b}$都是m维的向量</p><p>$\frac{\partial \mathbf{z}^l}{\partial w_{ij}^l} &#x3D; [ \frac{\partial z^l_1}{\partial w_{ij}^l}, \frac{\partial z^l_2}{\partial w_{ij}^l},…,\frac{\partial z^l_{i}}{\partial w_{ij}^l},…,\frac{\partial z^l_m}{\partial w_{ij}^l}] &#x3D; [0,0,…,\frac{\partial a^{l-1} \mathbf{w}_{i:}^l+b^{l}<em>i}{\partial w</em>{ij}^l},..,0] &#x3D; [0,0,…,a^{l-1}_j,0]$</p><p>同理，将$a_j^{l-1}$看作$1$，可得：</p><p>$\frac{\partial \mathbf{z}^l}{\partial \mathbf{b}^l} &#x3D; \mathbf{I}_m$</p><p>再令，$ \frac{\partial L(y,\hat{y})}{\partial \mathbf{z}^l }  &#x3D;  \mathbf{\delta}^l$，则损失函数对参数z，b求导最后可以写作：</p><p>$\frac{\partial L(y,\hat{y})}{\partial w_{ij}^l} &#x3D; \delta^l_i a_j^{l-1} $</p><p>$\frac{\partial L(y,\hat{y})}{\partial \mathbf{b}^l} &#x3D; \bold{\delta}^l$</p><p><strong>Step2:</strong>  根据递推公式表示$\bold{\delta}^l$</p><p>$\mathbf{z}^{l+1} &#x3D; \mathbf{W}^{l+1} \mathbf{a}^{l} + \mathbf{b}^{l+1}$</p><p>$\mathbf{a}^l &#x3D; f(\mathbf{z}^l)$</p><p>即$\mathbf{z}^l \rightarrow \mathbf{a}^l \rightarrow \mathbf{z}^{l+1}$</p><p>$\bold{\delta}^l &#x3D; \frac{\partial L(y,\hat{y})}{\partial \mathbf{z}^l }  &#x3D; \frac{\partial L(y,\hat{y})}{\partial \mathbf{z}^{l+1} } \frac{\partial \mathbf{z}^{l+1}}{\partial \mathbf{a}^l } \frac{\partial \mathbf{a}^l}{\partial \mathbf{z}^l }  &#x3D; \mathbf{\delta^{l+1}} (\mathbf{W}^{l+1})^T diag(f^{‘}(\mathbf{z}^l)) &#x3D; f^{‘}(\mathbf{z}^l) \odot \mathbf{\delta^{l+1}} (\mathbf{W}^{l+1})^T$</p><p>随机梯度下降训练过程：</p><p>输入：训练数据，验证数据</p><p>首先 随机初始化参数$\mathbf{W}, \mathbf{b}$</p><p>重复以下操作：</p><p>  对样本随机重排</p><p>  for n &#x3D;1 .. N do:</p><ol><li><p>选择一个样本$(\mathbf{x}^n, y^n)$，计算前馈的每一层的净输入$\mathbf{z}^l$和激活值$\mathbf{a}^l$，并计算每一层的误差$\bold{\delta}^l$，这个可以通过step2的公式计算</p></li><li><p>反向传播，计算每层的偏导，可用step1的公式计算</p><p>$\frac{\partial L(y,\hat{y})}{\partial w_{ij}^l} &#x3D; \delta^l_i a_j^{l-1} $</p><p>$\frac{\partial L(y,\hat{y})}{\partial \mathbf{b}^l} &#x3D; \bold{\delta}^l$</p></li><li><p>更新参数</p><p>$W^l \leftarrow W^l - \alpha\frac{\partial R(W,b)}{\partial W^l} $</p><p>$b^l \leftarrow b^l - \alpha\frac{\partial R(W,b)}{\partial b^l} $</p></li></ol><p>  end</p><p>直至模型在验证集上的错误率不在下降</p><p>举例：</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608170939640.png" alt="image-20240608170939640"></p><p>先按传播顺序计算出各个净输入和激活值</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/image-20240608171752988.png" alt="image-20240608171752988"></p><ol><li><p>计算输出层</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/56d0197d29a72eaa8310d7fedd501891-17178392786981.png" alt="img"></p><p>计算这层的误差：$\mathbf{\delta}^l &#x3D; \hat{y} (1- \hat{y}) (\hat{y} - y)$</p><p>更新权值：$\frac{\partial L(y,\hat{y})}{\partial w_{ij}^l} &#x3D; \delta^l_i a_j^{l-1}$</p></li><li><p>计算隐藏层</p><p><img src="/2024/06/03/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/56d0197d29a72eaa8310d7fedd501891.png" alt="img"></p><p>计算这层的误差：$\mathbf{\delta}^l &#x3D; \hat{y} (1- \hat{y}) \sum(\mathbf{\delta}^{l+1}<em>j)w</em>{ij}$</p><p>更新权值：$\frac{\partial L(y,\hat{y})}{\partial w_{ij}^l} &#x3D; \delta^l_i a_j^{l-1}$</p></li></ol><h2 id="重点"><a href="#重点" class="headerlink" title="重点"></a>重点</h2><ol><li>前馈神经网络特点</li><li>激活函数的定义及其导数</li><li>反向传播算法</li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>预训练模型</title>
    <link href="/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/"/>
    <url>/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/</url>
    
    <content type="html"><![CDATA[<h1 id="预训练模型"><a href="#预训练模型" class="headerlink" title="预训练模型"></a>预训练模型</h1><h2 id="预训练简介"><a href="#预训练简介" class="headerlink" title="预训练简介"></a>预训练简介</h2><p><strong>预训练</strong>：通过子监督学习从大规模数据里获取与具体任务无关的预训练模型的过程。</p><p>预训练任务：</p><ol><li><p>掩码语言模型（编码器）：将一些位置的token替换成特殊的[MASK]字符，预测这些被替换的字符</p><p><img src="/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240602212830234.png" alt="image-20240602212830234"></p><p>只计算掩码部分的loss，其余部分不计算loss</p><span id="more"></span></li><li><p>因果语言模型（解码器）：输入完整序列，基于上文预测当前token</p><p><img src="/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240602213105457.png" alt="image-20240602213105457"></p><p>eos代表句子结束</p></li><li><p>序列到序列模型：编码器解码器方式，预测部分放到解码器里面</p><p>只计算解码器的loss，不计算解码器部分</p></li></ol><h2 id="文本摘要"><a href="#文本摘要" class="headerlink" title="文本摘要"></a>文本摘要</h2><p><strong>文本摘要：</strong>输入长文本，将长文本转成简短的摘要</p><p>任务类别：单文档单语言摘要</p><p>评价指标：Rouge-1(基于1-gram) Rouge-2(基于2-gram) Rouge-L(基于LCS)</p><table><thead><tr><th>原始文本</th><th>1-gram</th><th>2-gram</th></tr></thead><tbody><tr><td>今天不错</td><td>今 天 不 错</td><td>今天 天不 不错</td></tr><tr><td>今天太阳不错</td><td>今 天 太 阳 不 错</td><td>今天 天太 太阳 阳不 不错</td></tr></tbody></table><p>Rouge-1   P &#x3D; 4&#x2F;4   R&#x3D;4&#x2F;6  F &#x3D; 2PR(P+R)</p><p>Rouge-2   P &#x3D; 2&#x2F;3   R&#x3D;2&#x2F;5</p><p>LCS(最长公共子序列)   P &#x3D; 4&#x2F;4   R&#x3D;4&#x2F;6</p><h3 id="数据处理"><a href="#数据处理" class="headerlink" title="数据处理"></a>数据处理</h3><p>input和labels分开处理，labels最后一定要有eos_token</p><p>labels不仅是标签，还是解码器的输入</p><p>数据集：</p><h2 id="对话机器人"><a href="#对话机器人" class="headerlink" title="对话机器人"></a>对话机器人</h2><h1 id="参数微调fine-tuning"><a href="#参数微调fine-tuning" class="headerlink" title="参数微调fine-tuning"></a>参数微调fine-tuning</h1><h2 id="beat-fit"><a href="#beat-fit" class="headerlink" title="beat-fit"></a>beat-fit</h2><p>只对bias求梯度，其他的参数冻结</p><h2 id="prompt-Tuning"><a href="#prompt-Tuning" class="headerlink" title="prompt-Tuning"></a>prompt-Tuning</h2><p><img src="/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/image-20240605211808432.png" alt="image-20240605211808432"></p><p>冻结主模型所有参数，在训练数据前加入一小段prompt，只训练prompt的embedding层。</p><p>hard prompt：指定prompt</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">config = PromptTuningConfig(task_type=TaskType.CAUSAL_LM, prompt_tuning_init=PromptTuningInit.TEXT, <br>                            prompt_tuning_init_text = <span class="hljs-string">&quot;下面是一段人与机器人的对话。&quot;</span>,<br>                            num_virtual_tokens  = <span class="hljs-built_in">len</span>(tokenizer(<span class="hljs-string">&quot;下面是一段人与机器人的对话。&quot;</span>)[<span class="hljs-string">&quot;input_ids&quot;</span>]), <br>                            tokenizer_name_or_path = <span class="hljs-string">&quot;Langboat/bloom-389m-zh&quot;</span>)<br><br><br></code></pre></td></tr></table></figure><p>soft prompt：不指定prompt，让模型自行学习  ，对模型需要进行适配多轮才能有好效果</p><h2 id="P-Tuning"><a href="#P-Tuning" class="headerlink" title="P-Tuning"></a>P-Tuning</h2><p><img src="/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/image-20240605212049075.png" alt="image-20240605212049075"></p><p>在Prompt-Tuning基础上，对prompt部分进行进一步的编码计算，加速收敛。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> PromptEncoderConfig, TaskType, get_peft_model, PromptEncoderReparameterizationType<br><br>config = PromptEncoderConfig(task_type = TaskType.CAUSAL_LM, num_virtual_tokens = <span class="hljs-number">10</span>, <br>                             encoder_reparameterization_type = PromptEncoderReparameterizationType.LSTM,<br>                             encoder_dropout = <span class="hljs-number">0.1</span>, encoder_num_layers = <span class="hljs-number">2</span>, encoder_hidden_size =<span class="hljs-number">1024</span><br>                             )<br>config<br></code></pre></td></tr></table></figure><h2 id="Prefix-Tuning"><a href="#Prefix-Tuning" class="headerlink" title="Prefix-Tuning"></a>Prefix-Tuning</h2><p>past_key_values：Transformer模型中历史计算过的key和value的结果，会存在重复计算，可将结果缓存。</p><p>通过past_key_values的形式将可学习的部分放到了模型中的每一层，这部分内容又称为前缀。</p><p><img src="/2024/06/02/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B/image-20240605214006018.png" alt="image-20240605214006018"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Transformer</title>
    <link href="/2024/06/02/Transformer/"/>
    <url>/2024/06/02/Transformer/</url>
    
    <content type="html"><![CDATA[<h1 id="Transformer的理解"><a href="#Transformer的理解" class="headerlink" title="Transformer的理解"></a>Transformer的理解</h1><h1 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h1><p><img src="https://img-blog.csdnimg.cn/direct/8a8c78c6941948a1827a013fe1da2bb3.png" alt="在这里插入图片描述"></p><h1 id="ENCODER"><a href="#ENCODER" class="headerlink" title="ENCODER"></a>ENCODER</h1><p><img src="https://img-blog.csdnimg.cn/direct/e93387aeabd749789fcb9379ef913203.png" alt="在这里插入图片描述"></p><h2 id="输入部分"><a href="#输入部分" class="headerlink" title="输入部分"></a>输入部分</h2><p>对拆分后的语句x &#x3D; [batch_size, seq_len]进行以下操作</p><span id="more"></span><ol><li>Embedding<br>将离散的输入（如单词索引或其他类别特征）转换为稠密的实数向量，以便可以在神经网络中使用。</li><li>位置编码<br>与RNN相比，RNN是一个字一个字的输入，自然每个字的顺序关系信息就会保留下来。但在Encoder中，一个句子的每一个字（词）是并行计算的（下一节解释），所以我们在输入的时候需要提前引入位置信息。<br>位置信息由： pos（一句话中的第几个字） 和 i （这个字编码成向量后的第i维) 来确定<br>下面是Positional Encoding的公式：<br>i为 偶 数 时 ,  $PE_{pos, i}&#x3D; sin( pos&#x2F; 10000^{2i&#x2F; d_{model}})$<br>i为 奇 数 时 ,  $PE_{pos, i}&#x3D; cos( pos&#x2F; 10000^{2i&#x2F; d_{model}})$<br>$d_{model}$指想用多长的 vector 来表达一个词(embedding_dim)</li></ol><p>通过输入部分<br>x： [batch_size, seq_len, embedding_dim]</p><p><img src="https://img-blog.csdnimg.cn/direct/2cd5cdb5641248179485166cd2044e44.png" alt="在这里插入图片描述"></p><h2 id="多头注意力机制"><a href="#多头注意力机制" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h2><ol><li>单头注意力机制<br>对一句话中第i个字的字向量$a_i$，产生三个矩阵Q, K ，V<br>Q,K,V的维度都为[batch_size, seq_len, embedding_dim]</li></ol><p>将$a_i$分别与上面三个矩阵相乘，得到三个向量$q_i, k_i, v_i$<br>如果要计算第1个字向量与句子中所有字向量的注意力：<br>将查询向量$q_1$与 所有的字向量的键向量$k_i$相乘得到$alpha_{10}, alpha_{11},…,alpha_{1,seqlen}$<br>将这写数值进行softmax处理后， 分别与$v_i$相乘再合加得到最终结果$b_1$</p><p><img src="https://img-blog.csdnimg.cn/direct/1e55284965a44375b70d7f35a5869142.png" alt="在这里插入图片描述"></p><ol start="2"><li>多头注意力机制<br>把$Q,K,V$三个大矩阵变成n个小矩阵（seq_len, embedding_dim&#x2F;n)     n&#x3D;8<br>用上节相同的方式计算8个矩阵，然后把每一个head-Attention计算出来的b矩阵拼在一起，作为输出</li></ol><h2 id="Add-LN"><a href="#Add-LN" class="headerlink" title="Add&amp;LN"></a>Add&amp;LN</h2><p>Add是用了残差神经网络的思想，也就是把Multi-Head Attention的输入的a矩阵直接加上Multi-Head Attention的输出b矩阵（好处是可以让网络训练的更深）得到的和 $\bar{b}$矩阵</p><p>再在经过Layer normalization（归一化，作用加快训练速度，加速收敛）把<br> 每一行（也就是每个句子）做归一为标准正态分布，最后得到$\hat{b}$<br>BN 和 LN：</p><ol><li>LN： 在一个样本内做归一化 适于RNN,transformer</li><li>BN： 对batch_size里面的样本按对应的特征做归一化  适于CNN<br><img src="https://img-blog.csdnimg.cn/direct/8f699727d3214959addb03e90822d558.png" alt="在这里插入图片描述"></li></ol><h2 id="Feed-forward前馈神经网络"><a href="#Feed-forward前馈神经网络" class="headerlink" title="Feed_forward前馈神经网络"></a>Feed_forward前馈神经网络</h2><p>把Add &amp; Layer normalization输出$\hat{b}$，经过两个全连接层，再经过Add &amp; Layer normalization得到最后输出 o 矩阵</p><h1 id="DECODER"><a href="#DECODER" class="headerlink" title="DECODER"></a>DECODER</h1><h2 id="masked-多头注意力机制"><a href="#masked-多头注意力机制" class="headerlink" title="masked_多头注意力机制"></a>masked_多头注意力机制</h2><p> 比如我们在中英文翻译时候，会先把”我是学生”整个句子输入到Encoder中，得到最后一层的输出后，才会在Decoder输入”S I am a student”（s表示开始）,但是”S I am a student”这个句子我们不会一起输入，而是在T0时刻先输入”S”预测，预测第一个词”I”；在下一个T1时刻，同时输入”S”和”I”到Decoder预测下一个单词”am”；然后在T2时刻把”S,I,am”同时输入到Decoder预测下一个单词”a”,依次把整个句子输入到Decoder,预测出”I am a student E”</p><h2 id="多头注意力机制-1"><a href="#多头注意力机制-1" class="headerlink" title="多头注意力机制"></a>多头注意力机制</h2><p>Decoder 的 Multi-Head Attention 的输入来自两部分，<br>K，V 矩阵来自Encoder的输出，<br>Q 矩阵来自 Masked Multi-Head Attention 的输出<br><img src="https://img-blog.csdnimg.cn/direct/3f06bddb5c124448b71e5df6a4c2429d.png" alt="在这里插入图片描述"></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>first-blog</title>
    <link href="/2024/06/02/first-blog/"/>
    <url>/2024/06/02/first-blog/</url>
    
    <content type="html"><![CDATA[<h1 id="数据结构复习篇1——哈夫曼树"><a href="#数据结构复习篇1——哈夫曼树" class="headerlink" title="数据结构复习篇1——哈夫曼树"></a>数据结构复习篇1——哈夫曼树</h1><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><ol><li><p>结点的带权路径：从根结点到该结点之间的路径长度与该结点的权的乘积</p></li><li><p>树的带权路径：所有叶子结点的带权路径长度之和</p><p>举例：</p><p><img src="/2024/06/02/first-blog/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240602203936732.png" alt="image-20240602203936732"></p></li><li><p>哈夫曼树：将n个权值作为二叉树的n个叶子结点，若树的带权路径长度达到最小，则这棵树被称为哈夫曼树</p></li></ol><span id="more"></span><h2 id="构造哈夫曼树"><a href="#构造哈夫曼树" class="headerlink" title="构造哈夫曼树"></a>构造哈夫曼树</h2><p>给我们n个结点，如何构造出一颗哈夫曼树呢？</p><p>在这里，我们可以尝试使用贪心的策略，如果要是的WPL最小，那权值大的点应该放在深度很小的地方，权值小的点应该放到底层，即深度很大的地方。所以每次构造时选择两个权值最小的结点进行构造。</p><p>举例：给定4，7，9，10这四个结点，如何构造一个是的WPL最小的二叉树呢？</p><p><strong>·Step1:</strong> 从这个结点集合中选择权值最小的两个点，组成一个新结点。</p><p>在这个例子里面，选择 4 和 7 ，构造成新的点11</p><p><strong>·Step2:</strong> 把选过的结点从结点集合排出，新结点11加入结点集合，重复step1，直至结点集合里面只有一个结点为止。</p><p>根据上述步骤，最后组成的哈夫曼树如图所示。</p><p><img src="/2024/06/02/first-blog/Users\TJL\AppData\Roaming\Typora\typora-user-images\image-20240602204852758.png" alt="image-20240602204852758"></p><h2 id="代码实践"><a href="#代码实践" class="headerlink" title="代码实践"></a>代码实践</h2><p>对于上述的方法，很容易想到如果要构造一个哈夫曼树可以使用最小堆（priority_queue）实现。每次从堆中弹出的两个结点即是我们要取的构造新结点的组成结点。</p><p>这里，我们将二叉树扩展到K叉树，即每个父结点由K个字节的组成。</p><h3 id="荷马史诗"><a href="#荷马史诗" class="headerlink" title="荷马史诗"></a>荷马史诗</h3><p>引入问题：（该问题来自acwing<a href="https://www.acwing.com/problem/content/151/">149. 荷马史诗 - AcWing题库</a>)</p><p>一部《荷马史诗》中有 n 种不同的单词，从 1到 n 进行编号。其中第 i𝑖 种单词出现的总次数为 𝑤𝑖。</p><p>达达想要用 𝑘 进制串 𝑠𝑖 来替换第 𝑖 种单词，使得其满足如下要求:</p><p>对于任意的 1≤i,j≤n，i≠j1≤𝑖,𝑗≤𝑛，𝑖≠𝑗，都有：𝑠𝑖 不是 𝑠𝑗 的前缀。</p><p>现在达达想要知道，如何选择 𝑠𝑖，才能使替换以后得到的新的《荷马史诗》长度最小。</p><p>在确保总长度最小的情况下，达达还想知道最长的 𝑠𝑖 的最短长度是多少？</p><p>一个字符串被称为 k𝑘 进制字符串，当且仅当它的每个字符是 0 到 𝑘−1 之间（包括 0 和 𝑘−1）的整数。</p><p>字符串 𝑆𝑡𝑟1 被称为字符串 𝑆𝑡𝑟2 的前缀，当且仅当：存在 1≤t≤m1≤𝑡≤𝑚，使得 𝑆𝑡𝑟1&#x3D;𝑆𝑡𝑟2[1..𝑡]。</p><p>其中，𝑚 是字符串𝑆𝑡𝑟2 的长度，𝑆𝑡𝑟2[1..𝑡] 表示𝑆𝑡𝑟2 的前 𝑡 个字符组成的字符串。</p><p><strong>注意</strong>: 请使用 64 位整数进行输入输出、储存和计算。</p><p>输出文件包括 2 行。</p><p>第 1 行输出 1 个整数，为《荷马史诗》经过重新编码以后的最短长度。</p><p>第 2 行输出 1 个整数，为保证最短总长度的情况下，最长字符串 𝑠𝑖 的最短长度。</p><p>对于该问题，很明显可以看出需要我们构造一个k叉树使其$WPL$达到最小，另外，对于权值相同的结点，要优先考虑深度小的来构造，避免树的深度过大。</p><p>下面是代码实现。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;iostream&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;queue&gt;</span></span><br><span class="hljs-meta"># <span class="hljs-keyword">include</span> <span class="hljs-string">&lt;vector&gt;</span></span><br><br><span class="hljs-keyword">using</span> <span class="hljs-keyword">namespace</span> std;<br><br><span class="hljs-keyword">typedef</span> <span class="hljs-type">long</span> <span class="hljs-type">long</span>  LL;<br>priority_queue&lt;pair&lt;LL, <span class="hljs-type">int</span>&gt;, vector&lt;pair&lt;LL, <span class="hljs-type">int</span>&gt;&gt;, greater&lt;pair&lt;LL, <span class="hljs-type">int</span>&gt;&gt;&gt; h;<br><br><span class="hljs-type">int</span> n,k;<br><br><br><span class="hljs-function"><span class="hljs-type">int</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span>&#123;<br>    cin&gt;&gt;n&gt;&gt;k;<br>    LL a;<br>    <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;n; i++)&#123;<br>        cin&gt;&gt;a;<br>        h.<span class="hljs-built_in">push</span>(&#123;a,<span class="hljs-number">0</span>&#125;);<br>    &#125;<br>    <span class="hljs-comment">//对于填不满k叉树的结点，补0来处理</span><br>    <span class="hljs-keyword">while</span>((n<span class="hljs-number">-1</span>)%(k<span class="hljs-number">-1</span>))&#123;<br>        h.<span class="hljs-built_in">push</span>(&#123;<span class="hljs-number">0</span>,<span class="hljs-number">0</span>&#125;);<br>        n++;<br>    &#125;<br>    <br>    LL res = <span class="hljs-number">0</span>;<br>    <br>    <span class="hljs-keyword">while</span>(h.<span class="hljs-built_in">size</span>()&gt;<span class="hljs-number">1</span>)&#123;<br>        <br>        <span class="hljs-type">int</span> depth = <span class="hljs-number">0</span>;<br>        LL s = <span class="hljs-number">0</span>;<br>        <span class="hljs-keyword">for</span>(<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>; i&lt;k; i++)&#123;<br>            <span class="hljs-keyword">auto</span> p = h.<span class="hljs-built_in">top</span>();<br>            s += p.first;<br>            depth = <span class="hljs-built_in">max</span>(depth, p.second);<br>            h.<span class="hljs-built_in">pop</span>();<br>        &#125;<br>        res += s;<br>        h.<span class="hljs-built_in">push</span>(&#123;s, depth+<span class="hljs-number">1</span>&#125;);<br>        <br>        <br>    &#125;<br>    cout&lt;&lt;res&lt;&lt;endl;<br>    cout&lt;&lt;h.<span class="hljs-built_in">top</span>().second&lt;&lt;endl;<br>    <br>    <br>    <br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/06/02/hello-world/"/>
    <url>/2024/06/02/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
